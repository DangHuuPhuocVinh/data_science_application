{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preface**\n",
    "**This project was inspired and referenced in [TensorFlow roBERTa - [0.705]](https://www.kaggle.com/code/cdeotte/tensorflow-roberta-0-705)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1/ About team**\n",
    "|StuID  |        Name       |     Kaggle    |       Github      |\n",
    "|-------|-------------------|---------------|-------------------|\n",
    "|1752052|Dang Huu Phuoc Vinh|[V_Notebook](https://www.kaggle.com/danghuuphuocvinh)|[V_Github](https://github.com/DangHuuPhuocVinh/data_science_application)\n",
    "|1753097|Le Nguyen Minh Tam |[T_Notebook](https://www.kaggle.com/minhtamlenguyen)|[T_Github](https://github.com/lnmtam1999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2/ About competition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1/ Name of competition**\n",
    "**[Tweet Sentiment Extraction](https://www.kaggle.com/competitions/tweet-sentiment-extraction) organized by [Kaggle](https://www.kaggle.com/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2/ Prize**\n",
    "**15000 USD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.3/ Description**\n",
    "  **E.g: \"My ridiculous dog is amazing.\" [sentiment: positive]**\n",
    "\n",
    "  **With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.**\n",
    "\n",
    "  **In this competition we've extracted support phrases from [Figure Eight's Data for Everyone platform](https://appen.com/datasets-resource-center/). The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.4/ Input and Output**\n",
    "- **Input: textID, text and sentiment**\n",
    "- **Output: selected_text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.5/ Columns**\n",
    "-  **textID - unique ID for each piece of text**\n",
    "-  **text - the text of the tweet**\n",
    "-  **sentiment - the general sentiment of the tweet**\n",
    "-  **selected_text - [train only] the text that supports the tweet's sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.6/ Meaningful**\n",
    "- **After doing this project, we can have a dataset with the phrases that were selected for using at any other NLP project**\n",
    "- **Can use for detecting some keywords that have sentiment** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.7/ Evaluation**\n",
    " ![img](https://user-images.githubusercontent.com/35680794/174698744-57b2f116-fbe4-4fb6-9216-2e83e0494dca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3/ Model roBERTa for this project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4/ Developing the project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1/ Import Libraries, Data and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:37.249683Z",
     "iopub.status.busy": "2022-07-10T19:03:37.249400Z",
     "iopub.status.idle": "2022-07-10T19:03:37.256307Z",
     "shell.execute_reply": "2022-07-10T19:03:37.255411Z",
     "shell.execute_reply.started": "2022-07-10T19:03:37.249641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1.1/ Tokenizer**\n",
    "First of all, we use tokenizer to convert the word to array for the computer can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:40.525850Z",
     "iopub.status.busy": "2022-07-10T19:03:40.525552Z",
     "iopub.status.idle": "2022-07-10T19:03:40.902687Z",
     "shell.execute_reply": "2022-07-10T19:03:40.901896Z",
     "shell.execute_reply.started": "2022-07-10T19:03:40.525819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 92\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAX_LEN** = 96 then for each training row, RoBERTa receives 96 tokens. The reason to use **lowercase** and **add_prefix_space** because when spelling with RoBERTa :\" helllo\", \"hello\", \" Hello\", and \"Hello\" use the same \" hello\" token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2/ Training data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.1/ Adjust the input**\n",
    "In this stage we ready our data for the model, all the input will be change to numerical and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:44.266109Z",
     "iopub.status.busy": "2022-07-10T19:03:44.265840Z",
     "iopub.status.idle": "2022-07-10T19:03:53.105947Z",
     "shell.execute_reply": "2022-07-10T19:03:53.105081Z",
     "shell.execute_reply.started": "2022-07-10T19:03:44.266080Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[    0   939 12905 ...     1     1     1]\n",
      " [    0    98  3036 ...     1     1     1]\n",
      " [    0   127  3504 ...     1     1     1]\n",
      " ...\n",
      " [    0  1423   857 ...     1     1     1]\n",
      " [    0    53    24 ...     1     1     1]\n",
      " [    0    70    42 ...     1     1     1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [(0, 4), (4, 9), (9, 12), (12, 18), (18, 24), (24, 27), (27, 29), (29, 33), (33, 36), (36, 37), (37, 44), (44, 45), (45, 47), (47, 49), (49, 50), (50, 53), (53, 54), (54, 57), (57, 59)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3/ Test Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same as the Training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:04:17.944124Z",
     "iopub.status.busy": "2022-07-10T19:04:17.943856Z",
     "iopub.status.idle": "2022-07-10T19:04:18.241347Z",
     "shell.execute_reply": "2022-07-10T19:04:18.240640Z",
     "shell.execute_reply.started": "2022-07-10T19:04:17.944095Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the original text is [ i][ am][ having][ a][ great][ day]</s>[ positive]</s> with tokens 0,1,2,3,4,5,6,7,8,9,10 and the selected text is \"great day\", then the training has start index = 5 and end index = 6.\n",
    "\n",
    "If our model also predicts a = 5 and b = 6 and we try to select the text from [ i][ am][ having][ a][ great][ day], the indices 5 and 6 will not return \"great day\". Instead we must use tokens[4:6] to get great day. We subtract 1 because the is now removed. And we add 1 to b because python indexing for list[3:5] does not return 5 it only returns 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4/ Build roBERTa model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4.1/ Building the bones of model**\n",
    "Built the bones of a roBERTa model, using the model has already train by the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:05:21.352602Z",
     "iopub.status.busy": "2022-07-10T19:05:21.352259Z",
     "iopub.status.idle": "2022-07-10T19:05:21.364895Z",
     "shell.execute_reply": "2022-07-10T19:05:21.364038Z",
     "shell.execute_reply.started": "2022-07-10T19:05:21.352569Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5/ Create metric**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We code this for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:06:16.388950Z",
     "iopub.status.busy": "2022-07-10T19:06:16.388632Z",
     "iopub.status.idle": "2022-07-10T19:06:16.395186Z",
     "shell.execute_reply": "2022-07-10T19:06:16.394374Z",
     "shell.execute_reply.started": "2022-07-10T19:06:16.388923Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.6/ Train roBERTa model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:07:44.465971Z",
     "iopub.status.busy": "2022-07-10T19:07:44.465660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.6180 - activation_loss: 1.2717 - activation_1_loss: 1.3463\n",
      "Epoch 00001: val_loss improved from inf to 1.85094, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 284s 13ms/sample - loss: 2.6163 - activation_loss: 1.2710 - activation_1_loss: 1.3453 - val_loss: 1.8509 - val_activation_loss: 0.9334 - val_activation_1_loss: 0.9172\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.7824 - activation_loss: 0.8849 - activation_1_loss: 0.8975\n",
      "Epoch 00002: val_loss improved from 1.85094 to 1.75013, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 265s 12ms/sample - loss: 1.7824 - activation_loss: 0.8849 - activation_1_loss: 0.8975 - val_loss: 1.7501 - val_activation_loss: 0.8712 - val_activation_1_loss: 0.8789\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.6160 - activation_loss: 0.8085 - activation_1_loss: 0.8075\n",
      "Epoch 00003: val_loss improved from 1.75013 to 1.73966, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 265s 12ms/sample - loss: 1.6162 - activation_loss: 0.8085 - activation_1_loss: 0.8077 - val_loss: 1.7397 - val_activation_loss: 0.8662 - val_activation_1_loss: 0.8732\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.6810914375665961\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1608 - activation_loss: 1.0754 - activation_1_loss: 1.0854\n",
      "Epoch 00001: val_loss improved from inf to 1.71738, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 303s 14ms/sample - loss: 2.1608 - activation_loss: 1.0748 - activation_1_loss: 1.0852 - val_loss: 1.7174 - val_activation_loss: 0.8743 - val_activation_1_loss: 0.8423\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6500 - activation_loss: 0.8424 - activation_1_loss: 0.8077\n",
      "Epoch 00002: val_loss improved from 1.71738 to 1.66343, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 281s 13ms/sample - loss: 1.6501 - activation_loss: 0.8438 - activation_1_loss: 0.8076 - val_loss: 1.6634 - val_activation_loss: 0.8348 - val_activation_1_loss: 0.8282\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4904 - activation_loss: 0.7661 - activation_1_loss: 0.7243\n",
      "Epoch 00003: val_loss improved from 1.66343 to 1.66041, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 280s 13ms/sample - loss: 1.4904 - activation_loss: 0.7650 - activation_1_loss: 0.7235 - val_loss: 1.6604 - val_activation_loss: 0.8436 - val_activation_1_loss: 0.8161\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.7051354113471853\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1810 - activation_loss: 1.0825 - activation_1_loss: 1.0984\n",
      "Epoch 00001: val_loss improved from inf to 1.71114, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 305s 14ms/sample - loss: 2.1810 - activation_loss: 1.0827 - activation_1_loss: 1.0981 - val_loss: 1.7111 - val_activation_loss: 0.8724 - val_activation_1_loss: 0.8387\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6733 - activation_loss: 0.8504 - activation_1_loss: 0.8230\n",
      "Epoch 00002: val_loss improved from 1.71114 to 1.63758, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 281s 13ms/sample - loss: 1.6732 - activation_loss: 0.8491 - activation_1_loss: 0.8218 - val_loss: 1.6376 - val_activation_loss: 0.8330 - val_activation_1_loss: 0.8048\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4791 - activation_loss: 0.7641 - activation_1_loss: 0.7149\n",
      "Epoch 00003: val_loss did not improve from 1.63758\n",
      "21985/21985 [==============================] - 279s 13ms/sample - loss: 1.4790 - activation_loss: 0.7639 - activation_1_loss: 0.7147 - val_loss: 1.6742 - val_activation_loss: 0.8509 - val_activation_1_loss: 0.8236\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 3 Jaccard = 0.6997808371750929\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1630 - activation_loss: 1.0831 - activation_1_loss: 1.0800\n",
      "Epoch 00001: val_loss improved from inf to 1.65780, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 303s 14ms/sample - loss: 2.1629 - activation_loss: 1.0815 - activation_1_loss: 1.0784 - val_loss: 1.6578 - val_activation_loss: 0.8478 - val_activation_1_loss: 0.8101\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6319 - activation_loss: 0.8425 - activation_1_loss: 0.7895\n",
      "Epoch 00002: val_loss improved from 1.65780 to 1.60991, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 280s 13ms/sample - loss: 1.6319 - activation_loss: 0.8413 - activation_1_loss: 0.7886 - val_loss: 1.6099 - val_activation_loss: 0.8265 - val_activation_1_loss: 0.7835\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4859 - activation_loss: 0.7715 - activation_1_loss: 0.7144\n",
      "Epoch 00003: val_loss did not improve from 1.60991\n",
      "21985/21985 [==============================] - 280s 13ms/sample - loss: 1.4859 - activation_loss: 0.7704 - activation_1_loss: 0.7134 - val_loss: 1.6126 - val_activation_loss: 0.8199 - val_activation_1_loss: 0.7925\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 4 Jaccard = 0.7031080851565811\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1680 - activation_loss: 1.0829 - activation_1_loss: 1.0851\n",
      "Epoch 00001: val_loss improved from inf to 1.73462, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 304s 14ms/sample - loss: 2.1681 - activation_loss: 1.0846 - activation_1_loss: 1.0855 - val_loss: 1.7346 - val_activation_loss: 0.8760 - val_activation_1_loss: 0.8578\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6495 - activation_loss: 0.8436 - activation_1_loss: 0.8059\n",
      "Epoch 00002: val_loss improved from 1.73462 to 1.62147, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 280s 13ms/sample - loss: 1.6496 - activation_loss: 0.8437 - activation_1_loss: 0.8084 - val_loss: 1.6215 - val_activation_loss: 0.8437 - val_activation_1_loss: 0.7770\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5389 - activation_loss: 0.7834 - activation_1_loss: 0.7555\n",
      "Epoch 00003: val_loss improved from 1.62147 to 1.60147, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 280s 13ms/sample - loss: 1.5388 - activation_loss: 0.7822 - activation_1_loss: 0.7544 - val_loss: 1.6015 - val_activation_loss: 0.8263 - val_activation_1_loss: 0.7744\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 5 Jaccard = 0.7123838286608022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.7/ Kaggle submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>472c3e2c41</td>\n",
       "      <td>Getting somewhere with my first 'real' KiokuDB and catal...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>getting somewhere with my first 'real' kiokudb and cata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8be365118e</td>\n",
       "      <td>you are lame  go make me breakfast!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>lame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>15892f296d</td>\n",
       "      <td>wishes that you would see not with your eyes but with yo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wishes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>65e715d6e5</td>\n",
       "      <td>I`ve been there.  The only place I have flown out of si...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i`ve been there. the only place i have flown out of sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>f50fcc36b5</td>\n",
       "      <td>i love you so much tay (: youre so amazing &lt;3 you shoul...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i love you so much tay (: youre so amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>ce4b5fe266</td>\n",
       "      <td>`s current goal is to lose 10 pounds by next tuesday! 12...</td>\n",
       "      <td>positive</td>\n",
       "      <td>soooo ready</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>41ff25c568</td>\n",
       "      <td>This is how much hair falls off everytime I shower. It`s...</td>\n",
       "      <td>negative</td>\n",
       "      <td>it`s more sad than disgusting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>8d4c510735</td>\n",
       "      <td>I hope you didn`t take that quote personally!! I`ve bee...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i hope you didn`t take that quote personally!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577</th>\n",
       "      <td>bf580a287e</td>\n",
       "      <td>All ready for work... except my arm</td>\n",
       "      <td>negative</td>\n",
       "      <td>except my arm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3517</th>\n",
       "      <td>558fe1316b</td>\n",
       "      <td>On a 10min brake. At wrrkk ... its 11:06 and its over at...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>on a 10min brake. at wrrkk ... its 11:06 and its over a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>16f31448e1</td>\n",
       "      <td>so now that I have a whole 20 followers, 5 of them actua...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>so now that i have a whole 20 followers, 5 of them actu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>2452a0aa6a</td>\n",
       "      <td>i know they are delicious.miss germany but like holland...</td>\n",
       "      <td>positive</td>\n",
       "      <td>delicious.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>47617218fe</td>\n",
       "      <td>Hooray!</td>\n",
       "      <td>positive</td>\n",
       "      <td>hooray!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>53eacec2ef</td>\n",
       "      <td>Instrumentalists like to give singers a hard time. But ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>hard time. but the vocalist`s job is the toughest when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>5225c062f9</td>\n",
       "      <td>We knew what what you meant!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>we knew what what you meant!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2407</th>\n",
       "      <td>d8d52d2cb2</td>\n",
       "      <td>I`m loving Please Don`t Leave Me by Pink!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i`m loving please don`t leave me by pink!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>ad94e8b662</td>\n",
       "      <td>Hi Heather. I dont know why... but I just saw your mess...</td>\n",
       "      <td>negative</td>\n",
       "      <td>have not been very twitter friendly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>54670ca807</td>\n",
       "      <td>skype call with billie but my webcam dont work</td>\n",
       "      <td>neutral</td>\n",
       "      <td>skype call with billie but my webcam dont work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>31dbbd2bb4</td>\n",
       "      <td>Ha! Did she find out? The only thing Google came up wit...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ha! did she find out? the only thing google came up wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>746d87f93e</td>\n",
       "      <td>Should have left car and walked home! I might need someo...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>should have left car and walked home! i might need some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>19de569235</td>\n",
       "      <td>went twitter crazy! last nigh :p well thats what happens...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>went twitter crazy! last nigh :p well thats what happen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>3d128b58d0</td>\n",
       "      <td>Your birthday? WELL Happy birthday</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy birthday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>9e4228219d</td>\n",
       "      <td>me too!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>me too!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>5ce968f9be</td>\n",
       "      <td>Nice!!!!!!!! I WILL be there</td>\n",
       "      <td>positive</td>\n",
       "      <td>nice!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>c4f4b37c8b</td>\n",
       "      <td>very very bad headache that is getting worse by the minu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>very very bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>3ba8596881</td>\n",
       "      <td>Im so angry right now .. today im not doind nothing and ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>im so angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>ed0ace22bf</td>\n",
       "      <td>u don`t have school today????</td>\n",
       "      <td>neutral</td>\n",
       "      <td>u don`t have school today????</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>666f836b32</td>\n",
       "      <td>_rockstar umm how bout em?..aint get 2 see da game  but ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>_rockstar umm how bout em?..aint get 2 see da game but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>ddfc2cc8b3</td>\n",
       "      <td>Planing on going on a little ride on the mtb on monday  ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>planing on going on a little ride on the mtb on monday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>8a768f4305</td>\n",
       "      <td>un cross them please..I was planning on buying a lambo ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>un cross them please..i was planning on buying a lambo ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "992   472c3e2c41  Getting somewhere with my first 'real' KiokuDB and catal...   \n",
       "24    8be365118e                         you are lame  go make me breakfast!!   \n",
       "741   15892f296d  wishes that you would see not with your eyes but with yo...   \n",
       "1216  65e715d6e5   I`ve been there.  The only place I have flown out of si...   \n",
       "2245  f50fcc36b5   i love you so much tay (: youre so amazing <3 you shoul...   \n",
       "692   ce4b5fe266  `s current goal is to lose 10 pounds by next tuesday! 12...   \n",
       "1424  41ff25c568  This is how much hair falls off everytime I shower. It`s...   \n",
       "1522  8d4c510735   I hope you didn`t take that quote personally!! I`ve bee...   \n",
       "2577  bf580a287e                          All ready for work... except my arm   \n",
       "3517  558fe1316b  On a 10min brake. At wrrkk ... its 11:06 and its over at...   \n",
       "391   16f31448e1  so now that I have a whole 20 followers, 5 of them actua...   \n",
       "278   2452a0aa6a   i know they are delicious.miss germany but like holland...   \n",
       "2453  47617218fe                                                      Hooray!   \n",
       "1298  53eacec2ef   Instrumentalists like to give singers a hard time. But ...   \n",
       "1002  5225c062f9                                 We knew what what you meant!   \n",
       "2407  d8d52d2cb2                    I`m loving Please Don`t Leave Me by Pink!   \n",
       "497   ad94e8b662   Hi Heather. I dont know why... but I just saw your mess...   \n",
       "101   54670ca807               skype call with billie but my webcam dont work   \n",
       "585   31dbbd2bb4   Ha! Did she find out? The only thing Google came up wit...   \n",
       "42    746d87f93e  Should have left car and walked home! I might need someo...   \n",
       "458   19de569235  went twitter crazy! last nigh :p well thats what happens...   \n",
       "965   3d128b58d0                           Your birthday? WELL Happy birthday   \n",
       "912   9e4228219d                                                      me too!   \n",
       "414   5ce968f9be                                 Nice!!!!!!!! I WILL be there   \n",
       "3084  c4f4b37c8b  very very bad headache that is getting worse by the minu...   \n",
       "1958  3ba8596881  Im so angry right now .. today im not doind nothing and ...   \n",
       "680   ed0ace22bf                                u don`t have school today????   \n",
       "1589  666f836b32  _rockstar umm how bout em?..aint get 2 see da game  but ...   \n",
       "754   ddfc2cc8b3  Planing on going on a little ride on the mtb on monday  ...   \n",
       "2508  8a768f4305   un cross them please..I was planning on buying a lambo ...   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "992    neutral   getting somewhere with my first 'real' kiokudb and cata...  \n",
       "24    negative                                                         lame  \n",
       "741   positive                                                       wishes  \n",
       "1216   neutral   i`ve been there. the only place i have flown out of sin...  \n",
       "2245  positive                   i love you so much tay (: youre so amazing  \n",
       "692   positive                                                  soooo ready  \n",
       "1424  negative                                it`s more sad than disgusting  \n",
       "1522  negative               i hope you didn`t take that quote personally!!  \n",
       "2577  negative                                                except my arm  \n",
       "3517   neutral   on a 10min brake. at wrrkk ... its 11:06 and its over a...  \n",
       "391    neutral   so now that i have a whole 20 followers, 5 of them actu...  \n",
       "278   positive                                                   delicious.  \n",
       "2453  positive                                                      hooray!  \n",
       "1298  negative   hard time. but the vocalist`s job is the toughest when ...  \n",
       "1002   neutral                                 we knew what what you meant!  \n",
       "2407   neutral                    i`m loving please don`t leave me by pink!  \n",
       "497   negative                       have not been very twitter friendly...  \n",
       "101    neutral               skype call with billie but my webcam dont work  \n",
       "585    neutral   ha! did she find out? the only thing google came up wit...  \n",
       "42     neutral   should have left car and walked home! i might need some...  \n",
       "458    neutral   went twitter crazy! last nigh :p well thats what happen...  \n",
       "965   positive                                               happy birthday  \n",
       "912    neutral                                                      me too!  \n",
       "414   positive                                                 nice!!!!!!!!  \n",
       "3084  negative                                                very very bad  \n",
       "1958  negative                                                  im so angry  \n",
       "680    neutral                                u don`t have school today????  \n",
       "1589   neutral   _rockstar umm how bout em?..aint get 2 see da game but ...  \n",
       "754    neutral   planing on going on a little ride on the mtb on monday ...  \n",
       "2508   neutral   un cross them please..i was planning on buying a lambo ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
