{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preface**\n",
    "**This project was inspired and referenced in [TensorFlow roBERTa - [0.705]](https://www.kaggle.com/code/cdeotte/tensorflow-roberta-0-705)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1/ About team**\n",
    "|StuID  |        Name       |     Kaggle    |       Github      |\n",
    "|-------|-------------------|---------------|-------------------|\n",
    "|1752052|Dang Huu Phuoc Vinh|[V_Notebook](https://www.kaggle.com/danghuuphuocvinh)|[V_Github](https://github.com/DangHuuPhuocVinh/data_science_application)\n",
    "|1753097|Le Nguyen Minh Tam |[T_Notebook](https://www.kaggle.com/minhtamlenguyen)|[T_Github](https://github.com/lnmtam1999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2/ About competition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1/ Name of competition**\n",
    "**[Tweet Sentiment Extraction](https://www.kaggle.com/competitions/tweet-sentiment-extraction) organized by [Kaggle](https://www.kaggle.com/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2/ Prize**\n",
    "**15000 USD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.3/ Description**\n",
    "  **E.g: \"My ridiculous dog is amazing.\" [sentiment: positive]**\n",
    "\n",
    "  **With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.**\n",
    "\n",
    "  **In this competition we've extracted support phrases from [Figure Eight's Data for Everyone platform](https://appen.com/datasets-resource-center/). The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.4/ Input and Output**\n",
    "- **Input: textID, text and sentiment**\n",
    "- **Output: selected_text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.5/ Columns**\n",
    "-  **textID - unique ID for each piece of text**\n",
    "-  **text - the text of the tweet**\n",
    "-  **sentiment - the general sentiment of the tweet**\n",
    "-  **selected_text - [train only] the text that supports the tweet's sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.6/ Meaningful**\n",
    "- **After doing this project, we can have a dataset with the phrases that were selected for using at any other NLP project**\n",
    "- **Can use for detecting some keywords that have sentiment** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.7/ Evaluation**\n",
    " ![img](https://user-images.githubusercontent.com/35680794/174698744-57b2f116-fbe4-4fb6-9216-2e83e0494dca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3/ Model roBERTa for this project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4/ Developing the project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1/ Import Libraries, Data and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:37.249683Z",
     "iopub.status.busy": "2022-07-10T19:03:37.249400Z",
     "iopub.status.idle": "2022-07-10T19:03:37.256307Z",
     "shell.execute_reply": "2022-07-10T19:03:37.255411Z",
     "shell.execute_reply.started": "2022-07-10T19:03:37.249641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1.1/ Tokenizer**\n",
    "First of all, we use tokenizer to convert the word to array for the computer can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:40.525850Z",
     "iopub.status.busy": "2022-07-10T19:03:40.525552Z",
     "iopub.status.idle": "2022-07-10T19:03:40.902687Z",
     "shell.execute_reply": "2022-07-10T19:03:40.901896Z",
     "shell.execute_reply.started": "2022-07-10T19:03:40.525819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 98\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAX_LEN** = 96 then for each training row, RoBERTa receives 96 tokens. The reason to use **lowercase** and **add_prefix_space** because when spelling with RoBERTa :\" helllo\", \"hello\", \" Hello\", and \"Hello\" use the same \" hello\" token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2/ Training data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.1/ Adjust the input**\n",
    "In this stage we ready our data for the model, all the input will be change to numerical and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:44.266109Z",
     "iopub.status.busy": "2022-07-10T19:03:44.265840Z",
     "iopub.status.idle": "2022-07-10T19:03:53.105947Z",
     "shell.execute_reply": "2022-07-10T19:03:53.105081Z",
     "shell.execute_reply.started": "2022-07-10T19:03:44.266080Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[    0   939 12905 ...     1     1     1]\n",
      " [    0    98  3036 ...     1     1     1]\n",
      " [    0   127  3504 ...     1     1     1]\n",
      " ...\n",
      " [    0  1423   857 ...     1     1     1]\n",
      " [    0    53    24 ...     1     1     1]\n",
      " [    0    70    42 ...     1     1     1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [(0, 4), (4, 9), (9, 12), (12, 18), (18, 24), (24, 27), (27, 29), (29, 33), (33, 36), (36, 37), (37, 44), (44, 45), (45, 47), (47, 49), (49, 50), (50, 53), (53, 54), (54, 57), (57, 59)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3/ Test Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same as the Training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:04:17.944124Z",
     "iopub.status.busy": "2022-07-10T19:04:17.943856Z",
     "iopub.status.idle": "2022-07-10T19:04:18.241347Z",
     "shell.execute_reply": "2022-07-10T19:04:18.240640Z",
     "shell.execute_reply.started": "2022-07-10T19:04:17.944095Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the original text is [ i][ am][ having][ a][ great][ day]</s>[ positive]</s> with tokens 0,1,2,3,4,5,6,7,8,9,10 and the selected text is \"great day\", then the training has start index = 5 and end index = 6.\n",
    "\n",
    "If our model also predicts a = 5 and b = 6 and we try to select the text from [ i][ am][ having][ a][ great][ day], the indices 5 and 6 will not return \"great day\". Instead we must use tokens[4:6] to get great day. We subtract 1 because the is now removed. And we add 1 to b because python indexing for list[3:5] does not return 5 it only returns 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4/ Build roBERTa model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4.1/ Building the bones of model**\n",
    "Built the bones of a roBERTa model, using the model has already train by the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:05:21.352602Z",
     "iopub.status.busy": "2022-07-10T19:05:21.352259Z",
     "iopub.status.idle": "2022-07-10T19:05:21.364895Z",
     "shell.execute_reply": "2022-07-10T19:05:21.364038Z",
     "shell.execute_reply.started": "2022-07-10T19:05:21.352569Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5/ Create metric**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We code this for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:06:16.388950Z",
     "iopub.status.busy": "2022-07-10T19:06:16.388632Z",
     "iopub.status.idle": "2022-07-10T19:06:16.395186Z",
     "shell.execute_reply": "2022-07-10T19:06:16.394374Z",
     "shell.execute_reply.started": "2022-07-10T19:06:16.388923Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.6/ Train roBERTa model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:07:44.465971Z",
     "iopub.status.busy": "2022-07-10T19:07:44.465660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.1183 - activation_loss: 1.0636 - activation_1_loss: 1.0547\n",
      "Epoch 00001: val_loss improved from inf to 1.71663, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 293s 13ms/sample - loss: 2.1181 - activation_loss: 1.0634 - activation_1_loss: 1.0547 - val_loss: 1.7166 - val_activation_loss: 0.8985 - val_activation_1_loss: 0.8180\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.6060 - activation_loss: 0.8281 - activation_1_loss: 0.7779\n",
      "Epoch 00002: val_loss improved from 1.71663 to 1.65358, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 277s 13ms/sample - loss: 1.6061 - activation_loss: 0.8281 - activation_1_loss: 0.7780 - val_loss: 1.6536 - val_activation_loss: 0.8596 - val_activation_1_loss: 0.7937\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.4755 - activation_loss: 0.7635 - activation_1_loss: 0.7120\n",
      "Epoch 00003: val_loss did not improve from 1.65358\n",
      "21984/21984 [==============================] - 275s 13ms/sample - loss: 1.4756 - activation_loss: 0.7634 - activation_1_loss: 0.7122 - val_loss: 1.7041 - val_activation_loss: 0.8860 - val_activation_1_loss: 0.8179\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.7039994406036497\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1969 - activation_loss: 1.1038 - activation_1_loss: 1.0931\n",
      "Epoch 00001: val_loss improved from inf to 1.64062, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 309s 14ms/sample - loss: 2.1969 - activation_loss: 1.1043 - activation_1_loss: 1.0930 - val_loss: 1.6406 - val_activation_loss: 0.8496 - val_activation_1_loss: 0.7903\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6335 - activation_loss: 0.8463 - activation_1_loss: 0.7872\n",
      "Epoch 00002: val_loss improved from 1.64062 to 1.60299, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 290s 13ms/sample - loss: 1.6335 - activation_loss: 0.8451 - activation_1_loss: 0.7875 - val_loss: 1.6030 - val_activation_loss: 0.8319 - val_activation_1_loss: 0.7705\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4887 - activation_loss: 0.7749 - activation_1_loss: 0.7138\n",
      "Epoch 00003: val_loss did not improve from 1.60299\n",
      "21985/21985 [==============================] - 289s 13ms/sample - loss: 1.4888 - activation_loss: 0.7769 - activation_1_loss: 0.7142 - val_loss: 1.6510 - val_activation_loss: 0.8478 - val_activation_1_loss: 0.8026\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.7098360414815998\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2915 - activation_loss: 1.1387 - activation_1_loss: 1.1528\n",
      "Epoch 00001: val_loss improved from inf to 1.71735, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 310s 14ms/sample - loss: 2.2915 - activation_loss: 1.1389 - activation_1_loss: 1.1520 - val_loss: 1.7173 - val_activation_loss: 0.8747 - val_activation_1_loss: 0.8428\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.7216 - activation_loss: 0.8668 - activation_1_loss: 0.8547\n",
      "Epoch 00002: val_loss improved from 1.71735 to 1.66252, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 290s 13ms/sample - loss: 1.7216 - activation_loss: 0.8675 - activation_1_loss: 0.8564 - val_loss: 1.6625 - val_activation_loss: 0.8506 - val_activation_1_loss: 0.8121\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6098 - activation_loss: 0.8123 - activation_1_loss: 0.7976\n",
      "Epoch 00003: val_loss did not improve from 1.66252\n",
      "21985/21985 [==============================] - 289s 13ms/sample - loss: 1.6098 - activation_loss: 0.8131 - activation_1_loss: 0.7978 - val_loss: 1.7068 - val_activation_loss: 0.8684 - val_activation_1_loss: 0.8386\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 3 Jaccard = 0.7011620582073523\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2176 - activation_loss: 1.1033 - activation_1_loss: 1.1143\n",
      "Epoch 00001: val_loss improved from inf to 1.65338, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 310s 14ms/sample - loss: 2.2176 - activation_loss: 1.1018 - activation_1_loss: 1.1156 - val_loss: 1.6534 - val_activation_loss: 0.8544 - val_activation_1_loss: 0.7989\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6489 - activation_loss: 0.8518 - activation_1_loss: 0.7971\n",
      "Epoch 00002: val_loss improved from 1.65338 to 1.60904, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 290s 13ms/sample - loss: 1.6489 - activation_loss: 0.8507 - activation_1_loss: 0.7990 - val_loss: 1.6090 - val_activation_loss: 0.8245 - val_activation_1_loss: 0.7844\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5081 - activation_loss: 0.7789 - activation_1_loss: 0.7292\n",
      "Epoch 00003: val_loss did not improve from 1.60904\n",
      "21985/21985 [==============================] - 288s 13ms/sample - loss: 1.5080 - activation_loss: 0.7782 - activation_1_loss: 0.7283 - val_loss: 1.6219 - val_activation_loss: 0.8352 - val_activation_1_loss: 0.7868\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 4 Jaccard = 0.708248811576504\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2472 - activation_loss: 1.0903 - activation_1_loss: 1.1568\n",
      "Epoch 00001: val_loss improved from inf to 1.74644, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 309s 14ms/sample - loss: 2.2472 - activation_loss: 1.0905 - activation_1_loss: 1.1572 - val_loss: 1.7464 - val_activation_loss: 0.8774 - val_activation_1_loss: 0.8685\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6951 - activation_loss: 0.8605 - activation_1_loss: 0.8345\n",
      "Epoch 00002: val_loss improved from 1.74644 to 1.62032, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 290s 13ms/sample - loss: 1.6950 - activation_loss: 0.8599 - activation_1_loss: 0.8334 - val_loss: 1.6203 - val_activation_loss: 0.8416 - val_activation_1_loss: 0.7777\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5374 - activation_loss: 0.7924 - activation_1_loss: 0.7450\n",
      "Epoch 00003: val_loss improved from 1.62032 to 1.60319, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 290s 13ms/sample - loss: 1.5374 - activation_loss: 0.7920 - activation_1_loss: 0.7444 - val_loss: 1.6032 - val_activation_loss: 0.8406 - val_activation_1_loss: 0.7619\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 5 Jaccard = 0.7113469160887076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.7/ Kaggle submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>cb4adb2254</td>\n",
       "      <td>I wanna do something tonight after work.... But I dunno ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i wanna do something tonight after work.... but i dunno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>582e5d7420</td>\n",
       "      <td>Pulled in all directions and not knowing where to go</td>\n",
       "      <td>negative</td>\n",
       "      <td>not knowing where to go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>799e17e7a7</td>\n",
       "      <td>it`s nice to leave the office when the sun is still up</td>\n",
       "      <td>positive</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>898c63c69f</td>\n",
       "      <td>I`ve never been to Prague, but if i had the money, it`l...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i`ve never been to prague, but if i had the money, it`l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3349</th>\n",
       "      <td>a9b155d544</td>\n",
       "      <td>live chat soooon most likely.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>live chat soooon most likely.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>0e4bcb3a52</td>\n",
       "      <td>Happy Mother`s day VALK!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>afd2159e57</td>\n",
       "      <td>thanks for a nice blog post!  should however be given s...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>b3cf47c9c0</td>\n",
       "      <td>That is so sweet!! Have a good day</td>\n",
       "      <td>positive</td>\n",
       "      <td>that is so sweet!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>59f6677324</td>\n",
       "      <td>The Heater blew up</td>\n",
       "      <td>negative</td>\n",
       "      <td>blew up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>74676aaaed</td>\n",
       "      <td>So full from a great lunch and now stuck in traffic</td>\n",
       "      <td>neutral</td>\n",
       "      <td>so full from a great lunch and now stuck in traffic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>2ae6fffca5</td>\n",
       "      <td>Good Morning campers, I`m not a happy bear this morning,...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good morning campers, i`m not a happy bear this morning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>213f65851d</td>\n",
       "      <td>Day is going well so far. Meeting until 4 though.</td>\n",
       "      <td>positive</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>e0d893b360</td>\n",
       "      <td>oh and twas my very brilliant idea if I do say so mysel...</td>\n",
       "      <td>positive</td>\n",
       "      <td>brilliant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>d7607b8133</td>\n",
       "      <td>i wear a lot of white  http://tinyurl.com/dlbltg</td>\n",
       "      <td>negative</td>\n",
       "      <td>i wear a lot of white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>97eeb0161d</td>\n",
       "      <td>Wow, its hot and miserable. People are probably killing ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>miserable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3012</th>\n",
       "      <td>9f89432794</td>\n",
       "      <td>Well, looks like it is going to be another night without...</td>\n",
       "      <td>negative</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2721</th>\n",
       "      <td>63d6ff064f</td>\n",
       "      <td>after 11 months....back to twitter again</td>\n",
       "      <td>neutral</td>\n",
       "      <td>after 11 months....back to twitter again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>458f7b2493</td>\n",
       "      <td>finally feel back into swing of things here at work afte...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sigh of relief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>2813c4cf7e</td>\n",
       "      <td>Don`t you hate it when you`re left with one slice of bre...</td>\n",
       "      <td>negative</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>bbe85e3495</td>\n",
       "      <td>missin the #ia2009 guys really</td>\n",
       "      <td>neutral</td>\n",
       "      <td>missin the #ia2009 guys really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>b9b532f7a2</td>\n",
       "      <td>U were great, as always. But, can`t we do an east Germa...</td>\n",
       "      <td>positive</td>\n",
       "      <td>great,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>440e8fc447</td>\n",
       "      <td>Hockey was so fukinï¿½ good  **** you hole! xD</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hockey was so fukinï¿½ good **** you hole! xd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>b1999f0951</td>\n",
       "      <td>Now I have a sunburn</td>\n",
       "      <td>negative</td>\n",
       "      <td>sunburn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>401cc22f49</td>\n",
       "      <td>is on facebook</td>\n",
       "      <td>neutral</td>\n",
       "      <td>is on facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>8df42cd45a</td>\n",
       "      <td>FOLLOW ME BABY U SAID N DA CHAT U WILL  LUV YA GOODNITE</td>\n",
       "      <td>positive</td>\n",
       "      <td>luv ya goodnite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>a0b1828b67</td>\n",
       "      <td>'Brides a la mode' pow wow first thing this morning   Th...</td>\n",
       "      <td>positive</td>\n",
       "      <td>lovely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234</th>\n",
       "      <td>1f2445122f</td>\n",
       "      <td>Cleaning all day today after he runs off to do his stuff...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i feel much better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>b6cd695d48</td>\n",
       "      <td>Working... And attempting to keep squirrels away...........</td>\n",
       "      <td>negative</td>\n",
       "      <td>i should stick to spiders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3201</th>\n",
       "      <td>db4bb26337</td>\n",
       "      <td>Sensation Ocean of White Portugal: absolutely amazing  ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>absolutely amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>b93d5198af</td>\n",
       "      <td>so far I`ve tried to kill myself TWICE in the 15 mins I`...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stupid things.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "2374  cb4adb2254  I wanna do something tonight after work.... But I dunno ...   \n",
       "2231  582e5d7420         Pulled in all directions and not knowing where to go   \n",
       "180   799e17e7a7       it`s nice to leave the office when the sun is still up   \n",
       "902   898c63c69f   I`ve never been to Prague, but if i had the money, it`l...   \n",
       "3349  a9b155d544                                live chat soooon most likely.   \n",
       "2195  0e4bcb3a52                                    Happy Mother`s day VALK!!   \n",
       "2242  afd2159e57   thanks for a nice blog post!  should however be given s...   \n",
       "2606  b3cf47c9c0                           That is so sweet!! Have a good day   \n",
       "1048  59f6677324                                           The Heater blew up   \n",
       "3082  74676aaaed          So full from a great lunch and now stuck in traffic   \n",
       "2515  2ae6fffca5  Good Morning campers, I`m not a happy bear this morning,...   \n",
       "3314  213f65851d            Day is going well so far. Meeting until 4 though.   \n",
       "1463  e0d893b360   oh and twas my very brilliant idea if I do say so mysel...   \n",
       "713   d7607b8133             i wear a lot of white  http://tinyurl.com/dlbltg   \n",
       "1073  97eeb0161d  Wow, its hot and miserable. People are probably killing ...   \n",
       "3012  9f89432794  Well, looks like it is going to be another night without...   \n",
       "2721  63d6ff064f                     after 11 months....back to twitter again   \n",
       "955   458f7b2493  finally feel back into swing of things here at work afte...   \n",
       "492   2813c4cf7e  Don`t you hate it when you`re left with one slice of bre...   \n",
       "1437  bbe85e3495                               missin the #ia2009 guys really   \n",
       "3034  b9b532f7a2   U were great, as always. But, can`t we do an east Germa...   \n",
       "1730  440e8fc447               Hockey was so fukinï¿½ good  **** you hole! xD   \n",
       "135   b1999f0951                                         Now I have a sunburn   \n",
       "420   401cc22f49                                               is on facebook   \n",
       "468   8df42cd45a      FOLLOW ME BABY U SAID N DA CHAT U WILL  LUV YA GOODNITE   \n",
       "826   a0b1828b67  'Brides a la mode' pow wow first thing this morning   Th...   \n",
       "2234  1f2445122f  Cleaning all day today after he runs off to do his stuff...   \n",
       "2274  b6cd695d48  Working... And attempting to keep squirrels away...........   \n",
       "3201  db4bb26337   Sensation Ocean of White Portugal: absolutely amazing  ...   \n",
       "3363  b93d5198af  so far I`ve tried to kill myself TWICE in the 15 mins I`...   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "2374   neutral   i wanna do something tonight after work.... but i dunno...  \n",
       "2231  negative                                      not knowing where to go  \n",
       "180   positive                                                         nice  \n",
       "902   positive   i`ve never been to prague, but if i had the money, it`l...  \n",
       "3349   neutral                                live chat soooon most likely.  \n",
       "2195  positive                                                        happy  \n",
       "2242  positive                                                       thanks  \n",
       "2606  positive                                           that is so sweet!!  \n",
       "1048  negative                                                      blew up  \n",
       "3082   neutral          so full from a great lunch and now stuck in traffic  \n",
       "2515   neutral   good morning campers, i`m not a happy bear this morning...  \n",
       "3314  positive                                                         well  \n",
       "1463  positive                                                    brilliant  \n",
       "713   negative                                        i wear a lot of white  \n",
       "1073  negative                                                   miserable.  \n",
       "3012  negative                                                      missing  \n",
       "2721   neutral                     after 11 months....back to twitter again  \n",
       "955   positive                                               sigh of relief  \n",
       "492   negative                                                         hate  \n",
       "1437   neutral                               missin the #ia2009 guys really  \n",
       "3034  positive                                                       great,  \n",
       "1730   neutral                hockey was so fukinï¿½ good **** you hole! xd  \n",
       "135   negative                                                      sunburn  \n",
       "420    neutral                                               is on facebook  \n",
       "468   positive                                              luv ya goodnite  \n",
       "826   positive                                                       lovely  \n",
       "2234  positive                                           i feel much better  \n",
       "2274  negative                                    i should stick to spiders  \n",
       "3201  positive                                           absolutely amazing  \n",
       "3363  negative                                               stupid things.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
