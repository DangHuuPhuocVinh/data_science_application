{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preface**\n",
    "**This project was inspired and referenced in [TensorFlow roBERTa - [0.705]](https://www.kaggle.com/code/cdeotte/tensorflow-roberta-0-705)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1/ About team**\n",
    "|StuID  |        Name       |     Kaggle    |       Github      |\n",
    "|-------|-------------------|---------------|-------------------|\n",
    "|1752052|Dang Huu Phuoc Vinh|[V_Notebook](https://www.kaggle.com/danghuuphuocvinh)|[V_Github](https://github.com/DangHuuPhuocVinh/data_science_application)\n",
    "|1753097|Le Nguyen Minh Tam |[T_Notebook](https://www.kaggle.com/minhtamlenguyen)|[T_Github](https://github.com/lnmtam1999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2/ About competition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1/ Name of competition**\n",
    "**[Tweet Sentiment Extraction](https://www.kaggle.com/competitions/tweet-sentiment-extraction) organized by [Kaggle](https://www.kaggle.com/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2/ Prize**\n",
    "**15000 USD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.3/ Description**\n",
    "  **E.g: \"My ridiculous dog is amazing.\" [sentiment: positive]**\n",
    "\n",
    "  **With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.**\n",
    "\n",
    "  **In this competition we've extracted support phrases from [Figure Eight's Data for Everyone platform](https://appen.com/datasets-resource-center/). The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.4/ Input and Output**\n",
    "- **Input: textID, text and sentiment**\n",
    "- **Output: selected_text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.5/ Columns**\n",
    "-  **textID - unique ID for each piece of text**\n",
    "-  **text - the text of the tweet**\n",
    "-  **sentiment - the general sentiment of the tweet**\n",
    "-  **selected_text - [train only] the text that supports the tweet's sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.6/ Meaningful**\n",
    "- **After doing this project, we can have a dataset with the phrases that were selected for using at any other NLP project**\n",
    "- **Can use for detecting some keywords that have sentiment** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.7/ Evaluation**\n",
    " ![img](https://user-images.githubusercontent.com/35680794/174698744-57b2f116-fbe4-4fb6-9216-2e83e0494dca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3/ Model roBERTa for this project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4/ Developing the project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1/ Import Libraries, Data and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:37.249683Z",
     "iopub.status.busy": "2022-07-10T19:03:37.249400Z",
     "iopub.status.idle": "2022-07-10T19:03:37.256307Z",
     "shell.execute_reply": "2022-07-10T19:03:37.255411Z",
     "shell.execute_reply.started": "2022-07-10T19:03:37.249641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1.1/ Tokenizer**\n",
    "First of all, we use tokenizer to convert the word to array for the computer can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:40.525850Z",
     "iopub.status.busy": "2022-07-10T19:03:40.525552Z",
     "iopub.status.idle": "2022-07-10T19:03:40.902687Z",
     "shell.execute_reply": "2022-07-10T19:03:40.901896Z",
     "shell.execute_reply.started": "2022-07-10T19:03:40.525819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 93\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAX_LEN** = 96 then for each training row, RoBERTa receives 96 tokens. The reason to use **lowercase** and **add_prefix_space** because when spelling with RoBERTa :\" helllo\", \"hello\", \" Hello\", and \"Hello\" use the same \" hello\" token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2/ Training data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.1/ Adjust the input**\n",
    "In this stage we ready our data for the model, all the input will be change to numerical and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:44.266109Z",
     "iopub.status.busy": "2022-07-10T19:03:44.265840Z",
     "iopub.status.idle": "2022-07-10T19:03:53.105947Z",
     "shell.execute_reply": "2022-07-10T19:03:53.105081Z",
     "shell.execute_reply.started": "2022-07-10T19:03:44.266080Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[    0   939 12905 ...     1     1     1]\n",
      " [    0    98  3036 ...     1     1     1]\n",
      " [    0   127  3504 ...     1     1     1]\n",
      " ...\n",
      " [    0  1423   857 ...     1     1     1]\n",
      " [    0    53    24 ...     1     1     1]\n",
      " [    0    70    42 ...     1     1     1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [(0, 4), (4, 9), (9, 12), (12, 18), (18, 24), (24, 27), (27, 29), (29, 33), (33, 36), (36, 37), (37, 44), (44, 45), (45, 47), (47, 49), (49, 50), (50, 53), (53, 54), (54, 57), (57, 59)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3/ Test Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same as the Training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:04:17.944124Z",
     "iopub.status.busy": "2022-07-10T19:04:17.943856Z",
     "iopub.status.idle": "2022-07-10T19:04:18.241347Z",
     "shell.execute_reply": "2022-07-10T19:04:18.240640Z",
     "shell.execute_reply.started": "2022-07-10T19:04:17.944095Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the original text is [ i][ am][ having][ a][ great][ day]</s>[ positive]</s> with tokens 0,1,2,3,4,5,6,7,8,9,10 and the selected text is \"great day\", then the training has start index = 5 and end index = 6.\n",
    "\n",
    "If our model also predicts a = 5 and b = 6 and we try to select the text from [ i][ am][ having][ a][ great][ day], the indices 5 and 6 will not return \"great day\". Instead we must use tokens[4:6] to get great day. We subtract 1 because the is now removed. And we add 1 to b because python indexing for list[3:5] does not return 5 it only returns 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4/ Build roBERTa model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4.1/ Building the bones of model**\n",
    "Built the bones of a roBERTa model, using the model has already train by the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:05:21.352602Z",
     "iopub.status.busy": "2022-07-10T19:05:21.352259Z",
     "iopub.status.idle": "2022-07-10T19:05:21.364895Z",
     "shell.execute_reply": "2022-07-10T19:05:21.364038Z",
     "shell.execute_reply.started": "2022-07-10T19:05:21.352569Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5/ Create metric**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We code this for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:06:16.388950Z",
     "iopub.status.busy": "2022-07-10T19:06:16.388632Z",
     "iopub.status.idle": "2022-07-10T19:06:16.395186Z",
     "shell.execute_reply": "2022-07-10T19:06:16.394374Z",
     "shell.execute_reply.started": "2022-07-10T19:06:16.388923Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.6/ Train roBERTa model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:07:44.465971Z",
     "iopub.status.busy": "2022-07-10T19:07:44.465660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.1955 - activation_loss: 1.0888 - activation_1_loss: 1.1067\n",
      "Epoch 00001: val_loss improved from inf to 1.70292, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 291s 13ms/sample - loss: 2.1951 - activation_loss: 1.0884 - activation_1_loss: 1.1067 - val_loss: 1.7029 - val_activation_loss: 0.8726 - val_activation_1_loss: 0.8303\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.6369 - activation_loss: 0.8429 - activation_1_loss: 0.7940\n",
      "Epoch 00002: val_loss improved from 1.70292 to 1.65763, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 272s 12ms/sample - loss: 1.6367 - activation_loss: 0.8427 - activation_1_loss: 0.7940 - val_loss: 1.6576 - val_activation_loss: 0.8504 - val_activation_1_loss: 0.8072\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.4899 - activation_loss: 0.7698 - activation_1_loss: 0.7201\n",
      "Epoch 00003: val_loss did not improve from 1.65763\n",
      "21984/21984 [==============================] - 270s 12ms/sample - loss: 1.4893 - activation_loss: 0.7695 - activation_1_loss: 0.7198 - val_loss: 1.6601 - val_activation_loss: 0.8629 - val_activation_1_loss: 0.7966\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 28s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.7049456490629492\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1413 - activation_loss: 1.0789 - activation_1_loss: 1.0623\n",
      "Epoch 00001: val_loss improved from inf to 1.64970, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 308s 14ms/sample - loss: 2.1412 - activation_loss: 1.0777 - activation_1_loss: 1.0624 - val_loss: 1.6497 - val_activation_loss: 0.8426 - val_activation_1_loss: 0.8064\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6327 - activation_loss: 0.8425 - activation_1_loss: 0.7902\n",
      "Epoch 00002: val_loss did not improve from 1.64970\n",
      "21985/21985 [==============================] - 284s 13ms/sample - loss: 1.6328 - activation_loss: 0.8449 - activation_1_loss: 0.7913 - val_loss: 1.6542 - val_activation_loss: 0.8332 - val_activation_1_loss: 0.8205\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4934 - activation_loss: 0.7699 - activation_1_loss: 0.7235\n",
      "Epoch 00003: val_loss improved from 1.64970 to 1.63107, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 286s 13ms/sample - loss: 1.4934 - activation_loss: 0.7688 - activation_1_loss: 0.7225 - val_loss: 1.6311 - val_activation_loss: 0.8372 - val_activation_1_loss: 0.7933\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.6998653573005454\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1872 - activation_loss: 1.0722 - activation_1_loss: 1.1150\n",
      "Epoch 00001: val_loss improved from inf to 1.70186, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 308s 14ms/sample - loss: 2.1871 - activation_loss: 1.0706 - activation_1_loss: 1.1134 - val_loss: 1.7019 - val_activation_loss: 0.8634 - val_activation_1_loss: 0.8386\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6359 - activation_loss: 0.8444 - activation_1_loss: 0.7915\n",
      "Epoch 00002: val_loss improved from 1.70186 to 1.62034, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 285s 13ms/sample - loss: 1.6358 - activation_loss: 0.8432 - activation_1_loss: 0.7910 - val_loss: 1.6203 - val_activation_loss: 0.8286 - val_activation_1_loss: 0.7919\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4746 - activation_loss: 0.7648 - activation_1_loss: 0.7098\n",
      "Epoch 00003: val_loss did not improve from 1.62034\n",
      "21985/21985 [==============================] - 284s 13ms/sample - loss: 1.4745 - activation_loss: 0.7637 - activation_1_loss: 0.7088 - val_loss: 1.6318 - val_activation_loss: 0.8374 - val_activation_1_loss: 0.7948\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 3 Jaccard = 0.7039219522554268\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1817 - activation_loss: 1.0829 - activation_1_loss: 1.0988\n",
      "Epoch 00001: val_loss improved from inf to 1.73782, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 306s 14ms/sample - loss: 2.1816 - activation_loss: 1.0813 - activation_1_loss: 1.0972 - val_loss: 1.7378 - val_activation_loss: 0.8876 - val_activation_1_loss: 0.8503\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6603 - activation_loss: 0.8501 - activation_1_loss: 0.8102\n",
      "Epoch 00002: val_loss improved from 1.73782 to 1.64988, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 286s 13ms/sample - loss: 1.6603 - activation_loss: 0.8489 - activation_1_loss: 0.8124 - val_loss: 1.6499 - val_activation_loss: 0.8330 - val_activation_1_loss: 0.8170\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5422 - activation_loss: 0.7942 - activation_1_loss: 0.7480\n",
      "Epoch 00003: val_loss improved from 1.64988 to 1.62115, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 285s 13ms/sample - loss: 1.5422 - activation_loss: 0.7943 - activation_1_loss: 0.7478 - val_loss: 1.6212 - val_activation_loss: 0.8081 - val_activation_1_loss: 0.8128\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 28s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 4 Jaccard = 0.7055689229303901\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1716 - activation_loss: 1.0883 - activation_1_loss: 1.0833\n",
      "Epoch 00001: val_loss improved from inf to 1.64966, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 308s 14ms/sample - loss: 2.1717 - activation_loss: 1.0894 - activation_1_loss: 1.0843 - val_loss: 1.6497 - val_activation_loss: 0.8715 - val_activation_1_loss: 0.7774\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6429 - activation_loss: 0.8462 - activation_1_loss: 0.7967\n",
      "Epoch 00002: val_loss improved from 1.64966 to 1.57775, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 286s 13ms/sample - loss: 1.6429 - activation_loss: 0.8459 - activation_1_loss: 0.7956 - val_loss: 1.5778 - val_activation_loss: 0.8224 - val_activation_1_loss: 0.7546\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4897 - activation_loss: 0.7669 - activation_1_loss: 0.7228\n",
      "Epoch 00003: val_loss did not improve from 1.57775\n",
      "21985/21985 [==============================] - 284s 13ms/sample - loss: 1.4897 - activation_loss: 0.7658 - activation_1_loss: 0.7246 - val_loss: 1.6641 - val_activation_loss: 0.8772 - val_activation_1_loss: 0.7862\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 5 Jaccard = 0.7126204207447392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.7/ Kaggle submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>40d5393a6e</td>\n",
       "      <td>Signing off for the night. Watching 'The Reader' and I`m...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>signing off for the night. watching 'the reader' and i`...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>95db70d9c2</td>\n",
       "      <td>yup! way to **** early and I`m already at work</td>\n",
       "      <td>neutral</td>\n",
       "      <td>yup! way to **** early and i`m already at work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>c00890fdfc</td>\n",
       "      <td>i see you hannah klein!. lookin good today</td>\n",
       "      <td>positive</td>\n",
       "      <td>lookin good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>d9e1b10a4f</td>\n",
       "      <td>wow.. tomorrow and then it`s over. i`ll never see some o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>it`s kind of sad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>ace00f13fb</td>\n",
       "      <td>Just Saw Confessions Of A Shopoholic...Totally fell in l...</td>\n",
       "      <td>positive</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>4dda8fd71a</td>\n",
       "      <td>Headed to verizon. Praying that pinkberry is going to ma...</td>\n",
       "      <td>positive</td>\n",
       "      <td>praying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>7c5b5c6f96</td>\n",
       "      <td>_Jim I so much want to see the screen adaptation of Kick...</td>\n",
       "      <td>negative</td>\n",
       "      <td>my heart sank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>3ed1480471</td>\n",
       "      <td>wonders if anyone would care if she died tomorrow</td>\n",
       "      <td>negative</td>\n",
       "      <td>wonders if anyone would care if she died tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ae93ad52a0</td>\n",
       "      <td>So hot today =_=  don`t like it and i hate my new timeta...</td>\n",
       "      <td>negative</td>\n",
       "      <td>hate my new timetable, having such a bad week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0fe049b7ea</td>\n",
       "      <td>Someone out there will soon  be the lucky recipient of m...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>someone out there will soon be the lucky recipient of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663</th>\n",
       "      <td>896669ae51</td>\n",
       "      <td>I forget how much I miss my tribe til I have limited acc...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i forget how much i miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2785</th>\n",
       "      <td>a1b435d7b2</td>\n",
       "      <td>is obsessing over chris pine  heehee...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>is obsessing over chris pine heehee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>06b8d9e26c</td>\n",
       "      <td>welcome</td>\n",
       "      <td>positive</td>\n",
       "      <td>welcome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2213</th>\n",
       "      <td>5f455a3147</td>\n",
       "      <td>Flat tire city</td>\n",
       "      <td>neutral</td>\n",
       "      <td>flat tire city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>23ea783065</td>\n",
       "      <td>I really wish I could</td>\n",
       "      <td>positive</td>\n",
       "      <td>i really wish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>55c69c2d52</td>\n",
       "      <td>did he ask for your Twitter ID? Your sun sign?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>did he ask for your twitter id? your sun sign?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3488</th>\n",
       "      <td>27829d97bd</td>\n",
       "      <td>'sometime around midnight' by The Airborne Toxic Event, ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>literally cannot escape it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3404</th>\n",
       "      <td>b5eb308679</td>\n",
       "      <td>browsin thru the videos in my multiply and i saw the vid...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>fc5de3287d</td>\n",
       "      <td>: nooo  i don`t know why...i click on TweetDeck_0_25_ma...</td>\n",
       "      <td>negative</td>\n",
       "      <td>so sad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3356</th>\n",
       "      <td>4e1101e520</td>\n",
       "      <td>Its weird being at the guy`s house without them here. I ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i don`t like it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>e035bf28b6</td>\n",
       "      <td>1:30am goin to sleeeeep</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1:30am goin to sleeeeep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3044</th>\n",
       "      <td>700c41e7c0</td>\n",
       "      <td>AP: North Korea could opt for devastating land assault.....</td>\n",
       "      <td>negative</td>\n",
       "      <td>scary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>a04c8c8695</td>\n",
       "      <td>Why do I have to enter my registration details every ti...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>why do i have to enter my registration details every ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>1914dd82b2</td>\n",
       "      <td>He`s too old school for burnouts. Just a lap around the...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>he`s too old school for burnouts. just a lap around the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>fee936fdc3</td>\n",
       "      <td>wasn`t the 'layers' just absolute positioned divs? Web ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wasn`t the 'layers' just absolute positioned divs? web ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>24e58e264b</td>\n",
       "      <td>_carter THE LINK DOESNT WORK</td>\n",
       "      <td>neutral</td>\n",
       "      <td>_carter the link doesnt work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>d49d1b7de1</td>\n",
       "      <td>I am sorry that you are feeling that way.</td>\n",
       "      <td>negative</td>\n",
       "      <td>i am sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2581</th>\n",
       "      <td>b659d14173</td>\n",
       "      <td>i am SOOO mad...yesterday and today...SUCKED!!!so much!</td>\n",
       "      <td>negative</td>\n",
       "      <td>i am sooo mad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>e14bd46ac9</td>\n",
       "      <td>how sad was Hollyoaks</td>\n",
       "      <td>negative</td>\n",
       "      <td>how sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3322</th>\n",
       "      <td>68238c0940</td>\n",
       "      <td>My sister is having a stupid party &amp;&amp; I wanna hang out w...</td>\n",
       "      <td>negative</td>\n",
       "      <td>stupid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "2478  40d5393a6e  Signing off for the night. Watching 'The Reader' and I`m...   \n",
       "1121  95db70d9c2               yup! way to **** early and I`m already at work   \n",
       "1236  c00890fdfc                   i see you hannah klein!. lookin good today   \n",
       "635   d9e1b10a4f  wow.. tomorrow and then it`s over. i`ll never see some o...   \n",
       "2600  ace00f13fb  Just Saw Confessions Of A Shopoholic...Totally fell in l...   \n",
       "445   4dda8fd71a  Headed to verizon. Praying that pinkberry is going to ma...   \n",
       "2991  7c5b5c6f96  _Jim I so much want to see the screen adaptation of Kick...   \n",
       "3092  3ed1480471            wonders if anyone would care if she died tomorrow   \n",
       "15    ae93ad52a0  So hot today =_=  don`t like it and i hate my new timeta...   \n",
       "427   0fe049b7ea  Someone out there will soon  be the lucky recipient of m...   \n",
       "2663  896669ae51  I forget how much I miss my tribe til I have limited acc...   \n",
       "2785  a1b435d7b2                      is obsessing over chris pine  heehee...   \n",
       "1353  06b8d9e26c                                                      welcome   \n",
       "2213  5f455a3147                                               Flat tire city   \n",
       "1307  23ea783065                                        I really wish I could   \n",
       "33    55c69c2d52               did he ask for your Twitter ID? Your sun sign?   \n",
       "3488  27829d97bd  'sometime around midnight' by The Airborne Toxic Event, ...   \n",
       "3404  b5eb308679  browsin thru the videos in my multiply and i saw the vid...   \n",
       "2343  fc5de3287d   : nooo  i don`t know why...i click on TweetDeck_0_25_ma...   \n",
       "3356  4e1101e520  Its weird being at the guy`s house without them here. I ...   \n",
       "3383  e035bf28b6                                      1:30am goin to sleeeeep   \n",
       "3044  700c41e7c0  AP: North Korea could opt for devastating land assault.....   \n",
       "69    a04c8c8695   Why do I have to enter my registration details every ti...   \n",
       "1158  1914dd82b2   He`s too old school for burnouts. Just a lap around the...   \n",
       "491   fee936fdc3   wasn`t the 'layers' just absolute positioned divs? Web ...   \n",
       "1230  24e58e264b                                 _carter THE LINK DOESNT WORK   \n",
       "535   d49d1b7de1                    I am sorry that you are feeling that way.   \n",
       "2581  b659d14173      i am SOOO mad...yesterday and today...SUCKED!!!so much!   \n",
       "2494  e14bd46ac9                                        how sad was Hollyoaks   \n",
       "3322  68238c0940  My sister is having a stupid party && I wanna hang out w...   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "2478   neutral   signing off for the night. watching 'the reader' and i`...  \n",
       "1121   neutral               yup! way to **** early and i`m already at work  \n",
       "1236  positive                                                  lookin good  \n",
       "635   negative                                            it`s kind of sad.  \n",
       "2600  positive                                                         love  \n",
       "445   positive                                                      praying  \n",
       "2991  negative                                                my heart sank  \n",
       "3092  negative            wonders if anyone would care if she died tomorrow  \n",
       "15    negative                hate my new timetable, having such a bad week  \n",
       "427    neutral   someone out there will soon be the lucky recipient of m...  \n",
       "2663  positive                                     i forget how much i miss  \n",
       "2785   neutral                       is obsessing over chris pine heehee...  \n",
       "1353  positive                                                      welcome  \n",
       "2213   neutral                                               flat tire city  \n",
       "1307  positive                                                i really wish  \n",
       "33     neutral               did he ask for your twitter id? your sun sign?  \n",
       "3488  negative                                  literally cannot escape it.  \n",
       "3404  negative                                                       i miss  \n",
       "2343  negative                                                    so sad...  \n",
       "3356  negative                                              i don`t like it  \n",
       "3383   neutral                                      1:30am goin to sleeeeep  \n",
       "3044  negative                                                        scary  \n",
       "69     neutral   why do i have to enter my registration details every ti...  \n",
       "1158   neutral   he`s too old school for burnouts. just a lap around the...  \n",
       "491    neutral   wasn`t the 'layers' just absolute positioned divs? web ...  \n",
       "1230   neutral                                 _carter the link doesnt work  \n",
       "535   negative                                                   i am sorry  \n",
       "2581  negative                                             i am sooo mad...  \n",
       "2494  negative                                                      how sad  \n",
       "3322  negative                                                       stupid  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
