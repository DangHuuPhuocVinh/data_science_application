{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preface**\n",
    "**This project was inspired and referenced in [TensorFlow roBERTa - [0.705]](https://www.kaggle.com/code/cdeotte/tensorflow-roberta-0-705)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1/ About team**\n",
    "|StuID  |        Name       |     Kaggle    |       Github      |\n",
    "|-------|-------------------|---------------|-------------------|\n",
    "|1752052|Dang Huu Phuoc Vinh|[V_Notebook](https://www.kaggle.com/danghuuphuocvinh)|[V_Github](https://github.com/DangHuuPhuocVinh/data_science_application)\n",
    "|1753097|Le Nguyen Minh Tam |[T_Notebook](https://www.kaggle.com/minhtamlenguyen)|[T_Github](https://github.com/lnmtam1999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2/ About competition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1/ Name of competition**\n",
    "**[Tweet Sentiment Extraction](https://www.kaggle.com/competitions/tweet-sentiment-extraction) organized by [Kaggle](https://www.kaggle.com/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2/ Prize**\n",
    "**15000 USD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.3/ Description**\n",
    "  **E.g: \"My ridiculous dog is amazing.\" [sentiment: positive]**\n",
    "\n",
    "  **With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.**\n",
    "\n",
    "  **In this competition we've extracted support phrases from [Figure Eight's Data for Everyone platform](https://appen.com/datasets-resource-center/). The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.4/ Input and Output**\n",
    "- **Input: textID, text and sentiment**\n",
    "- **Output: selected_text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.5/ Columns**\n",
    "-  **textID - unique ID for each piece of text**\n",
    "-  **text - the text of the tweet**\n",
    "-  **sentiment - the general sentiment of the tweet**\n",
    "-  **selected_text - [train only] the text that supports the tweet's sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.6/ Meaningful**\n",
    "- **After doing this project, we can have a dataset with the phrases that were selected for using at any other NLP project**\n",
    "- **Can use for detecting some keywords that have sentiment** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.7/ Evaluation**\n",
    " ![img](https://user-images.githubusercontent.com/35680794/174698744-57b2f116-fbe4-4fb6-9216-2e83e0494dca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3/ Model roBERTa for this project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4/ Developing the project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1/ Import Libraries, Data and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:37.249683Z",
     "iopub.status.busy": "2022-07-10T19:03:37.249400Z",
     "iopub.status.idle": "2022-07-10T19:03:37.256307Z",
     "shell.execute_reply": "2022-07-10T19:03:37.255411Z",
     "shell.execute_reply.started": "2022-07-10T19:03:37.249641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1.1/ Tokenizer**\n",
    "First of all, we use tokenizer to convert the word to array for the computer can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:40.525850Z",
     "iopub.status.busy": "2022-07-10T19:03:40.525552Z",
     "iopub.status.idle": "2022-07-10T19:03:40.902687Z",
     "shell.execute_reply": "2022-07-10T19:03:40.901896Z",
     "shell.execute_reply.started": "2022-07-10T19:03:40.525819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 97\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAX_LEN** = 96 then for each training row, RoBERTa receives 96 tokens. The reason to use **lowercase** and **add_prefix_space** because when spelling with RoBERTa :\" helllo\", \"hello\", \" Hello\", and \"Hello\" use the same \" hello\" token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2/ Training data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.1/ Adjust the input**\n",
    "In this stage we ready our data for the model, all the input will be change to numerical and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:44.266109Z",
     "iopub.status.busy": "2022-07-10T19:03:44.265840Z",
     "iopub.status.idle": "2022-07-10T19:03:53.105947Z",
     "shell.execute_reply": "2022-07-10T19:03:53.105081Z",
     "shell.execute_reply.started": "2022-07-10T19:03:44.266080Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[    0   939 12905 ...     1     1     1]\n",
      " [    0    98  3036 ...     1     1     1]\n",
      " [    0   127  3504 ...     1     1     1]\n",
      " ...\n",
      " [    0  1423   857 ...     1     1     1]\n",
      " [    0    53    24 ...     1     1     1]\n",
      " [    0    70    42 ...     1     1     1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [(0, 4), (4, 9), (9, 12), (12, 18), (18, 24), (24, 27), (27, 29), (29, 33), (33, 36), (36, 37), (37, 44), (44, 45), (45, 47), (47, 49), (49, 50), (50, 53), (53, 54), (54, 57), (57, 59)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3/ Test Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same as the Training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:04:17.944124Z",
     "iopub.status.busy": "2022-07-10T19:04:17.943856Z",
     "iopub.status.idle": "2022-07-10T19:04:18.241347Z",
     "shell.execute_reply": "2022-07-10T19:04:18.240640Z",
     "shell.execute_reply.started": "2022-07-10T19:04:17.944095Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the original text is [ i][ am][ having][ a][ great][ day]</s>[ positive]</s> with tokens 0,1,2,3,4,5,6,7,8,9,10 and the selected text is \"great day\", then the training has start index = 5 and end index = 6.\n",
    "\n",
    "If our model also predicts a = 5 and b = 6 and we try to select the text from [ i][ am][ having][ a][ great][ day], the indices 5 and 6 will not return \"great day\". Instead we must use tokens[4:6] to get great day. We subtract 1 because the is now removed. And we add 1 to b because python indexing for list[3:5] does not return 5 it only returns 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4/ Build roBERTa model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4.1/ Building the bones of model**\n",
    "Built the bones of a roBERTa model, using the model has already train by the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:05:21.352602Z",
     "iopub.status.busy": "2022-07-10T19:05:21.352259Z",
     "iopub.status.idle": "2022-07-10T19:05:21.364895Z",
     "shell.execute_reply": "2022-07-10T19:05:21.364038Z",
     "shell.execute_reply.started": "2022-07-10T19:05:21.352569Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5/ Create metric**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We code this for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:06:16.388950Z",
     "iopub.status.busy": "2022-07-10T19:06:16.388632Z",
     "iopub.status.idle": "2022-07-10T19:06:16.395186Z",
     "shell.execute_reply": "2022-07-10T19:06:16.394374Z",
     "shell.execute_reply.started": "2022-07-10T19:06:16.388923Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.6/ Train roBERTa model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:07:44.465971Z",
     "iopub.status.busy": "2022-07-10T19:07:44.465660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.1827 - activation_loss: 1.0817 - activation_1_loss: 1.1011\n",
      "Epoch 00001: val_loss improved from inf to 1.68020, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 290s 13ms/sample - loss: 2.1819 - activation_loss: 1.0816 - activation_1_loss: 1.1004 - val_loss: 1.6802 - val_activation_loss: 0.8550 - val_activation_1_loss: 0.8248\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.6341 - activation_loss: 0.8404 - activation_1_loss: 0.7937\n",
      "Epoch 00002: val_loss improved from 1.68020 to 1.65274, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 275s 13ms/sample - loss: 1.6335 - activation_loss: 0.8402 - activation_1_loss: 0.7933 - val_loss: 1.6527 - val_activation_loss: 0.8532 - val_activation_1_loss: 0.7992\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.4927 - activation_loss: 0.7707 - activation_1_loss: 0.7220\n",
      "Epoch 00003: val_loss did not improve from 1.65274\n",
      "21984/21984 [==============================] - 273s 12ms/sample - loss: 1.4921 - activation_loss: 0.7705 - activation_1_loss: 0.7217 - val_loss: 1.6781 - val_activation_loss: 0.8575 - val_activation_1_loss: 0.8203\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.6997183870700572\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1038 - activation_loss: 1.0600 - activation_1_loss: 1.0438\n",
      "Epoch 00001: val_loss improved from inf to 1.63457, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 307s 14ms/sample - loss: 2.1037 - activation_loss: 1.0585 - activation_1_loss: 1.0426 - val_loss: 1.6346 - val_activation_loss: 0.8404 - val_activation_1_loss: 0.7935\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6284 - activation_loss: 0.8412 - activation_1_loss: 0.7871\n",
      "Epoch 00002: val_loss improved from 1.63457 to 1.61607, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 288s 13ms/sample - loss: 1.6284 - activation_loss: 0.8400 - activation_1_loss: 0.7891 - val_loss: 1.6161 - val_activation_loss: 0.8370 - val_activation_1_loss: 0.7783\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4776 - activation_loss: 0.7647 - activation_1_loss: 0.7129\n",
      "Epoch 00003: val_loss did not improve from 1.61607\n",
      "21985/21985 [==============================] - 287s 13ms/sample - loss: 1.4778 - activation_loss: 0.7640 - activation_1_loss: 0.7183 - val_loss: 1.6732 - val_activation_loss: 0.8634 - val_activation_1_loss: 0.8092\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.7119041072104035\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1710 - activation_loss: 1.0759 - activation_1_loss: 1.0951\n",
      "Epoch 00001: val_loss improved from inf to 1.67664, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 308s 14ms/sample - loss: 2.1709 - activation_loss: 1.0748 - activation_1_loss: 1.0941 - val_loss: 1.6766 - val_activation_loss: 0.8516 - val_activation_1_loss: 0.8252\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6468 - activation_loss: 0.8480 - activation_1_loss: 0.7988\n",
      "Epoch 00002: val_loss improved from 1.67664 to 1.62822, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 288s 13ms/sample - loss: 1.6467 - activation_loss: 0.8468 - activation_1_loss: 0.7977 - val_loss: 1.6282 - val_activation_loss: 0.8290 - val_activation_1_loss: 0.7993\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4991 - activation_loss: 0.7755 - activation_1_loss: 0.7235\n",
      "Epoch 00003: val_loss did not improve from 1.62822\n",
      "21985/21985 [==============================] - 287s 13ms/sample - loss: 1.4991 - activation_loss: 0.7759 - activation_1_loss: 0.7227 - val_loss: 1.6569 - val_activation_loss: 0.8669 - val_activation_1_loss: 0.7902\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 3 Jaccard = 0.6995682021045114\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2729 - activation_loss: 1.1404 - activation_1_loss: 1.1325\n",
      "Epoch 00001: val_loss improved from inf to 1.67653, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 307s 14ms/sample - loss: 2.2729 - activation_loss: 1.1395 - activation_1_loss: 1.1316 - val_loss: 1.6765 - val_activation_loss: 0.8560 - val_activation_1_loss: 0.8205\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6306 - activation_loss: 0.8451 - activation_1_loss: 0.7855\n",
      "Epoch 00002: val_loss improved from 1.67653 to 1.59010, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 288s 13ms/sample - loss: 1.6306 - activation_loss: 0.8439 - activation_1_loss: 0.7859 - val_loss: 1.5901 - val_activation_loss: 0.8139 - val_activation_1_loss: 0.7761\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4752 - activation_loss: 0.7656 - activation_1_loss: 0.7096\n",
      "Epoch 00003: val_loss did not improve from 1.59010\n",
      "21985/21985 [==============================] - 287s 13ms/sample - loss: 1.4752 - activation_loss: 0.7645 - activation_1_loss: 0.7123 - val_loss: 1.6319 - val_activation_loss: 0.8262 - val_activation_1_loss: 0.8057\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 4 Jaccard = 0.7100973994066376\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2086 - activation_loss: 1.0871 - activation_1_loss: 1.1215\n",
      "Epoch 00001: val_loss improved from inf to 1.66706, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 308s 14ms/sample - loss: 2.2087 - activation_loss: 1.0864 - activation_1_loss: 1.1251 - val_loss: 1.6671 - val_activation_loss: 0.8660 - val_activation_1_loss: 0.8007\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6694 - activation_loss: 0.8557 - activation_1_loss: 0.8137\n",
      "Epoch 00002: val_loss improved from 1.66706 to 1.59905, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 289s 13ms/sample - loss: 1.6693 - activation_loss: 0.8545 - activation_1_loss: 0.8132 - val_loss: 1.5991 - val_activation_loss: 0.8294 - val_activation_1_loss: 0.7689\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5146 - activation_loss: 0.7848 - activation_1_loss: 0.7298\n",
      "Epoch 00003: val_loss did not improve from 1.59905\n",
      "21985/21985 [==============================] - 287s 13ms/sample - loss: 1.5146 - activation_loss: 0.7837 - activation_1_loss: 0.7288 - val_loss: 1.6084 - val_activation_loss: 0.8490 - val_activation_1_loss: 0.7589\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 5 Jaccard = 0.7090039384651644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.7/ Kaggle submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>c435fb9416</td>\n",
       "      <td>I wish i had my iPod , i need some jonas . I miss their ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i wish i had my ipod , i need some jonas . i miss their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>650fcedd22</td>\n",
       "      <td>he`s back! meeting him in 40 mins</td>\n",
       "      <td>neutral</td>\n",
       "      <td>he`s back! meeting him in 40 mins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>32e183bbad</td>\n",
       "      <td>s/s aus fshionwk- zimmermann,illionare,dhini + gail sorr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>loved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>51a31aa9a1</td>\n",
       "      <td>Hannah slept here last night  just gave her her 5:00 AM ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hannah slept here last night just gave her her 5:00 am ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>4c0329867c</td>\n",
       "      <td>yeah...I hope it turns out... 20 more minutes!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>yeah...i hope it turns out... 20 more minutes!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>78a3ef0435</td>\n",
       "      <td>Quite a few. Peacocks and Oceanographer`s Choice are my...</td>\n",
       "      <td>positive</td>\n",
       "      <td>favorites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>fe588dce16</td>\n",
       "      <td>then am anticipating the next ten days!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>then am anticipating the next ten days!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>81de3c6143</td>\n",
       "      <td>The day started so wonderful, but now our kids our cryin...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>the day started so wonderful, but now our kids our cryi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>5a4ebbbdba</td>\n",
       "      <td>back to bed</td>\n",
       "      <td>neutral</td>\n",
       "      <td>back to bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>12b499b4ca</td>\n",
       "      <td>i had walkathon this morning.  and i`m among the first w...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i had walkathon this morning. and i`m among the first w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>ab5b510355</td>\n",
       "      <td>My Denny`s shut down in the winter.  I have to do 50 mi...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>my denny`s shut down in the winter. i have to do 50 mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>fe2f3f3359</td>\n",
       "      <td>you`re alive!!!!!!  Go w/ The Notebook...it`ll make you...</td>\n",
       "      <td>negative</td>\n",
       "      <td>it`ll make you cry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>43ee369d34</td>\n",
       "      <td>Following new other #sanctuary fans! See the wonders of ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>69789cff46</td>\n",
       "      <td>Pleeaaasee come out sun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>pleeaaasee come out sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>49c713c76d</td>\n",
       "      <td>Is It The Bit Where Hollie Started Crying?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>is it the bit where hollie started crying?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>e48cf1bff4</td>\n",
       "      <td>I saw that you were calling but cannot answer as I`m in...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i saw that you were calling but cannot answer as i`m in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>eaf8a522e9</td>\n",
       "      <td>arrr bummer  who do you want to win?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>arrr bummer who do you want to win?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>5a3c9bf459</td>\n",
       "      <td>was just about to try and  dl the sims 3 but realize my ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>was just about to try and dl the sims 3 but realize my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>e09f5b6b95</td>\n",
       "      <td>went to the cd store to search for the cd. But</td>\n",
       "      <td>neutral</td>\n",
       "      <td>went to the cd store to search for the cd. but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>00c8cfed18</td>\n",
       "      <td>I dont wanna im to spanish today</td>\n",
       "      <td>negative</td>\n",
       "      <td>i dont wanna im to spanish today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>1c0b70106f</td>\n",
       "      <td>Did a historical Jesus ever exist? Im finding it hard to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>7a897c9999</td>\n",
       "      <td>http://twitpic.com/4wptj one of my prized mags/book/ann...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>one of my prized mags/book/annuals dunno if any others ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>676b733c57</td>\n",
       "      <td>Buying pretty shiny beads and things  I feel quite girli...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>buying pretty shiny beads and things i feel quite girlish.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>36631a41d0</td>\n",
       "      <td>well piss on that. I can`t get into their site @ work n...</td>\n",
       "      <td>negative</td>\n",
       "      <td>well piss on that.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>63c7425486</td>\n",
       "      <td>it`s friday but i have to work the weekend</td>\n",
       "      <td>neutral</td>\n",
       "      <td>it`s friday but i have to work the weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>d60072b03a</td>\n",
       "      <td>He can`t fix it.   I guess I`ll write until I get too bo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>lame.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2911</th>\n",
       "      <td>3dd127f4af</td>\n",
       "      <td>hey hunnie  how are u?? I miss talkin to u! Ty for the ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hey hunnie how are u?? i miss talkin to u! ty for the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>ef5ba215b6</td>\n",
       "      <td>a band from Hawaii with a Sublime sound, so all you subl...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>a band from hawaii with a sublime sound, so all you sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>3160620b36</td>\n",
       "      <td>Mozzer cancelled tonight by the looks of it.</td>\n",
       "      <td>positive</td>\n",
       "      <td>mozzer cancelled tonight by the looks of it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>bce5834e03</td>\n",
       "      <td>I misses my bed  so sleepy!</td>\n",
       "      <td>negative</td>\n",
       "      <td>i misses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "1071  c435fb9416  I wish i had my iPod , i need some jonas . I miss their ...   \n",
       "1765  650fcedd22                            he`s back! meeting him in 40 mins   \n",
       "2276  32e183bbad  s/s aus fshionwk- zimmermann,illionare,dhini + gail sorr...   \n",
       "1159  51a31aa9a1  Hannah slept here last night  just gave her her 5:00 AM ...   \n",
       "2077  4c0329867c               yeah...I hope it turns out... 20 more minutes!   \n",
       "1322  78a3ef0435   Quite a few. Peacocks and Oceanographer`s Choice are my...   \n",
       "698   fe588dce16                      then am anticipating the next ten days!   \n",
       "2003  81de3c6143  The day started so wonderful, but now our kids our cryin...   \n",
       "1481  5a4ebbbdba                                                  back to bed   \n",
       "1315  12b499b4ca  i had walkathon this morning.  and i`m among the first w...   \n",
       "2013  ab5b510355   My Denny`s shut down in the winter.  I have to do 50 mi...   \n",
       "1032  fe2f3f3359   you`re alive!!!!!!  Go w/ The Notebook...it`ll make you...   \n",
       "2332  43ee369d34  Following new other #sanctuary fans! See the wonders of ...   \n",
       "631   69789cff46                                      Pleeaaasee come out sun   \n",
       "2418  49c713c76d                   Is It The Bit Where Hollie Started Crying?   \n",
       "987   e48cf1bff4   I saw that you were calling but cannot answer as I`m in...   \n",
       "1904  eaf8a522e9                         arrr bummer  who do you want to win?   \n",
       "1771  5a3c9bf459  was just about to try and  dl the sims 3 but realize my ...   \n",
       "1720  e09f5b6b95               went to the cd store to search for the cd. But   \n",
       "1436  00c8cfed18                             I dont wanna im to spanish today   \n",
       "3440  1c0b70106f  Did a historical Jesus ever exist? Im finding it hard to...   \n",
       "1966  7a897c9999   http://twitpic.com/4wptj one of my prized mags/book/ann...   \n",
       "2246  676b733c57  Buying pretty shiny beads and things  I feel quite girli...   \n",
       "1346  36631a41d0   well piss on that. I can`t get into their site @ work n...   \n",
       "2874  63c7425486                   it`s friday but i have to work the weekend   \n",
       "2094  d60072b03a  He can`t fix it.   I guess I`ll write until I get too bo...   \n",
       "2911  3dd127f4af   hey hunnie  how are u?? I miss talkin to u! Ty for the ...   \n",
       "1729  ef5ba215b6  a band from Hawaii with a Sublime sound, so all you subl...   \n",
       "346   3160620b36                 Mozzer cancelled tonight by the looks of it.   \n",
       "385   bce5834e03                                  I misses my bed  so sleepy!   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "1071   neutral   i wish i had my ipod , i need some jonas . i miss their...  \n",
       "1765   neutral                            he`s back! meeting him in 40 mins  \n",
       "2276  positive                                                        loved  \n",
       "1159   neutral   hannah slept here last night just gave her her 5:00 am ...  \n",
       "2077   neutral               yeah...i hope it turns out... 20 more minutes!  \n",
       "1322  positive                                                    favorites  \n",
       "698    neutral                      then am anticipating the next ten days!  \n",
       "2003   neutral   the day started so wonderful, but now our kids our cryi...  \n",
       "1481   neutral                                                  back to bed  \n",
       "1315   neutral   i had walkathon this morning. and i`m among the first w...  \n",
       "2013   neutral   my denny`s shut down in the winter. i have to do 50 mil...  \n",
       "1032  negative                                           it`ll make you cry  \n",
       "2332  positive                                                         good  \n",
       "631    neutral                                      pleeaaasee come out sun  \n",
       "2418   neutral                   is it the bit where hollie started crying?  \n",
       "987    neutral   i saw that you were calling but cannot answer as i`m in...  \n",
       "1904   neutral                          arrr bummer who do you want to win?  \n",
       "1771   neutral   was just about to try and dl the sims 3 but realize my ...  \n",
       "1720   neutral               went to the cd store to search for the cd. but  \n",
       "1436  negative                             i dont wanna im to spanish today  \n",
       "3440  negative                                                         bugs  \n",
       "1966   neutral   one of my prized mags/book/annuals dunno if any others ...  \n",
       "2246   neutral   buying pretty shiny beads and things i feel quite girlish.  \n",
       "1346  negative                                           well piss on that.  \n",
       "2874   neutral                   it`s friday but i have to work the weekend  \n",
       "2094  negative                                                        lame.  \n",
       "2911   neutral   hey hunnie how are u?? i miss talkin to u! ty for the f...  \n",
       "1729   neutral   a band from hawaii with a sublime sound, so all you sub...  \n",
       "346   positive                 mozzer cancelled tonight by the looks of it.  \n",
       "385   negative                                                     i misses  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
