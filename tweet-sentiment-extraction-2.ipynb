{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preface**\n",
    "**This project was inspired and referenced in [TensorFlow roBERTa - [0.705]](https://www.kaggle.com/code/cdeotte/tensorflow-roberta-0-705)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1/ About team**\n",
    "|StuID  |        Name       |     Kaggle    |       Github      |\n",
    "|-------|-------------------|---------------|-------------------|\n",
    "|1752052|Dang Huu Phuoc Vinh|[V_Notebook](https://www.kaggle.com/danghuuphuocvinh)|[V_Github](https://github.com/DangHuuPhuocVinh/data_science_application)\n",
    "|1753097|Le Nguyen Minh Tam |[T_Notebook](https://www.kaggle.com/minhtamlenguyen)|[T_Github](https://github.com/lnmtam1999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2/ About competition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1/ Name of competition**\n",
    "**[Tweet Sentiment Extraction](https://www.kaggle.com/competitions/tweet-sentiment-extraction) organized by [Kaggle](https://www.kaggle.com/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2/ Prize**\n",
    "**15000 USD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.3/ Description**\n",
    "  **E.g: \"My ridiculous dog is amazing.\" [sentiment: positive]**\n",
    "\n",
    "  **With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.**\n",
    "\n",
    "  **In this competition we've extracted support phrases from [Figure Eight's Data for Everyone platform](https://appen.com/datasets-resource-center/). The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.4/ Input and Output**\n",
    "- **Input: textID, text and sentiment**\n",
    "- **Output: selected_text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.5/ Columns**\n",
    "-  **textID - unique ID for each piece of text**\n",
    "-  **text - the text of the tweet**\n",
    "-  **sentiment - the general sentiment of the tweet**\n",
    "-  **selected_text - [train only] the text that supports the tweet's sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.6/ Meaningful**\n",
    "- **After doing this project, we can have a dataset with the phrases that were selected for using at any other NLP project**\n",
    "- **Can use for detecting some keywords that have sentiment** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.7/ Evaluation**\n",
    " ![img](https://user-images.githubusercontent.com/35680794/174698744-57b2f116-fbe4-4fb6-9216-2e83e0494dca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3/ Model roBERTa for this project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4/ Developing the project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1/ Import Libraries, Data and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:37.249683Z",
     "iopub.status.busy": "2022-07-10T19:03:37.249400Z",
     "iopub.status.idle": "2022-07-10T19:03:37.256307Z",
     "shell.execute_reply": "2022-07-10T19:03:37.255411Z",
     "shell.execute_reply.started": "2022-07-10T19:03:37.249641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1.1/ Tokenizer**\n",
    "First of all, we use tokenizer to convert the word to array for the computer can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:40.525850Z",
     "iopub.status.busy": "2022-07-10T19:03:40.525552Z",
     "iopub.status.idle": "2022-07-10T19:03:40.902687Z",
     "shell.execute_reply": "2022-07-10T19:03:40.901896Z",
     "shell.execute_reply.started": "2022-07-10T19:03:40.525819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 94\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAX_LEN** = 96 then for each training row, RoBERTa receives 96 tokens. The reason to use **lowercase** and **add_prefix_space** because when spelling with RoBERTa :\" helllo\", \"hello\", \" Hello\", and \"Hello\" use the same \" hello\" token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2/ Training data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.1/ Adjust the input**\n",
    "In this stage we ready our data for the model, all the input will be change to numerical and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:44.266109Z",
     "iopub.status.busy": "2022-07-10T19:03:44.265840Z",
     "iopub.status.idle": "2022-07-10T19:03:53.105947Z",
     "shell.execute_reply": "2022-07-10T19:03:53.105081Z",
     "shell.execute_reply.started": "2022-07-10T19:03:44.266080Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[    0   939 12905 ...     1     1     1]\n",
      " [    0    98  3036 ...     1     1     1]\n",
      " [    0   127  3504 ...     1     1     1]\n",
      " ...\n",
      " [    0  1423   857 ...     1     1     1]\n",
      " [    0    53    24 ...     1     1     1]\n",
      " [    0    70    42 ...     1     1     1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [(0, 4), (4, 9), (9, 12), (12, 18), (18, 24), (24, 27), (27, 29), (29, 33), (33, 36), (36, 37), (37, 44), (44, 45), (45, 47), (47, 49), (49, 50), (50, 53), (53, 54), (54, 57), (57, 59)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3/ Test Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same as the Training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:04:17.944124Z",
     "iopub.status.busy": "2022-07-10T19:04:17.943856Z",
     "iopub.status.idle": "2022-07-10T19:04:18.241347Z",
     "shell.execute_reply": "2022-07-10T19:04:18.240640Z",
     "shell.execute_reply.started": "2022-07-10T19:04:17.944095Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the original text is [ i][ am][ having][ a][ great][ day]</s>[ positive]</s> with tokens 0,1,2,3,4,5,6,7,8,9,10 and the selected text is \"great day\", then the training has start index = 5 and end index = 6.\n",
    "\n",
    "If our model also predicts a = 5 and b = 6 and we try to select the text from [ i][ am][ having][ a][ great][ day], the indices 5 and 6 will not return \"great day\". Instead we must use tokens[4:6] to get great day. We subtract 1 because the is now removed. And we add 1 to b because python indexing for list[3:5] does not return 5 it only returns 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4/ Build roBERTa model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4.1/ Building the bones of model**\n",
    "Built the bones of a roBERTa model, using the model has already train by the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:05:21.352602Z",
     "iopub.status.busy": "2022-07-10T19:05:21.352259Z",
     "iopub.status.idle": "2022-07-10T19:05:21.364895Z",
     "shell.execute_reply": "2022-07-10T19:05:21.364038Z",
     "shell.execute_reply.started": "2022-07-10T19:05:21.352569Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5/ Create metric**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We code this for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:06:16.388950Z",
     "iopub.status.busy": "2022-07-10T19:06:16.388632Z",
     "iopub.status.idle": "2022-07-10T19:06:16.395186Z",
     "shell.execute_reply": "2022-07-10T19:06:16.394374Z",
     "shell.execute_reply.started": "2022-07-10T19:06:16.388923Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.6/ Train roBERTa model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:07:44.465971Z",
     "iopub.status.busy": "2022-07-10T19:07:44.465660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.2110 - activation_loss: 1.0805 - activation_1_loss: 1.1305\n",
      "Epoch 00001: val_loss improved from inf to 1.71977, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 284s 13ms/sample - loss: 2.2100 - activation_loss: 1.0800 - activation_1_loss: 1.1301 - val_loss: 1.7198 - val_activation_loss: 0.8839 - val_activation_1_loss: 0.8357\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.7069 - activation_loss: 0.8585 - activation_1_loss: 0.8483\n",
      "Epoch 00002: val_loss improved from 1.71977 to 1.70237, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 270s 12ms/sample - loss: 1.7071 - activation_loss: 0.8587 - activation_1_loss: 0.8485 - val_loss: 1.7024 - val_activation_loss: 0.8840 - val_activation_1_loss: 0.8180\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.5653 - activation_loss: 0.8032 - activation_1_loss: 0.7621\n",
      "Epoch 00003: val_loss did not improve from 1.70237\n",
      "21984/21984 [==============================] - 268s 12ms/sample - loss: 1.5650 - activation_loss: 0.8030 - activation_1_loss: 0.7620 - val_loss: 1.7105 - val_activation_loss: 0.8658 - val_activation_1_loss: 0.8447\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 26s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.7003481395671466\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2266 - activation_loss: 1.0980 - activation_1_loss: 1.1286\n",
      "Epoch 00001: val_loss improved from inf to 1.73409, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 301s 14ms/sample - loss: 2.2266 - activation_loss: 1.0975 - activation_1_loss: 1.1288 - val_loss: 1.7341 - val_activation_loss: 0.8728 - val_activation_1_loss: 0.8605\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.7228 - activation_loss: 0.8720 - activation_1_loss: 0.8507\n",
      "Epoch 00002: val_loss improved from 1.73409 to 1.67866, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 283s 13ms/sample - loss: 1.7228 - activation_loss: 0.8730 - activation_1_loss: 0.8506 - val_loss: 1.6787 - val_activation_loss: 0.8440 - val_activation_1_loss: 0.8340\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5644 - activation_loss: 0.8011 - activation_1_loss: 0.7633\n",
      "Epoch 00003: val_loss improved from 1.67866 to 1.63718, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 283s 13ms/sample - loss: 1.5645 - activation_loss: 0.8035 - activation_1_loss: 0.7652 - val_loss: 1.6372 - val_activation_loss: 0.8481 - val_activation_1_loss: 0.7887\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 26s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.703362252968354\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1753 - activation_loss: 1.0962 - activation_1_loss: 1.0791\n",
      "Epoch 00001: val_loss improved from inf to 1.67343, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 300s 14ms/sample - loss: 2.1752 - activation_loss: 1.0946 - activation_1_loss: 1.0775 - val_loss: 1.6734 - val_activation_loss: 0.8551 - val_activation_1_loss: 0.8186\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6528 - activation_loss: 0.8531 - activation_1_loss: 0.7997\n",
      "Epoch 00002: val_loss did not improve from 1.67343\n",
      "21985/21985 [==============================] - 282s 13ms/sample - loss: 1.6528 - activation_loss: 0.8533 - activation_1_loss: 0.7991 - val_loss: 1.7012 - val_activation_loss: 0.8767 - val_activation_1_loss: 0.8247\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4879 - activation_loss: 0.7724 - activation_1_loss: 0.7155\n",
      "Epoch 00003: val_loss did not improve from 1.67343\n",
      "21985/21985 [==============================] - 281s 13ms/sample - loss: 1.4878 - activation_loss: 0.7713 - activation_1_loss: 0.7144 - val_loss: 1.6988 - val_activation_loss: 0.8495 - val_activation_1_loss: 0.8495\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 26s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 3 Jaccard = 0.6923034518793179\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1783 - activation_loss: 1.0824 - activation_1_loss: 1.0959\n",
      "Epoch 00001: val_loss improved from inf to 1.64884, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 300s 14ms/sample - loss: 2.1783 - activation_loss: 1.0816 - activation_1_loss: 1.0996 - val_loss: 1.6488 - val_activation_loss: 0.8374 - val_activation_1_loss: 0.8114\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6378 - activation_loss: 0.8382 - activation_1_loss: 0.7996\n",
      "Epoch 00002: val_loss improved from 1.64884 to 1.60265, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 283s 13ms/sample - loss: 1.6377 - activation_loss: 0.8370 - activation_1_loss: 0.7985 - val_loss: 1.6027 - val_activation_loss: 0.8262 - val_activation_1_loss: 0.7763\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6434 - activation_loss: 0.7849 - activation_1_loss: 0.8585\n",
      "Epoch 00003: val_loss did not improve from 1.60265\n",
      "21985/21985 [==============================] - 282s 13ms/sample - loss: 1.6433 - activation_loss: 0.7837 - activation_1_loss: 0.8573 - val_loss: 1.6427 - val_activation_loss: 0.8286 - val_activation_1_loss: 0.8142\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 26s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 4 Jaccard = 0.703967153535048\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2467 - activation_loss: 1.1318 - activation_1_loss: 1.1149\n",
      "Epoch 00001: val_loss improved from inf to 1.64984, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 301s 14ms/sample - loss: 2.2466 - activation_loss: 1.1302 - activation_1_loss: 1.1133 - val_loss: 1.6498 - val_activation_loss: 0.8609 - val_activation_1_loss: 0.7883\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6620 - activation_loss: 0.8577 - activation_1_loss: 0.8043\n",
      "Epoch 00002: val_loss improved from 1.64984 to 1.59680, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 283s 13ms/sample - loss: 1.6619 - activation_loss: 0.8564 - activation_1_loss: 0.8032 - val_loss: 1.5968 - val_activation_loss: 0.8384 - val_activation_1_loss: 0.7578\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5214 - activation_loss: 0.7846 - activation_1_loss: 0.7368\n",
      "Epoch 00003: val_loss did not improve from 1.59680\n",
      "21985/21985 [==============================] - 282s 13ms/sample - loss: 1.5213 - activation_loss: 0.7835 - activation_1_loss: 0.7362 - val_loss: 1.6410 - val_activation_loss: 0.8497 - val_activation_1_loss: 0.7904\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 26s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 5 Jaccard = 0.7070600024358096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.7/ Kaggle submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>18bcba79be</td>\n",
       "      <td>i`ve not had a reply on my topic yet  lolz welll its not...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i`ve not had a reply on my topic yet lolz welll its not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>e7e27b0c91</td>\n",
       "      <td>Welcome!</td>\n",
       "      <td>positive</td>\n",
       "      <td>welcome!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>d3d7b13278</td>\n",
       "      <td>Hi there.  I agree!  Small children should be running a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>81950f02a4</td>\n",
       "      <td>I got the new Silverstein CD. aha. Its AMAZING.  I highl...</td>\n",
       "      <td>positive</td>\n",
       "      <td>its amazing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>0e83e0955a</td>\n",
       "      <td>rt i have had few hearts the past few days on etsy  boo...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>rt i have had few hearts the past few days on etsy boo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>a4bb5d0c5c</td>\n",
       "      <td>OFFICIALLY booked for seattle with  and  it`s going to b...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>officially booked for seattle with and it`s going to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>81de3c6143</td>\n",
       "      <td>The day started so wonderful, but now our kids our cryin...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>the day started so wonderful, but now our kids our cryi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>43449f0068</td>\n",
       "      <td>_laura_LP ok thanks for the help! I hope they respond to...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>892b8a0cf8</td>\n",
       "      <td>so i guess we r sleepin over.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>so i guess we r sleepin over.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>8eedc5a2ed</td>\n",
       "      <td>I broke my ipod</td>\n",
       "      <td>negative</td>\n",
       "      <td>broke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>3c9419868b</td>\n",
       "      <td>staying afterschool today  not that i have any 'friends'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>likin` us &lt;3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>20f35978fb</td>\n",
       "      <td>I`m up wit cha!! Just got home from da reggae club, wis...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wishin i was sexin somebody!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>8d40f9e62d</td>\n",
       "      <td>hi ya demi! im glad ur back   http://twitpic.com/4vuuy ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>im glad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3523</th>\n",
       "      <td>036cd6abb0</td>\n",
       "      <td>My eyes are starting to hurt. So late. But must reach 2...</td>\n",
       "      <td>negative</td>\n",
       "      <td>hurt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>eba3cee9be</td>\n",
       "      <td>_Brown Thank you so much Natalie, hope u are well</td>\n",
       "      <td>positive</td>\n",
       "      <td>thank you so much natalie, hope u are well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>ea5c577cb7</td>\n",
       "      <td>lol exams i didn`t go to mcast or other school i finish...</td>\n",
       "      <td>negative</td>\n",
       "      <td>disappointed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>a717783c80</td>\n",
       "      <td>at work getting a quick bite to eat before having to kil...</td>\n",
       "      <td>negative</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>5ec14d968a</td>\n",
       "      <td>i wish we had sun lollies for me to get addicted  what ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i wish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>736f2bd7b6</td>\n",
       "      <td>Adams morgannnn for jumbo slice</td>\n",
       "      <td>neutral</td>\n",
       "      <td>adams morgannnn for jumbo slice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>dab7f56791</td>\n",
       "      <td>**** brah, u not happy?</td>\n",
       "      <td>negative</td>\n",
       "      <td>**** brah, u not happy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>ff7949688c</td>\n",
       "      <td>lol you guys are awesome</td>\n",
       "      <td>positive</td>\n",
       "      <td>awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>75dcba6339</td>\n",
       "      <td>watching my favorite tv shows on HULU.com for free</td>\n",
       "      <td>positive</td>\n",
       "      <td>favorite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>aa1a163174</td>\n",
       "      <td>Will do</td>\n",
       "      <td>neutral</td>\n",
       "      <td>will do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>ff7126ea72</td>\n",
       "      <td>gawww, why is facebook being so slow?</td>\n",
       "      <td>negative</td>\n",
       "      <td>gawww, why is facebook being so slow?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3489</th>\n",
       "      <td>603a6f30be</td>\n",
       "      <td>I think Max (my cat) may really be gone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i think max (my cat) may really be gone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>5683fb4e31</td>\n",
       "      <td>I honestly hate what I have said to some ppl sometimes. ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i honestly hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>fcc4f3baa5</td>\n",
       "      <td>ahh ok! Enjoy! I`ll miss it</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ahh ok! enjoy! i`ll miss it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>811dc7e401</td>\n",
       "      <td>_N9ne I`m not having a good day</td>\n",
       "      <td>negative</td>\n",
       "      <td>i`m not having a good day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2f883c6eb1</td>\n",
       "      <td>St joe is dirty.</td>\n",
       "      <td>negative</td>\n",
       "      <td>dirty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>33680fc87e</td>\n",
       "      <td>being a computer geek is entertaining.... i think.</td>\n",
       "      <td>positive</td>\n",
       "      <td>entertaining....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "3331  18bcba79be  i`ve not had a reply on my topic yet  lolz welll its not...   \n",
       "130   e7e27b0c91                                                     Welcome!   \n",
       "44    d3d7b13278   Hi there.  I agree!  Small children should be running a...   \n",
       "1717  81950f02a4  I got the new Silverstein CD. aha. Its AMAZING.  I highl...   \n",
       "683   0e83e0955a   rt i have had few hearts the past few days on etsy  boo...   \n",
       "1229  a4bb5d0c5c  OFFICIALLY booked for seattle with  and  it`s going to b...   \n",
       "2003  81de3c6143  The day started so wonderful, but now our kids our cryin...   \n",
       "796   43449f0068  _laura_LP ok thanks for the help! I hope they respond to...   \n",
       "1414  892b8a0cf8                                so i guess we r sleepin over.   \n",
       "949   8eedc5a2ed                                              I broke my ipod   \n",
       "1977  3c9419868b  staying afterschool today  not that i have any 'friends'...   \n",
       "3450  20f35978fb   I`m up wit cha!! Just got home from da reggae club, wis...   \n",
       "530   8d40f9e62d   hi ya demi! im glad ur back   http://twitpic.com/4vuuy ...   \n",
       "3523  036cd6abb0   My eyes are starting to hurt. So late. But must reach 2...   \n",
       "1654  eba3cee9be            _Brown Thank you so much Natalie, hope u are well   \n",
       "1557  ea5c577cb7   lol exams i didn`t go to mcast or other school i finish...   \n",
       "335   a717783c80  at work getting a quick bite to eat before having to kil...   \n",
       "2903  5ec14d968a   i wish we had sun lollies for me to get addicted  what ...   \n",
       "1779  736f2bd7b6                              Adams morgannnn for jumbo slice   \n",
       "1923  dab7f56791                                      **** brah, u not happy?   \n",
       "1058  ff7949688c                                     lol you guys are awesome   \n",
       "3081  75dcba6339           watching my favorite tv shows on HULU.com for free   \n",
       "1228  aa1a163174                                                      Will do   \n",
       "514   ff7126ea72                        gawww, why is facebook being so slow?   \n",
       "3489  603a6f30be                      I think Max (my cat) may really be gone   \n",
       "2557  5683fb4e31  I honestly hate what I have said to some ppl sometimes. ...   \n",
       "1241  fcc4f3baa5                                  ahh ok! Enjoy! I`ll miss it   \n",
       "1054  811dc7e401                              _N9ne I`m not having a good day   \n",
       "200   2f883c6eb1                                             St joe is dirty.   \n",
       "2070  33680fc87e           being a computer geek is entertaining.... i think.   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "3331   neutral   i`ve not had a reply on my topic yet lolz welll its not...  \n",
       "130   positive                                                     welcome!  \n",
       "44    positive                                                       happy,  \n",
       "1717  positive                                                 its amazing.  \n",
       "683    neutral   rt i have had few hearts the past few days on etsy boo ...  \n",
       "1229   neutral   officially booked for seattle with and it`s going to be...  \n",
       "2003   neutral   the day started so wonderful, but now our kids our cryi...  \n",
       "796   positive                                                       thanks  \n",
       "1414   neutral                                so i guess we r sleepin over.  \n",
       "949   negative                                                        broke  \n",
       "1977  positive                                                 likin` us <3  \n",
       "3450  positive                               wishin i was sexin somebody!!!  \n",
       "530   positive                                                      im glad  \n",
       "3523  negative                                                        hurt.  \n",
       "1654  positive                   thank you so much natalie, hope u are well  \n",
       "1557  negative                                                 disappointed  \n",
       "335   negative                                                         kill  \n",
       "2903  positive                                                       i wish  \n",
       "1779   neutral                              adams morgannnn for jumbo slice  \n",
       "1923  negative                                      **** brah, u not happy?  \n",
       "1058  positive                                                      awesome  \n",
       "3081  positive                                                     favorite  \n",
       "1228   neutral                                                      will do  \n",
       "514   negative                        gawww, why is facebook being so slow?  \n",
       "3489   neutral                      i think max (my cat) may really be gone  \n",
       "2557  negative                                              i honestly hate  \n",
       "1241   neutral                                  ahh ok! enjoy! i`ll miss it  \n",
       "1054  negative                                    i`m not having a good day  \n",
       "200   negative                                                       dirty.  \n",
       "2070  positive                                             entertaining....  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
