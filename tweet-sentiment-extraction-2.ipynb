{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries, Data and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:37.249683Z",
     "iopub.status.busy": "2022-07-10T19:03:37.249400Z",
     "iopub.status.idle": "2022-07-10T19:03:37.256307Z",
     "shell.execute_reply": "2022-07-10T19:03:37.255411Z",
     "shell.execute_reply.started": "2022-07-10T19:03:37.249641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we use tokenizer to convert the word to array for the computer can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:40.525850Z",
     "iopub.status.busy": "2022-07-10T19:03:40.525552Z",
     "iopub.status.idle": "2022-07-10T19:03:40.902687Z",
     "shell.execute_reply": "2022-07-10T19:03:40.901896Z",
     "shell.execute_reply.started": "2022-07-10T19:03:40.525819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 96\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAX_LEN** = 96 then for each training row, RoBERTa receives 96 tokens. The reason to use **lowercase** and **add_prefix_space** because when spelling with RoBERTa :\" helllo\", \"hello\", \" Hello\", and \"Hello\" use the same \" hello\" token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:44.266109Z",
     "iopub.status.busy": "2022-07-10T19:03:44.265840Z",
     "iopub.status.idle": "2022-07-10T19:03:53.105947Z",
     "shell.execute_reply": "2022-07-10T19:03:53.105081Z",
     "shell.execute_reply.started": "2022-07-10T19:03:44.266080Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:04:17.944124Z",
     "iopub.status.busy": "2022-07-10T19:04:17.943856Z",
     "iopub.status.idle": "2022-07-10T19:04:18.241347Z",
     "shell.execute_reply": "2022-07-10T19:04:18.240640Z",
     "shell.execute_reply.started": "2022-07-10T19:04:17.944095Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the original text is [ i][ am][ having][ a][ great][ day]</s>[ positive]</s> with tokens 0,1,2,3,4,5,6,7,8,9,10 and the selected text is \"great day\", then the training has start index = 5 and end index = 6.\n",
    "\n",
    "If our model also predicts a = 5 and b = 6 and we try to select the text from [ i][ am][ having][ a][ great][ day], the indices 5 and 6 will not return \"great day\". Instead we must use tokens[4:6] to get great day. We subtract 1 because the is now removed. And we add 1 to b because python indexing for list[3:5] does not return 5 it only returns 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build roBERTa model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:05:21.352602Z",
     "iopub.status.busy": "2022-07-10T19:05:21.352259Z",
     "iopub.status.idle": "2022-07-10T19:05:21.364895Z",
     "shell.execute_reply": "2022-07-10T19:05:21.364038Z",
     "shell.execute_reply.started": "2022-07-10T19:05:21.352569Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:06:16.388950Z",
     "iopub.status.busy": "2022-07-10T19:06:16.388632Z",
     "iopub.status.idle": "2022-07-10T19:06:16.395186Z",
     "shell.execute_reply": "2022-07-10T19:06:16.394374Z",
     "shell.execute_reply.started": "2022-07-10T19:06:16.388923Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train roBERTa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:07:44.465971Z",
     "iopub.status.busy": "2022-07-10T19:07:44.465660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.4773 - activation_loss: 1.1804 - activation_1_loss: 1.2969\n",
      "Epoch 00001: val_loss improved from inf to 2.30142, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 287s 13ms/sample - loss: 2.4773 - activation_loss: 1.1804 - activation_1_loss: 1.2969 - val_loss: 2.3014 - val_activation_loss: 1.0860 - val_activation_1_loss: 1.2151\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.8348 - activation_loss: 0.9262 - activation_1_loss: 0.9086\n",
      "Epoch 00002: val_loss improved from 2.30142 to 1.70930, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 272s 12ms/sample - loss: 1.8342 - activation_loss: 0.9259 - activation_1_loss: 0.9083 - val_loss: 1.7093 - val_activation_loss: 0.8882 - val_activation_1_loss: 0.8211\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.7419 - activation_loss: 0.8765 - activation_1_loss: 0.8653\n",
      "Epoch 00003: val_loss did not improve from 1.70930\n",
      "21984/21984 [==============================] - 270s 12ms/sample - loss: 1.7412 - activation_loss: 0.8763 - activation_1_loss: 0.8649 - val_loss: 1.7452 - val_activation_loss: 0.8858 - val_activation_1_loss: 0.8592\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 26s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.699967965368088\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1224 - activation_loss: 1.0699 - activation_1_loss: 1.0525\n",
      "Epoch 00001: val_loss improved from inf to 1.69307, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 303s 14ms/sample - loss: 2.1223 - activation_loss: 1.0684 - activation_1_loss: 1.0510 - val_loss: 1.6931 - val_activation_loss: 0.8797 - val_activation_1_loss: 0.8126\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6279 - activation_loss: 0.8397 - activation_1_loss: 0.7882\n",
      "Epoch 00002: val_loss improved from 1.69307 to 1.61712, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 285s 13ms/sample - loss: 1.6280 - activation_loss: 0.8414 - activation_1_loss: 0.7874 - val_loss: 1.6171 - val_activation_loss: 0.8363 - val_activation_1_loss: 0.7801\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4941 - activation_loss: 0.7730 - activation_1_loss: 0.7211\n",
      "Epoch 00003: val_loss did not improve from 1.61712\n",
      "21985/21985 [==============================] - 283s 13ms/sample - loss: 1.4941 - activation_loss: 0.7725 - activation_1_loss: 0.7207 - val_loss: 1.6331 - val_activation_loss: 0.8400 - val_activation_1_loss: 0.7923\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.7106180211131843\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2569 - activation_loss: 1.1331 - activation_1_loss: 1.1239\n",
      "Epoch 00001: val_loss improved from inf to 1.71348, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 304s 14ms/sample - loss: 2.2568 - activation_loss: 1.1314 - activation_1_loss: 1.1222 - val_loss: 1.7135 - val_activation_loss: 0.8854 - val_activation_1_loss: 0.8284\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6306 - activation_loss: 0.8431 - activation_1_loss: 0.7875\n",
      "Epoch 00002: val_loss improved from 1.71348 to 1.63636, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 285s 13ms/sample - loss: 1.6305 - activation_loss: 0.8419 - activation_1_loss: 0.7879 - val_loss: 1.6364 - val_activation_loss: 0.8390 - val_activation_1_loss: 0.7976\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4881 - activation_loss: 0.7712 - activation_1_loss: 0.7169\n",
      "Epoch 00003: val_loss did not improve from 1.63636\n",
      "21985/21985 [==============================] - 284s 13ms/sample - loss: 1.4881 - activation_loss: 0.7744 - activation_1_loss: 0.7163 - val_loss: 1.6944 - val_activation_loss: 0.8685 - val_activation_1_loss: 0.8262\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 3 Jaccard = 0.7002291071038375\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1954 - activation_loss: 1.0837 - activation_1_loss: 1.1117\n",
      "Epoch 00001: val_loss improved from inf to 1.62854, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 303s 14ms/sample - loss: 2.1956 - activation_loss: 1.0866 - activation_1_loss: 1.1138 - val_loss: 1.6285 - val_activation_loss: 0.8308 - val_activation_1_loss: 0.7980\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6401 - activation_loss: 0.8481 - activation_1_loss: 0.7920\n",
      "Epoch 00002: val_loss improved from 1.62854 to 1.60438, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 285s 13ms/sample - loss: 1.6400 - activation_loss: 0.8469 - activation_1_loss: 0.7909 - val_loss: 1.6044 - val_activation_loss: 0.8062 - val_activation_1_loss: 0.7985\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4948 - activation_loss: 0.7745 - activation_1_loss: 0.7203\n",
      "Epoch 00003: val_loss did not improve from 1.60438\n",
      "21985/21985 [==============================] - 284s 13ms/sample - loss: 1.4948 - activation_loss: 0.7741 - activation_1_loss: 0.7193 - val_loss: 1.6240 - val_activation_loss: 0.8277 - val_activation_1_loss: 0.7960\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 4 Jaccard = 0.7091553487270413\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1279 - activation_loss: 1.0635 - activation_1_loss: 1.0644\n",
      "Epoch 00001: val_loss improved from inf to 1.62298, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 304s 14ms/sample - loss: 2.1281 - activation_loss: 1.0684 - activation_1_loss: 1.0652 - val_loss: 1.6230 - val_activation_loss: 0.8483 - val_activation_1_loss: 0.7740\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6480 - activation_loss: 0.8483 - activation_1_loss: 0.7996\n",
      "Epoch 00002: val_loss improved from 1.62298 to 1.59412, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 285s 13ms/sample - loss: 1.6480 - activation_loss: 0.8483 - activation_1_loss: 0.7993 - val_loss: 1.5941 - val_activation_loss: 0.8407 - val_activation_1_loss: 0.7527\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4995 - activation_loss: 0.7736 - activation_1_loss: 0.7259\n",
      "Epoch 00003: val_loss did not improve from 1.59412\n",
      "21985/21985 [==============================] - 284s 13ms/sample - loss: 1.4994 - activation_loss: 0.7734 - activation_1_loss: 0.7254 - val_loss: 1.6126 - val_activation_loss: 0.8430 - val_activation_1_loss: 0.7692\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 5 Jaccard = 0.7089732018664447\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3235</th>\n",
       "      <td>665dcd4cf2</td>\n",
       "      <td>Yes.  Me too. I can`t understand why they have to do th...</td>\n",
       "      <td>positive</td>\n",
       "      <td>love.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>58f1b52257</td>\n",
       "      <td>i want to gooo</td>\n",
       "      <td>positive</td>\n",
       "      <td>i want to gooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>320a6e56fc</td>\n",
       "      <td>Every pair of jeans I own nowadays is very tight.  I thi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>tight.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2714</th>\n",
       "      <td>f525424cfe</td>\n",
       "      <td>i ate too much ice cream and curly fries</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i ate too much ice cream and curly fries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>8e4553e921</td>\n",
       "      <td>Let`s go!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>let`s go!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3236</th>\n",
       "      <td>0b4a2b2eb1</td>\n",
       "      <td>I hve a blister on my pinky nd it hurts soooo much!</td>\n",
       "      <td>negative</td>\n",
       "      <td>hurts soooo much!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>f420223044</td>\n",
       "      <td>Take me with u eric</td>\n",
       "      <td>neutral</td>\n",
       "      <td>take me with u eric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>21c6854561</td>\n",
       "      <td>I was going to but I`m stuck with it as I seem to have ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>c7e52408c8</td>\n",
       "      <td>Hour fifteen drive. Just left, bored and ate half my foo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>816bbfcd52</td>\n",
       "      <td>HAPPY MOTHERS DAY 2 ME</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>2334ad25e6</td>\n",
       "      <td>OOO YUK !! thats not good Im retching as we tweet</td>\n",
       "      <td>negative</td>\n",
       "      <td>thats not good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>62dd5efb10</td>\n",
       "      <td>-g`mornin` Twitterville  took the day off from Tweet`n y...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-g`mornin` twitterville took the day off from tweet`n y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>b221ecd789</td>\n",
       "      <td>Birthday girl in the house!! Tweet tweet suckas</td>\n",
       "      <td>negative</td>\n",
       "      <td>suckas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>4b273cd32a</td>\n",
       "      <td>Thank you! You should know that I am both happy and sad...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>thank you! you should know that i am both happy and sad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3321</th>\n",
       "      <td>a00d6e8a18</td>\n",
       "      <td>Hmm, had first pizza in ages and feel a bit sick now  Th...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hmm, had first pizza in ages and feel a bit sick now th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>bab53b4152</td>\n",
       "      <td>I tried sooooo hard to work from home today, but failed...</td>\n",
       "      <td>negative</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3356</th>\n",
       "      <td>4e1101e520</td>\n",
       "      <td>Its weird being at the guy`s house without them here. I ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i don`t like it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>49c713c76d</td>\n",
       "      <td>Is It The Bit Where Hollie Started Crying?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>is it the bit where hollie started crying?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>6a14956779</td>\n",
       "      <td>If we don`t pack, she can`t leave, right?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>if we don`t pack, she can`t leave, right?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>2fdd2501d1</td>\n",
       "      <td>Anyone have some advice??? I need it!!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anyone have some advice??? i need it!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>80420cfe74</td>\n",
       "      <td>oh no! my fun weekend with friends is gone! my mother ha...</td>\n",
       "      <td>negative</td>\n",
       "      <td>oh no! my fun weekend with friends is gone!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>a0bfb12773</td>\n",
       "      <td>You seem nice, you`re generous and you know your stuff</td>\n",
       "      <td>positive</td>\n",
       "      <td>you seem nice,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>11b14ae498</td>\n",
       "      <td>gonna go 2 bed now, night tweeters  , dont think about m...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>gonna go 2 bed now, night tweeters , dont think about m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>1dda981140</td>\n",
       "      <td>and so another week begins. this one has got to be bette...</td>\n",
       "      <td>positive</td>\n",
       "      <td>better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2733</th>\n",
       "      <td>d31f0597f5</td>\n",
       "      <td>That`s tonight?! Cool</td>\n",
       "      <td>positive</td>\n",
       "      <td>cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>760723a562</td>\n",
       "      <td>Sore throat. This is not good. I have four performances ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sore throat. this is not good.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3444</th>\n",
       "      <td>7d32d4866a</td>\n",
       "      <td>You are sooo lucky. My fiance is away w/the Marine Corp...</td>\n",
       "      <td>positive</td>\n",
       "      <td>you are sooo lucky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>109b636fa5</td>\n",
       "      <td>I thought  was against licensing stores bc it takes aw...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i thought was against licensing stores bc it takes away...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>4e05652b8c</td>\n",
       "      <td>Time to play the drums</td>\n",
       "      <td>neutral</td>\n",
       "      <td>time to play the drums</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>1305139c4a</td>\n",
       "      <td>_kikireestl nooo. you were on my yahoo account. hmm. i w...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>_kikireestl nooo. you were on my yahoo account. hmm. i ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "3235  665dcd4cf2   Yes.  Me too. I can`t understand why they have to do th...   \n",
       "438   58f1b52257                                               i want to gooo   \n",
       "1140  320a6e56fc  Every pair of jeans I own nowadays is very tight.  I thi...   \n",
       "2714  f525424cfe                     i ate too much ice cream and curly fries   \n",
       "676   8e4553e921                                                    Let`s go!   \n",
       "3236  0b4a2b2eb1          I hve a blister on my pinky nd it hurts soooo much!   \n",
       "2431  f420223044                                          Take me with u eric   \n",
       "1805  21c6854561   I was going to but I`m stuck with it as I seem to have ...   \n",
       "2885  c7e52408c8  Hour fifteen drive. Just left, bored and ate half my foo...   \n",
       "1460  816bbfcd52                                       HAPPY MOTHERS DAY 2 ME   \n",
       "1498  2334ad25e6            OOO YUK !! thats not good Im retching as we tweet   \n",
       "1470  62dd5efb10  -g`mornin` Twitterville  took the day off from Tweet`n y...   \n",
       "2330  b221ecd789              Birthday girl in the house!! Tweet tweet suckas   \n",
       "425   4b273cd32a   Thank you! You should know that I am both happy and sad...   \n",
       "3321  a00d6e8a18  Hmm, had first pizza in ages and feel a bit sick now  Th...   \n",
       "996   bab53b4152   I tried sooooo hard to work from home today, but failed...   \n",
       "3356  4e1101e520  Its weird being at the guy`s house without them here. I ...   \n",
       "2418  49c713c76d                   Is It The Bit Where Hollie Started Crying?   \n",
       "176   6a14956779                    If we don`t pack, she can`t leave, right?   \n",
       "388   2fdd2501d1                       Anyone have some advice??? I need it!!   \n",
       "880   80420cfe74  oh no! my fun weekend with friends is gone! my mother ha...   \n",
       "55    a0bfb12773       You seem nice, you`re generous and you know your stuff   \n",
       "1190  11b14ae498  gonna go 2 bed now, night tweeters  , dont think about m...   \n",
       "2291  1dda981140  and so another week begins. this one has got to be bette...   \n",
       "2733  d31f0597f5                                        That`s tonight?! Cool   \n",
       "1148  760723a562  Sore throat. This is not good. I have four performances ...   \n",
       "3444  7d32d4866a   You are sooo lucky. My fiance is away w/the Marine Corp...   \n",
       "393   109b636fa5    I thought  was against licensing stores bc it takes aw...   \n",
       "2687  4e05652b8c                                       Time to play the drums   \n",
       "3351  1305139c4a  _kikireestl nooo. you were on my yahoo account. hmm. i w...   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "3235  positive                                                        love.  \n",
       "438   positive                                               i want to gooo  \n",
       "1140  negative                                                       tight.  \n",
       "2714   neutral                     i ate too much ice cream and curly fries  \n",
       "676    neutral                                                    let`s go!  \n",
       "3236  negative                                            hurts soooo much!  \n",
       "2431   neutral                                          take me with u eric  \n",
       "1805  negative                                                          bad  \n",
       "2885  negative                                                        bored  \n",
       "1460  positive                                                        happy  \n",
       "1498  negative                                               thats not good  \n",
       "1470   neutral   -g`mornin` twitterville took the day off from tweet`n y...  \n",
       "2330  negative                                                       suckas  \n",
       "425    neutral   thank you! you should know that i am both happy and sad...  \n",
       "3321   neutral   hmm, had first pizza in ages and feel a bit sick now th...  \n",
       "996   negative                                                       failed  \n",
       "3356  negative                                              i don`t like it  \n",
       "2418   neutral                   is it the bit where hollie started crying?  \n",
       "176    neutral                    if we don`t pack, she can`t leave, right?  \n",
       "388    neutral                       anyone have some advice??? i need it!!  \n",
       "880   negative                  oh no! my fun weekend with friends is gone!  \n",
       "55    positive                                               you seem nice,  \n",
       "1190   neutral   gonna go 2 bed now, night tweeters , dont think about m...  \n",
       "2291  positive                                                       better  \n",
       "2733  positive                                                         cool  \n",
       "1148  negative                               sore throat. this is not good.  \n",
       "3444  positive                                          you are sooo lucky.  \n",
       "393    neutral   i thought was against licensing stores bc it takes away...  \n",
       "2687   neutral                                       time to play the drums  \n",
       "3351   neutral   _kikireestl nooo. you were on my yahoo account. hmm. i ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
