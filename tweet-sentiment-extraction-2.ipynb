{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preface**\n",
    "**This project was inspired and referenced in [TensorFlow roBERTa - [0.705]](https://www.kaggle.com/code/cdeotte/tensorflow-roberta-0-705)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1/ About team**\n",
    "|StuID  |        Name       |     Kaggle    |       Github      |\n",
    "|-------|-------------------|---------------|-------------------|\n",
    "|1752052|Dang Huu Phuoc Vinh|[V_Notebook](https://www.kaggle.com/danghuuphuocvinh)|[V_Github](https://github.com/DangHuuPhuocVinh/data_science_application)\n",
    "|1753097|Le Nguyen Minh Tam |[T_Notebook](https://www.kaggle.com/minhtamlenguyen)|[T_Github](https://github.com/lnmtam1999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2/ About competition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1/ Name of competition**\n",
    "**[Tweet Sentiment Extraction](https://www.kaggle.com/competitions/tweet-sentiment-extraction) organized by [Kaggle](https://www.kaggle.com/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2/ Prize**\n",
    "**15000 USD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.3/ Description**\n",
    "  **E.g: \"My ridiculous dog is amazing.\" [sentiment: positive]**\n",
    "\n",
    "  **With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.**\n",
    "\n",
    "  **In this competition we've extracted support phrases from [Figure Eight's Data for Everyone platform](https://appen.com/datasets-resource-center/). The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.4/ Input and Output**\n",
    "- **Input: textID, text and sentiment**\n",
    "- **Output: selected_text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.5/ Columns**\n",
    "-  **textID - unique ID for each piece of text**\n",
    "-  **text - the text of the tweet**\n",
    "-  **sentiment - the general sentiment of the tweet**\n",
    "-  **selected_text - [train only] the text that supports the tweet's sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.6/ Meaningful**\n",
    "- **After doing this project, we can have a dataset with the phrases that were selected for using at any other NLP project**\n",
    "- **Can use for detecting some keywords that have sentiment** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.7/ Evaluation**\n",
    " ![img](https://user-images.githubusercontent.com/35680794/174698744-57b2f116-fbe4-4fb6-9216-2e83e0494dca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3/ Model roBERTa for this project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4/ Developing the project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1/ Import Libraries, Data and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:37.249683Z",
     "iopub.status.busy": "2022-07-10T19:03:37.249400Z",
     "iopub.status.idle": "2022-07-10T19:03:37.256307Z",
     "shell.execute_reply": "2022-07-10T19:03:37.255411Z",
     "shell.execute_reply.started": "2022-07-10T19:03:37.249641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1.1/ Tokenizer**\n",
    "First of all, we use tokenizer to convert the word to array for the computer can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:40.525850Z",
     "iopub.status.busy": "2022-07-10T19:03:40.525552Z",
     "iopub.status.idle": "2022-07-10T19:03:40.902687Z",
     "shell.execute_reply": "2022-07-10T19:03:40.901896Z",
     "shell.execute_reply.started": "2022-07-10T19:03:40.525819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 95\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAX_LEN** = 96 then for each training row, RoBERTa receives 96 tokens. The reason to use **lowercase** and **add_prefix_space** because when spelling with RoBERTa :\" helllo\", \"hello\", \" Hello\", and \"Hello\" use the same \" hello\" token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2/ Training data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.1/ Adjust the input**\n",
    "In this stage we ready our data for the model, all the input will be change to numerical and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:44.266109Z",
     "iopub.status.busy": "2022-07-10T19:03:44.265840Z",
     "iopub.status.idle": "2022-07-10T19:03:53.105947Z",
     "shell.execute_reply": "2022-07-10T19:03:53.105081Z",
     "shell.execute_reply.started": "2022-07-10T19:03:44.266080Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[    0   939 12905 ...     1     1     1]\n",
      " [    0    98  3036 ...     1     1     1]\n",
      " [    0   127  3504 ...     1     1     1]\n",
      " ...\n",
      " [    0  1423   857 ...     1     1     1]\n",
      " [    0    53    24 ...     1     1     1]\n",
      " [    0    70    42 ...     1     1     1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [(0, 4), (4, 9), (9, 12), (12, 18), (18, 24), (24, 27), (27, 29), (29, 33), (33, 36), (36, 37), (37, 44), (44, 45), (45, 47), (47, 49), (49, 50), (50, 53), (53, 54), (54, 57), (57, 59)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3/ Test Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same as the Training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:04:17.944124Z",
     "iopub.status.busy": "2022-07-10T19:04:17.943856Z",
     "iopub.status.idle": "2022-07-10T19:04:18.241347Z",
     "shell.execute_reply": "2022-07-10T19:04:18.240640Z",
     "shell.execute_reply.started": "2022-07-10T19:04:17.944095Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the original text is [ i][ am][ having][ a][ great][ day]</s>[ positive]</s> with tokens 0,1,2,3,4,5,6,7,8,9,10 and the selected text is \"great day\", then the training has start index = 5 and end index = 6.\n",
    "\n",
    "If our model also predicts a = 5 and b = 6 and we try to select the text from [ i][ am][ having][ a][ great][ day], the indices 5 and 6 will not return \"great day\". Instead we must use tokens[4:6] to get great day. We subtract 1 because the is now removed. And we add 1 to b because python indexing for list[3:5] does not return 5 it only returns 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4/ Build roBERTa model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4.1/ Building the bones of model**\n",
    "Built the bones of a roBERTa model, using the model has already train by the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:05:21.352602Z",
     "iopub.status.busy": "2022-07-10T19:05:21.352259Z",
     "iopub.status.idle": "2022-07-10T19:05:21.364895Z",
     "shell.execute_reply": "2022-07-10T19:05:21.364038Z",
     "shell.execute_reply.started": "2022-07-10T19:05:21.352569Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5/ Create metric**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We code this for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:06:16.388950Z",
     "iopub.status.busy": "2022-07-10T19:06:16.388632Z",
     "iopub.status.idle": "2022-07-10T19:06:16.395186Z",
     "shell.execute_reply": "2022-07-10T19:06:16.394374Z",
     "shell.execute_reply.started": "2022-07-10T19:06:16.388923Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.6/ Train roBERTa model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:07:44.465971Z",
     "iopub.status.busy": "2022-07-10T19:07:44.465660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.2693 - activation_loss: 1.1050 - activation_1_loss: 1.1643\n",
      "Epoch 00001: val_loss improved from inf to 1.76629, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 289s 13ms/sample - loss: 2.2701 - activation_loss: 1.1049 - activation_1_loss: 1.1652 - val_loss: 1.7663 - val_activation_loss: 0.8977 - val_activation_1_loss: 0.8682\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.7410 - activation_loss: 0.8615 - activation_1_loss: 0.8795\n",
      "Epoch 00002: val_loss improved from 1.76629 to 1.72833, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 272s 12ms/sample - loss: 1.7406 - activation_loss: 0.8613 - activation_1_loss: 0.8793 - val_loss: 1.7283 - val_activation_loss: 0.8722 - val_activation_1_loss: 0.8558\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.6104 - activation_loss: 0.8102 - activation_1_loss: 0.8002\n",
      "Epoch 00003: val_loss did not improve from 1.72833\n",
      "21984/21984 [==============================] - 270s 12ms/sample - loss: 1.6101 - activation_loss: 0.8099 - activation_1_loss: 0.8001 - val_loss: 1.7582 - val_activation_loss: 0.8852 - val_activation_1_loss: 0.8732\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.6892963295559003\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2034 - activation_loss: 1.0916 - activation_1_loss: 1.1117\n",
      "Epoch 00001: val_loss improved from inf to 1.75245, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 304s 14ms/sample - loss: 2.2033 - activation_loss: 1.0901 - activation_1_loss: 1.1101 - val_loss: 1.7525 - val_activation_loss: 0.8691 - val_activation_1_loss: 0.8826\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.7714 - activation_loss: 0.8745 - activation_1_loss: 0.8969\n",
      "Epoch 00002: val_loss did not improve from 1.75245\n",
      "21985/21985 [==============================] - 283s 13ms/sample - loss: 1.7716 - activation_loss: 0.8734 - activation_1_loss: 0.9050 - val_loss: 1.9698 - val_activation_loss: 0.8945 - val_activation_1_loss: 1.0744\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.7011 - activation_loss: 0.8457 - activation_1_loss: 0.8553\n",
      "Epoch 00003: val_loss improved from 1.75245 to 1.69068, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 283s 13ms/sample - loss: 1.7011 - activation_loss: 0.8465 - activation_1_loss: 0.8553 - val_loss: 1.6907 - val_activation_loss: 0.8566 - val_activation_1_loss: 0.8332\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 26s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.7057148037286022\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1909 - activation_loss: 1.0774 - activation_1_loss: 1.1135\n",
      "Epoch 00001: val_loss improved from inf to 1.68750, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 303s 14ms/sample - loss: 2.1908 - activation_loss: 1.0765 - activation_1_loss: 1.1119 - val_loss: 1.6875 - val_activation_loss: 0.8662 - val_activation_1_loss: 0.8216\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6453 - activation_loss: 0.8480 - activation_1_loss: 0.7973\n",
      "Epoch 00002: val_loss improved from 1.68750 to 1.64826, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 284s 13ms/sample - loss: 1.6452 - activation_loss: 0.8470 - activation_1_loss: 0.7972 - val_loss: 1.6483 - val_activation_loss: 0.8555 - val_activation_1_loss: 0.7930\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5024 - activation_loss: 0.7797 - activation_1_loss: 0.7227\n",
      "Epoch 00003: val_loss did not improve from 1.64826\n",
      "21985/21985 [==============================] - 282s 13ms/sample - loss: 1.5023 - activation_loss: 0.7785 - activation_1_loss: 0.7217 - val_loss: 1.6836 - val_activation_loss: 0.8803 - val_activation_1_loss: 0.8038\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 3 Jaccard = 0.7050076301508549\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2004 - activation_loss: 1.0996 - activation_1_loss: 1.1007\n",
      "Epoch 00001: val_loss improved from inf to 1.64464, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 304s 14ms/sample - loss: 2.2003 - activation_loss: 1.0988 - activation_1_loss: 1.1001 - val_loss: 1.6446 - val_activation_loss: 0.8334 - val_activation_1_loss: 0.8114\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6390 - activation_loss: 0.8513 - activation_1_loss: 0.7877\n",
      "Epoch 00002: val_loss improved from 1.64464 to 1.60327, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 284s 13ms/sample - loss: 1.6389 - activation_loss: 0.8501 - activation_1_loss: 0.7866 - val_loss: 1.6033 - val_activation_loss: 0.8309 - val_activation_1_loss: 0.7723\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4762 - activation_loss: 0.7667 - activation_1_loss: 0.7094\n",
      "Epoch 00003: val_loss did not improve from 1.60327\n",
      "21985/21985 [==============================] - 282s 13ms/sample - loss: 1.4761 - activation_loss: 0.7656 - activation_1_loss: 0.7084 - val_loss: 1.6169 - val_activation_loss: 0.8434 - val_activation_1_loss: 0.7738\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 4 Jaccard = 0.7076396371312812\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1792 - activation_loss: 1.0723 - activation_1_loss: 1.1070\n",
      "Epoch 00001: val_loss improved from inf to 1.67515, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 304s 14ms/sample - loss: 2.1792 - activation_loss: 1.0710 - activation_1_loss: 1.1055 - val_loss: 1.6752 - val_activation_loss: 0.8597 - val_activation_1_loss: 0.8145\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6612 - activation_loss: 0.8505 - activation_1_loss: 0.8107\n",
      "Epoch 00002: val_loss improved from 1.67515 to 1.62164, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 284s 13ms/sample - loss: 1.6611 - activation_loss: 0.8493 - activation_1_loss: 0.8095 - val_loss: 1.6216 - val_activation_loss: 0.8516 - val_activation_1_loss: 0.7693\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5066 - activation_loss: 0.7766 - activation_1_loss: 0.7300\n",
      "Epoch 00003: val_loss improved from 1.62164 to 1.59029, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 284s 13ms/sample - loss: 1.5065 - activation_loss: 0.7760 - activation_1_loss: 0.7295 - val_loss: 1.5903 - val_activation_loss: 0.8318 - val_activation_1_loss: 0.7578\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 26s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 5 Jaccard = 0.7111440836734347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.7/ Kaggle submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3475</th>\n",
       "      <td>c956722677</td>\n",
       "      <td>My room is too hot to sleep in.</td>\n",
       "      <td>negative</td>\n",
       "      <td>my room is too hot to sleep in.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2908</th>\n",
       "      <td>7067e6046d</td>\n",
       "      <td>saw a whole lot of your stuff at Africa Joy Casterbridg...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thrilled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3193</th>\n",
       "      <td>d6b053947f</td>\n",
       "      <td>not at all happy...dont know what the reason is</td>\n",
       "      <td>negative</td>\n",
       "      <td>not at all happy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>1e2e4371a9</td>\n",
       "      <td>i hate stupid boys! arrgh</td>\n",
       "      <td>negative</td>\n",
       "      <td>i hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2870</th>\n",
       "      <td>41bb347c8c</td>\n",
       "      <td>The 3d version of Up sold out   regular version it is!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>the 3d version of up sold out regular version it is!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>a4d578eeb1</td>\n",
       "      <td>Is at work....  boo!  The Pin-Ups will be playing at Unc...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>is at work.... boo! the pin-ups will be playing at unco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>4cc7d9995d</td>\n",
       "      <td>gosh im bored. its early..n i wanna go..to sleeeepp why ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>gosh im bored.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>4fb2131d08</td>\n",
       "      <td>aww you are so helpful  we getting sweepy here, sis say...</td>\n",
       "      <td>positive</td>\n",
       "      <td>aww you are so helpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3480</th>\n",
       "      <td>50ba66785b</td>\n",
       "      <td>go on msn. i need to talk to you about something that h...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>go on msn. i need to talk to you about something that h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>37b50ef2a5</td>\n",
       "      <td>very sweet!!! HAHA. I am like super proud to be a new ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>very sweet!!! haha. i am like super proud to be a new m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2709</th>\n",
       "      <td>f347bda133</td>\n",
       "      <td>will be working at 7/11 this summer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>will be working at 7/11 this summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>56395aff30</td>\n",
       "      <td>Just finished watching the last of Star Wars</td>\n",
       "      <td>neutral</td>\n",
       "      <td>just finished watching the last of star wars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>bcb9426abb</td>\n",
       "      <td>I wish I could go to #BEA this weekend.</td>\n",
       "      <td>positive</td>\n",
       "      <td>wish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>ed3f95dd2a</td>\n",
       "      <td>I hope so ^^ Eclipse is up and running again btw!  http...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i hope so ^^ eclipse is up and running again btw! http:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>c10d6fac3d</td>\n",
       "      <td>I`m gonna cry    I went bad at my History test ! I reall...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i`m gonna cry i went bad at my history test ! i really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>d08af78eef</td>\n",
       "      <td>in august with anberlin. they`re not headlining though ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>in august with anberlin. they`re not headlining though ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>6c843b2f56</td>\n",
       "      <td>i dunno if he saw it  happy birthday girl!</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>887e90ff88</td>\n",
       "      <td>ew.  sorry zach</td>\n",
       "      <td>negative</td>\n",
       "      <td>sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>cd60606781</td>\n",
       "      <td>*shrugs* So do I but it was too funny plus you know yo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>love you..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>97dbf6e8e1</td>\n",
       "      <td>they do indeed  glad to hear everything is good with yo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>glad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>6e059414a2</td>\n",
       "      <td>_Cake just  little stomach bug, nothing serious</td>\n",
       "      <td>positive</td>\n",
       "      <td>nothing serious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1e3561e921</td>\n",
       "      <td>lol man i got 2 1 /2 hrs an iont how i woulda made it w...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>lol man i got 2 1 /2 hrs an iont how i woulda made it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>dce070190c</td>\n",
       "      <td>I`m at work....bored out of my mind.</td>\n",
       "      <td>negative</td>\n",
       "      <td>bored out of my mind.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2621</th>\n",
       "      <td>5875847743</td>\n",
       "      <td>First surprise birthday ever, best night I`ve had in a *...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>first surprise birthday ever, best night i`ve had in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>12d70a758e</td>\n",
       "      <td>why is it so **** cold ??!</td>\n",
       "      <td>negative</td>\n",
       "      <td>why is it so **** cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>6957e5f1bf</td>\n",
       "      <td>How come?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>how come?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514</th>\n",
       "      <td>7f62c60ffe</td>\n",
       "      <td>hey plz look &gt; http://www.twitpic.com/5m7vd &lt; what do u...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hey plz look &gt; http://www.twitpic.com/5m7vd &lt; what do u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862</th>\n",
       "      <td>2b18f3c81f</td>\n",
       "      <td>Im so tired and sick  i have to be better on Sunday so i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>im so tired and sick i have to be better on sunday so i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2665</th>\n",
       "      <td>87bfbb125b</td>\n",
       "      <td>just took my shirt off and my back is COVERED in blister...</td>\n",
       "      <td>negative</td>\n",
       "      <td>just took my shirt off and my back is covered in blisters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>50eb2a08c3</td>\n",
       "      <td>Going to Al Ain.  Need to check out one store. Hope i fi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "3475  c956722677                              My room is too hot to sleep in.   \n",
       "2908  7067e6046d   saw a whole lot of your stuff at Africa Joy Casterbridg...   \n",
       "3193  d6b053947f              not at all happy...dont know what the reason is   \n",
       "1520  1e2e4371a9                                    i hate stupid boys! arrgh   \n",
       "2870  41bb347c8c       The 3d version of Up sold out   regular version it is!   \n",
       "1763  a4d578eeb1  Is at work....  boo!  The Pin-Ups will be playing at Unc...   \n",
       "1082  4cc7d9995d  gosh im bored. its early..n i wanna go..to sleeeepp why ...   \n",
       "1953  4fb2131d08   aww you are so helpful  we getting sweepy here, sis say...   \n",
       "3480  50ba66785b   go on msn. i need to talk to you about something that h...   \n",
       "1081  37b50ef2a5    very sweet!!! HAHA. I am like super proud to be a new ...   \n",
       "2709  f347bda133                          will be working at 7/11 this summer   \n",
       "1137  56395aff30                 Just finished watching the last of Star Wars   \n",
       "137   bcb9426abb                      I wish I could go to #BEA this weekend.   \n",
       "2969  ed3f95dd2a   I hope so ^^ Eclipse is up and running again btw!  http...   \n",
       "1106  c10d6fac3d  I`m gonna cry    I went bad at my History test ! I reall...   \n",
       "3068  d08af78eef   in august with anberlin. they`re not headlining though ...   \n",
       "704   6c843b2f56                   i dunno if he saw it  happy birthday girl!   \n",
       "1616  887e90ff88                                              ew.  sorry zach   \n",
       "2275  cd60606781    *shrugs* So do I but it was too funny plus you know yo...   \n",
       "3272  97dbf6e8e1   they do indeed  glad to hear everything is good with yo...   \n",
       "2591  6e059414a2              _Cake just  little stomach bug, nothing serious   \n",
       "85    1e3561e921   lol man i got 2 1 /2 hrs an iont how i woulda made it w...   \n",
       "1042  dce070190c                         I`m at work....bored out of my mind.   \n",
       "2621  5875847743  First surprise birthday ever, best night I`ve had in a *...   \n",
       "3004  12d70a758e                                   why is it so **** cold ??!   \n",
       "1537  6957e5f1bf                                                    How come?   \n",
       "1514  7f62c60ffe   hey plz look > http://www.twitpic.com/5m7vd < what do u...   \n",
       "2862  2b18f3c81f  Im so tired and sick  i have to be better on Sunday so i...   \n",
       "2665  87bfbb125b  just took my shirt off and my back is COVERED in blister...   \n",
       "778   50eb2a08c3  Going to Al Ain.  Need to check out one store. Hope i fi...   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "3475  negative                              my room is too hot to sleep in.  \n",
       "2908  positive                                                     thrilled  \n",
       "3193  negative                                          not at all happy...  \n",
       "1520  negative                                                       i hate  \n",
       "2870   neutral         the 3d version of up sold out regular version it is!  \n",
       "1763   neutral   is at work.... boo! the pin-ups will be playing at unco...  \n",
       "1082  negative                                               gosh im bored.  \n",
       "1953  positive                                       aww you are so helpful  \n",
       "3480   neutral   go on msn. i need to talk to you about something that h...  \n",
       "1081  positive   very sweet!!! haha. i am like super proud to be a new m...  \n",
       "2709   neutral                          will be working at 7/11 this summer  \n",
       "1137   neutral                 just finished watching the last of star wars  \n",
       "137   positive                                                         wish  \n",
       "2969   neutral   i hope so ^^ eclipse is up and running again btw! http:...  \n",
       "1106  negative   i`m gonna cry i went bad at my history test ! i really ...  \n",
       "3068   neutral   in august with anberlin. they`re not headlining though ...  \n",
       "704   positive                                                        happy  \n",
       "1616  negative                                                        sorry  \n",
       "2275  positive                                                   love you..  \n",
       "3272  positive                                                         glad  \n",
       "2591  positive                                              nothing serious  \n",
       "85     neutral   lol man i got 2 1 /2 hrs an iont how i woulda made it w...  \n",
       "1042  negative                                        bored out of my mind.  \n",
       "2621   neutral   first surprise birthday ever, best night i`ve had in a ...  \n",
       "3004  negative                                       why is it so **** cold  \n",
       "1537   neutral                                                    how come?  \n",
       "1514   neutral   hey plz look > http://www.twitpic.com/5m7vd < what do u...  \n",
       "2862   neutral   im so tired and sick i have to be better on sunday so i...  \n",
       "2665  negative    just took my shirt off and my back is covered in blisters  \n",
       "778   positive                                                         hope  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
