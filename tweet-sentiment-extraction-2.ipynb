{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preface**\n",
    "**This project was inspired and referenced in [TensorFlow roBERTa - [0.705]](https://www.kaggle.com/code/cdeotte/tensorflow-roberta-0-705)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1/ About team**\n",
    "|StuID  |        Name       |     Kaggle    |       Github      |\n",
    "|-------|-------------------|---------------|-------------------|\n",
    "|1752052|Dang Huu Phuoc Vinh|[V_Notebook](https://www.kaggle.com/danghuuphuocvinh)|[V_Github](https://github.com/DangHuuPhuocVinh/data_science_application)\n",
    "|1753097|Le Nguyen Minh Tam |[T_Notebook](https://www.kaggle.com/minhtamlenguyen)|[T_Github](https://github.com/lnmtam1999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2/ About competition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1/ Name of competition**\n",
    "**[Tweet Sentiment Extraction](https://www.kaggle.com/competitions/tweet-sentiment-extraction) organized by [Kaggle](https://www.kaggle.com/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2/ Prize**\n",
    "**15000 USD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.3/ Description**\n",
    "  **E.g: \"My ridiculous dog is amazing.\" [sentiment: positive]**\n",
    "\n",
    "  **With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.**\n",
    "\n",
    "  **In this competition we've extracted support phrases from [Figure Eight's Data for Everyone platform](https://appen.com/datasets-resource-center/). The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.4/ Input and Output**\n",
    "- **Input: textID, text and sentiment**\n",
    "- **Output: selected_text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.5/ Columns**\n",
    "-  **textID - unique ID for each piece of text**\n",
    "-  **text - the text of the tweet**\n",
    "-  **sentiment - the general sentiment of the tweet**\n",
    "-  **selected_text - [train only] the text that supports the tweet's sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.6/ Meaningful**\n",
    "- **After doing this project, we can have a dataset with the phrases that were selected for using at any other NLP project**\n",
    "- **Can use for detecting some keywords that have sentiment** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.7/ Evaluation**\n",
    " ![img](https://user-images.githubusercontent.com/35680794/174698744-57b2f116-fbe4-4fb6-9216-2e83e0494dca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3/ Model roBERTa for this project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4/ Developing the project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1/ Import Libraries, Data and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:37.249683Z",
     "iopub.status.busy": "2022-07-10T19:03:37.249400Z",
     "iopub.status.idle": "2022-07-10T19:03:37.256307Z",
     "shell.execute_reply": "2022-07-10T19:03:37.255411Z",
     "shell.execute_reply.started": "2022-07-10T19:03:37.249641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1.1/ Tokenizer**\n",
    "First of all, we use tokenizer to convert the word to array for the computer can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:40.525850Z",
     "iopub.status.busy": "2022-07-10T19:03:40.525552Z",
     "iopub.status.idle": "2022-07-10T19:03:40.902687Z",
     "shell.execute_reply": "2022-07-10T19:03:40.901896Z",
     "shell.execute_reply.started": "2022-07-10T19:03:40.525819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 96\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAX_LEN** = 96 then for each training row, RoBERTa receives 96 tokens. The reason to use **lowercase** and **add_prefix_space** because when spelling with RoBERTa :\" helllo\", \"hello\", \" Hello\", and \"Hello\" use the same \" hello\" token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2/ Training data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.1/ Adjust the input**\n",
    "In this stage we ready our data for the model, all the input will be change to numerical and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:44.266109Z",
     "iopub.status.busy": "2022-07-10T19:03:44.265840Z",
     "iopub.status.idle": "2022-07-10T19:03:53.105947Z",
     "shell.execute_reply": "2022-07-10T19:03:53.105081Z",
     "shell.execute_reply.started": "2022-07-10T19:03:44.266080Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[    0   939 12905 ...     1     1     1]\n",
      " [    0    98  3036 ...     1     1     1]\n",
      " [    0   127  3504 ...     1     1     1]\n",
      " ...\n",
      " [    0  1423   857 ...     1     1     1]\n",
      " [    0    53    24 ...     1     1     1]\n",
      " [    0    70    42 ...     1     1     1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [(0, 4), (4, 9), (9, 12), (12, 18), (18, 24), (24, 27), (27, 29), (29, 33), (33, 36), (36, 37), (37, 44), (44, 45), (45, 47), (47, 49), (49, 50), (50, 53), (53, 54), (54, 57), (57, 59)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3/ Test Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same as the Training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:04:17.944124Z",
     "iopub.status.busy": "2022-07-10T19:04:17.943856Z",
     "iopub.status.idle": "2022-07-10T19:04:18.241347Z",
     "shell.execute_reply": "2022-07-10T19:04:18.240640Z",
     "shell.execute_reply.started": "2022-07-10T19:04:17.944095Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the original text is [ i][ am][ having][ a][ great][ day]</s>[ positive]</s> with tokens 0,1,2,3,4,5,6,7,8,9,10 and the selected text is \"great day\", then the training has start index = 5 and end index = 6.\n",
    "\n",
    "If our model also predicts a = 5 and b = 6 and we try to select the text from [ i][ am][ having][ a][ great][ day], the indices 5 and 6 will not return \"great day\". Instead we must use tokens[4:6] to get great day. We subtract 1 because the is now removed. And we add 1 to b because python indexing for list[3:5] does not return 5 it only returns 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4/ Build roBERTa model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4.1/ Building the bones of model**\n",
    "Built the bones of a roBERTa model, using the model has already train by the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:05:21.352602Z",
     "iopub.status.busy": "2022-07-10T19:05:21.352259Z",
     "iopub.status.idle": "2022-07-10T19:05:21.364895Z",
     "shell.execute_reply": "2022-07-10T19:05:21.364038Z",
     "shell.execute_reply.started": "2022-07-10T19:05:21.352569Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5/ Create metric**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We code this for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:06:16.388950Z",
     "iopub.status.busy": "2022-07-10T19:06:16.388632Z",
     "iopub.status.idle": "2022-07-10T19:06:16.395186Z",
     "shell.execute_reply": "2022-07-10T19:06:16.394374Z",
     "shell.execute_reply.started": "2022-07-10T19:06:16.388923Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.6/ Train roBERTa model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:07:44.465971Z",
     "iopub.status.busy": "2022-07-10T19:07:44.465660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.1689 - activation_loss: 1.0836 - activation_1_loss: 1.0853\n",
      "Epoch 00001: val_loss improved from inf to 1.68852, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 292s 13ms/sample - loss: 2.1689 - activation_loss: 1.0834 - activation_1_loss: 1.0854 - val_loss: 1.6885 - val_activation_loss: 0.8785 - val_activation_1_loss: 0.8096\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.6151 - activation_loss: 0.8352 - activation_1_loss: 0.7799\n",
      "Epoch 00002: val_loss improved from 1.68852 to 1.66134, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 275s 12ms/sample - loss: 1.6151 - activation_loss: 0.8350 - activation_1_loss: 0.7801 - val_loss: 1.6613 - val_activation_loss: 0.8627 - val_activation_1_loss: 0.7985\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.4601 - activation_loss: 0.7543 - activation_1_loss: 0.7058\n",
      "Epoch 00003: val_loss did not improve from 1.66134\n",
      "21984/21984 [==============================] - 273s 12ms/sample - loss: 1.4599 - activation_loss: 0.7545 - activation_1_loss: 0.7054 - val_loss: 1.6653 - val_activation_loss: 0.8531 - val_activation_1_loss: 0.8119\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 28s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.6991084365826593\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2118 - activation_loss: 1.1076 - activation_1_loss: 1.1042\n",
      "Epoch 00001: val_loss improved from inf to 1.69185, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 312s 14ms/sample - loss: 2.2118 - activation_loss: 1.1060 - activation_1_loss: 1.1042 - val_loss: 1.6919 - val_activation_loss: 0.8670 - val_activation_1_loss: 0.8241\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6728 - activation_loss: 0.8613 - activation_1_loss: 0.8115\n",
      "Epoch 00002: val_loss improved from 1.69185 to 1.62501, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 290s 13ms/sample - loss: 1.6728 - activation_loss: 0.8601 - activation_1_loss: 0.8120 - val_loss: 1.6250 - val_activation_loss: 0.8342 - val_activation_1_loss: 0.7904\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5392 - activation_loss: 0.7938 - activation_1_loss: 0.7454\n",
      "Epoch 00003: val_loss improved from 1.62501 to 1.59097, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 290s 13ms/sample - loss: 1.5393 - activation_loss: 0.7952 - activation_1_loss: 0.7469 - val_loss: 1.5910 - val_activation_loss: 0.8225 - val_activation_1_loss: 0.7680\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 28s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.715686827625368\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1796 - activation_loss: 1.0812 - activation_1_loss: 1.0984\n",
      "Epoch 00001: val_loss improved from inf to 1.77509, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 312s 14ms/sample - loss: 2.1796 - activation_loss: 1.0812 - activation_1_loss: 1.0982 - val_loss: 1.7751 - val_activation_loss: 0.9269 - val_activation_1_loss: 0.8486\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6402 - activation_loss: 0.8410 - activation_1_loss: 0.7992\n",
      "Epoch 00002: val_loss improved from 1.77509 to 1.68709, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 289s 13ms/sample - loss: 1.6401 - activation_loss: 0.8398 - activation_1_loss: 0.7981 - val_loss: 1.6871 - val_activation_loss: 0.8538 - val_activation_1_loss: 0.8336\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4839 - activation_loss: 0.7648 - activation_1_loss: 0.7191\n",
      "Epoch 00003: val_loss improved from 1.68709 to 1.68489, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 289s 13ms/sample - loss: 1.4838 - activation_loss: 0.7640 - activation_1_loss: 0.7183 - val_loss: 1.6849 - val_activation_loss: 0.8590 - val_activation_1_loss: 0.8259\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 28s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 5ms/sample\n",
      ">>>> FOLD 3 Jaccard = 0.6962079225904324\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2090 - activation_loss: 1.0849 - activation_1_loss: 1.1241\n",
      "Epoch 00001: val_loss improved from inf to 1.69712, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 313s 14ms/sample - loss: 2.2089 - activation_loss: 1.0833 - activation_1_loss: 1.1235 - val_loss: 1.6971 - val_activation_loss: 0.8554 - val_activation_1_loss: 0.8419\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6391 - activation_loss: 0.8479 - activation_1_loss: 0.7912\n",
      "Epoch 00002: val_loss improved from 1.69712 to 1.59563, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 290s 13ms/sample - loss: 1.6391 - activation_loss: 0.8475 - activation_1_loss: 0.7908 - val_loss: 1.5956 - val_activation_loss: 0.8233 - val_activation_1_loss: 0.7723\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6296 - activation_loss: 0.8243 - activation_1_loss: 0.8054\n",
      "Epoch 00003: val_loss did not improve from 1.59563\n",
      "21985/21985 [==============================] - 288s 13ms/sample - loss: 1.6297 - activation_loss: 0.8262 - activation_1_loss: 0.8057 - val_loss: 1.6411 - val_activation_loss: 0.8235 - val_activation_1_loss: 0.8176\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 28s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 4 Jaccard = 0.7031572895861712\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1736 - activation_loss: 1.0810 - activation_1_loss: 1.0927\n",
      "Epoch 00001: val_loss improved from inf to 1.65943, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 313s 14ms/sample - loss: 2.1735 - activation_loss: 1.0794 - activation_1_loss: 1.0912 - val_loss: 1.6594 - val_activation_loss: 0.8785 - val_activation_1_loss: 0.7804\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6450 - activation_loss: 0.8454 - activation_1_loss: 0.7995\n",
      "Epoch 00002: val_loss improved from 1.65943 to 1.63568, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 290s 13ms/sample - loss: 1.6449 - activation_loss: 0.8443 - activation_1_loss: 0.7985 - val_loss: 1.6357 - val_activation_loss: 0.8565 - val_activation_1_loss: 0.7785\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5008 - activation_loss: 0.7702 - activation_1_loss: 0.7306\n",
      "Epoch 00003: val_loss did not improve from 1.63568\n",
      "21985/21985 [==============================] - 288s 13ms/sample - loss: 1.5008 - activation_loss: 0.7711 - activation_1_loss: 0.7297 - val_loss: 1.6475 - val_activation_loss: 0.8552 - val_activation_1_loss: 0.7913\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 28s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 5 Jaccard = 0.70348566447022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.7/ Kaggle submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2798</th>\n",
       "      <td>f68a3ece4b</td>\n",
       "      <td>_twitz lol...hiiii yourself. Maybe lay off the patron an...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>_twitz lol...hiiii yourself. maybe lay off the patron a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>0b2ccf0b7e</td>\n",
       "      <td>lucky e went!  jealous!</td>\n",
       "      <td>negative</td>\n",
       "      <td>jealous!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3508</th>\n",
       "      <td>60e5c2c335</td>\n",
       "      <td>chillin bored drinking a margarita. txt me</td>\n",
       "      <td>negative</td>\n",
       "      <td>bored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2854</th>\n",
       "      <td>f446d26bfc</td>\n",
       "      <td>Is going to school.  leave me stuff.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>is going to school. leave me stuff.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>0b8b37ba52</td>\n",
       "      <td>chris came over for exactly one hour  i moped around aft...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>chris came over for exactly one hour i moped around aft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1e3561e921</td>\n",
       "      <td>lol man i got 2 1 /2 hrs an iont how i woulda made it w...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>lol man i got 2 1 /2 hrs an iont how i woulda made it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>efba862a1b</td>\n",
       "      <td>I just laughed out loud on the bus reading your stateme...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i just laughed out loud on the bus reading your stateme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>e6b01d5777</td>\n",
       "      <td>OH MY GOD! OH MY GOD! OH MY GOD! New SUPERNATURAL starts...</td>\n",
       "      <td>positive</td>\n",
       "      <td>oh my god! oh my god! oh my god! new supernatural start...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>a1b12787c2</td>\n",
       "      <td>A rare treat b/c we`re rarely ap and at `em that early!...</td>\n",
       "      <td>positive</td>\n",
       "      <td>a rare treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>a5e47d75c0</td>\n",
       "      <td>Please send me those youtube links, Erin watched most o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>didn`t get to see the cowboys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>e34af4fd97</td>\n",
       "      <td>E.L.O.  wow, brings back so many happy memories.  LOVE ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>9b3fcf1668</td>\n",
       "      <td>Jesus Christ meadowhall could do with better air con! To...</td>\n",
       "      <td>negative</td>\n",
       "      <td>too hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>f4d1a681af</td>\n",
       "      <td>productive day 1</td>\n",
       "      <td>positive</td>\n",
       "      <td>productive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>3f09fa2af2</td>\n",
       "      <td>My new site is stuck in the Google sandpit now  Test of ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>my new site is stuck in the google sandpit now test of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3234</th>\n",
       "      <td>e2b73ce8df</td>\n",
       "      <td>wow its only 9:30</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wow its only 9:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>36589d3c2d</td>\n",
       "      <td>Homework. ew fml. Should I pull an all nighter? I think ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>homework. ew fml. should i pull an all nighter? i think...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>1962095a32</td>\n",
       "      <td>rofl that`s what we like to hear</td>\n",
       "      <td>positive</td>\n",
       "      <td>like to hear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>12005b65fc</td>\n",
       "      <td>Waiting for my turn on wii fit gym closed</td>\n",
       "      <td>neutral</td>\n",
       "      <td>waiting for my turn on wii fit gym closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ab57d75f20</td>\n",
       "      <td>im really sorry i know wallah how u feel this life is ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>im really sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>a7af432585</td>\n",
       "      <td>Transition time is over; tonight I work night shift. I w...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>transition time is over; tonight i work night shift. i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2957</th>\n",
       "      <td>7ae1499198</td>\n",
       "      <td>_august84 - requoting the gyaan - the more you live life...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>_august84 - requoting the gyaan - the more you live lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>c0b7788e74</td>\n",
       "      <td>my monday running so fast!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>my monday running so fast!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>32b2c8b924</td>\n",
       "      <td>Aww Stanley, have you still got him, they can live for ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>aww stanley, have you still got him, they can live for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3251</th>\n",
       "      <td>e37a7b9128</td>\n",
       "      <td>Ouch, give me a heads up so I`ll know when to duck</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ouch, give me a heads up so i`ll know when to duck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>09e0880c54</td>\n",
       "      <td>not here for long hun, head hurts</td>\n",
       "      <td>negative</td>\n",
       "      <td>hurts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>b78e0c4c19</td>\n",
       "      <td>Thank you Kirst - your posts on running inspired me</td>\n",
       "      <td>positive</td>\n",
       "      <td>thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>e939fe583c</td>\n",
       "      <td>sometimes spam leaks in to the message board.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sometimes spam leaks in to the message board.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>3c048c6a14</td>\n",
       "      <td>hey Jess where is Josh??</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hey jess where is josh??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>b9b532f7a2</td>\n",
       "      <td>U were great, as always. But, can`t we do an east Germa...</td>\n",
       "      <td>positive</td>\n",
       "      <td>u were great,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>213f65851d</td>\n",
       "      <td>Day is going well so far. Meeting until 4 though.</td>\n",
       "      <td>positive</td>\n",
       "      <td>day is going well</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "2798  f68a3ece4b  _twitz lol...hiiii yourself. Maybe lay off the patron an...   \n",
       "2260  0b2ccf0b7e                                      lucky e went!  jealous!   \n",
       "3508  60e5c2c335                   chillin bored drinking a margarita. txt me   \n",
       "2854  f446d26bfc                         Is going to school.  leave me stuff.   \n",
       "1718  0b8b37ba52  chris came over for exactly one hour  i moped around aft...   \n",
       "85    1e3561e921   lol man i got 2 1 /2 hrs an iont how i woulda made it w...   \n",
       "1571  efba862a1b   I just laughed out loud on the bus reading your stateme...   \n",
       "1683  e6b01d5777  OH MY GOD! OH MY GOD! OH MY GOD! New SUPERNATURAL starts...   \n",
       "2437  a1b12787c2   A rare treat b/c we`re rarely ap and at `em that early!...   \n",
       "1655  a5e47d75c0   Please send me those youtube links, Erin watched most o...   \n",
       "166   e34af4fd97   E.L.O.  wow, brings back so many happy memories.  LOVE ...   \n",
       "431   9b3fcf1668  Jesus Christ meadowhall could do with better air con! To...   \n",
       "853   f4d1a681af                                             productive day 1   \n",
       "2321  3f09fa2af2  My new site is stuck in the Google sandpit now  Test of ...   \n",
       "3234  e2b73ce8df                                            wow its only 9:30   \n",
       "2336  36589d3c2d  Homework. ew fml. Should I pull an all nighter? I think ...   \n",
       "328   1962095a32                             rofl that`s what we like to hear   \n",
       "1191  12005b65fc                    Waiting for my turn on wii fit gym closed   \n",
       "48    ab57d75f20    im really sorry i know wallah how u feel this life is ...   \n",
       "932   a7af432585  Transition time is over; tonight I work night shift. I w...   \n",
       "2957  7ae1499198  _august84 - requoting the gyaan - the more you live life...   \n",
       "402   c0b7788e74                                   my monday running so fast!   \n",
       "2150  32b2c8b924   Aww Stanley, have you still got him, they can live for ...   \n",
       "3251  e37a7b9128           Ouch, give me a heads up so I`ll know when to duck   \n",
       "717   09e0880c54                            not here for long hun, head hurts   \n",
       "1455  b78e0c4c19          Thank you Kirst - your posts on running inspired me   \n",
       "2236  e939fe583c                sometimes spam leaks in to the message board.   \n",
       "2650  3c048c6a14                                     hey Jess where is Josh??   \n",
       "3034  b9b532f7a2   U were great, as always. But, can`t we do an east Germa...   \n",
       "3314  213f65851d            Day is going well so far. Meeting until 4 though.   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "2798   neutral   _twitz lol...hiiii yourself. maybe lay off the patron a...  \n",
       "2260  negative                                                     jealous!  \n",
       "3508  negative                                                        bored  \n",
       "2854   neutral                          is going to school. leave me stuff.  \n",
       "1718   neutral   chris came over for exactly one hour i moped around aft...  \n",
       "85     neutral   lol man i got 2 1 /2 hrs an iont how i woulda made it w...  \n",
       "1571   neutral   i just laughed out loud on the bus reading your stateme...  \n",
       "1683  positive   oh my god! oh my god! oh my god! new supernatural start...  \n",
       "2437  positive                                                 a rare treat  \n",
       "1655  negative                                didn`t get to see the cowboys  \n",
       "166   positive                                                        happy  \n",
       "431   negative                                                      too hot  \n",
       "853   positive                                                   productive  \n",
       "2321   neutral   my new site is stuck in the google sandpit now test of ...  \n",
       "3234   neutral                                            wow its only 9:30  \n",
       "2336   neutral   homework. ew fml. should i pull an all nighter? i think...  \n",
       "328   positive                                                 like to hear  \n",
       "1191   neutral                    waiting for my turn on wii fit gym closed  \n",
       "48    negative                                              im really sorry  \n",
       "932    neutral   transition time is over; tonight i work night shift. i ...  \n",
       "2957   neutral   _august84 - requoting the gyaan - the more you live lif...  \n",
       "402    neutral                                   my monday running so fast!  \n",
       "2150   neutral   aww stanley, have you still got him, they can live for ...  \n",
       "3251   neutral           ouch, give me a heads up so i`ll know when to duck  \n",
       "717   negative                                                        hurts  \n",
       "1455  positive                                                    thank you  \n",
       "2236   neutral                sometimes spam leaks in to the message board.  \n",
       "2650   neutral                                     hey jess where is josh??  \n",
       "3034  positive                                                u were great,  \n",
       "3314  positive                                            day is going well  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
