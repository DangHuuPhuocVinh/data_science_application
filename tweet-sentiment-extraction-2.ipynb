{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preface**\n",
    "**This project was inspired and referenced in [TensorFlow roBERTa - [0.705]](https://www.kaggle.com/code/cdeotte/tensorflow-roberta-0-705)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1/ About team**\n",
    "|StuID  |        Name       |     Kaggle    |       Github      |\n",
    "|-------|-------------------|---------------|-------------------|\n",
    "|1752052|Dang Huu Phuoc Vinh|[V_Notebook](https://www.kaggle.com/danghuuphuocvinh)|[V_Github](https://github.com/DangHuuPhuocVinh/data_science_application)\n",
    "|1753097|Le Nguyen Minh Tam |[T_Notebook](https://www.kaggle.com/minhtamlenguyen)|[T_Github](https://github.com/lnmtam1999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2/ About competition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1/ Name of competition**\n",
    "**[Tweet Sentiment Extraction](https://www.kaggle.com/competitions/tweet-sentiment-extraction) organized by [Kaggle](https://www.kaggle.com/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2/ Prize**\n",
    "**15000 USD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.3/ Description**\n",
    "  **E.g: \"My ridiculous dog is amazing.\" [sentiment: positive]**\n",
    "\n",
    "  **With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.**\n",
    "\n",
    "  **In this competition we've extracted support phrases from [Figure Eight's Data for Everyone platform](https://appen.com/datasets-resource-center/). The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.4/ Input and Output**\n",
    "- **Input: textID, text and sentiment**\n",
    "- **Output: selected_text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.5/ Columns**\n",
    "-  **textID - unique ID for each piece of text**\n",
    "-  **text - the text of the tweet**\n",
    "-  **sentiment - the general sentiment of the tweet**\n",
    "-  **selected_text - [train only] the text that supports the tweet's sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.6/ Meaningful**\n",
    "- **After doing this project, we can have a dataset with the phrases that were selected for using at any other NLP project**\n",
    "- **Can use for detecting some keywords that have sentiment** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.7/ Evaluation**\n",
    " ![img](https://user-images.githubusercontent.com/35680794/174698744-57b2f116-fbe4-4fb6-9216-2e83e0494dca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3/ Model roBERTa for this project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4/ Developing the project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1/ Import Libraries, Data and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:37.249683Z",
     "iopub.status.busy": "2022-07-10T19:03:37.249400Z",
     "iopub.status.idle": "2022-07-10T19:03:37.256307Z",
     "shell.execute_reply": "2022-07-10T19:03:37.255411Z",
     "shell.execute_reply.started": "2022-07-10T19:03:37.249641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1.1/ Tokenizer**\n",
    "First of all, we use tokenizer to convert the word to array for the computer can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:40.525850Z",
     "iopub.status.busy": "2022-07-10T19:03:40.525552Z",
     "iopub.status.idle": "2022-07-10T19:03:40.902687Z",
     "shell.execute_reply": "2022-07-10T19:03:40.901896Z",
     "shell.execute_reply.started": "2022-07-10T19:03:40.525819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 91\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAX_LEN** = 96 then for each training row, RoBERTa receives 96 tokens. The reason to use **lowercase** and **add_prefix_space** because when spelling with RoBERTa :\" helllo\", \"hello\", \" Hello\", and \"Hello\" use the same \" hello\" token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2/ Training data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.1/ Adjust the input**\n",
    "In this stage we ready our data for the model, all the input will be change to numerical and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:44.266109Z",
     "iopub.status.busy": "2022-07-10T19:03:44.265840Z",
     "iopub.status.idle": "2022-07-10T19:03:53.105947Z",
     "shell.execute_reply": "2022-07-10T19:03:53.105081Z",
     "shell.execute_reply.started": "2022-07-10T19:03:44.266080Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[    0   939 12905 ...     1     1     1]\n",
      " [    0    98  3036 ...     1     1     1]\n",
      " [    0   127  3504 ...     1     1     1]\n",
      " ...\n",
      " [    0  1423   857 ...     1     1     1]\n",
      " [    0    53    24 ...     1     1     1]\n",
      " [    0    70    42 ...     1     1     1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [(0, 4), (4, 9), (9, 12), (12, 18), (18, 24), (24, 27), (27, 29), (29, 33), (33, 36), (36, 37), (37, 44), (44, 45), (45, 47), (47, 49), (49, 50), (50, 53), (53, 54), (54, 57), (57, 59)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3/ Test Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same as the Training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:04:17.944124Z",
     "iopub.status.busy": "2022-07-10T19:04:17.943856Z",
     "iopub.status.idle": "2022-07-10T19:04:18.241347Z",
     "shell.execute_reply": "2022-07-10T19:04:18.240640Z",
     "shell.execute_reply.started": "2022-07-10T19:04:17.944095Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the original text is [ i][ am][ having][ a][ great][ day]</s>[ positive]</s> with tokens 0,1,2,3,4,5,6,7,8,9,10 and the selected text is \"great day\", then the training has start index = 5 and end index = 6.\n",
    "\n",
    "If our model also predicts a = 5 and b = 6 and we try to select the text from [ i][ am][ having][ a][ great][ day], the indices 5 and 6 will not return \"great day\". Instead we must use tokens[4:6] to get great day. We subtract 1 because the is now removed. And we add 1 to b because python indexing for list[3:5] does not return 5 it only returns 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4/ Build roBERTa model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4.1/ Building the bones of model**\n",
    "Built the bones of a roBERTa model, using the model has already train by the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:05:21.352602Z",
     "iopub.status.busy": "2022-07-10T19:05:21.352259Z",
     "iopub.status.idle": "2022-07-10T19:05:21.364895Z",
     "shell.execute_reply": "2022-07-10T19:05:21.364038Z",
     "shell.execute_reply.started": "2022-07-10T19:05:21.352569Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5/ Create metric**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We code this for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:06:16.388950Z",
     "iopub.status.busy": "2022-07-10T19:06:16.388632Z",
     "iopub.status.idle": "2022-07-10T19:06:16.395186Z",
     "shell.execute_reply": "2022-07-10T19:06:16.394374Z",
     "shell.execute_reply.started": "2022-07-10T19:06:16.388923Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.6/ Train roBERTa model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:07:44.465971Z",
     "iopub.status.busy": "2022-07-10T19:07:44.465660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.1794 - activation_loss: 1.0771 - activation_1_loss: 1.1023\n",
      "Epoch 00001: val_loss improved from inf to 1.67156, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 277s 13ms/sample - loss: 2.1773 - activation_loss: 1.0763 - activation_1_loss: 1.1011 - val_loss: 1.6716 - val_activation_loss: 0.8621 - val_activation_1_loss: 0.8091\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.7968 - activation_loss: 0.8858 - activation_1_loss: 0.9110\n",
      "Epoch 00002: val_loss improved from 1.67156 to 1.65726, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 261s 12ms/sample - loss: 1.7964 - activation_loss: 0.8858 - activation_1_loss: 0.9106 - val_loss: 1.6573 - val_activation_loss: 0.8548 - val_activation_1_loss: 0.8023\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.5691 - activation_loss: 0.8025 - activation_1_loss: 0.7666\n",
      "Epoch 00003: val_loss improved from 1.65726 to 1.63837, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 262s 12ms/sample - loss: 1.5701 - activation_loss: 0.8027 - activation_1_loss: 0.7674 - val_loss: 1.6384 - val_activation_loss: 0.8453 - val_activation_1_loss: 0.7931\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 26s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.7013372448250452\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1220 - activation_loss: 1.0632 - activation_1_loss: 1.0588\n",
      "Epoch 00001: val_loss improved from inf to 1.66223, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 293s 13ms/sample - loss: 2.1220 - activation_loss: 1.0617 - activation_1_loss: 1.0581 - val_loss: 1.6622 - val_activation_loss: 0.8600 - val_activation_1_loss: 0.8017\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6402 - activation_loss: 0.8442 - activation_1_loss: 0.7960\n",
      "Epoch 00002: val_loss improved from 1.66223 to 1.62189, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 275s 13ms/sample - loss: 1.6402 - activation_loss: 0.8430 - activation_1_loss: 0.7950 - val_loss: 1.6219 - val_activation_loss: 0.8380 - val_activation_1_loss: 0.7833\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5021 - activation_loss: 0.7755 - activation_1_loss: 0.7266\n",
      "Epoch 00003: val_loss did not improve from 1.62189\n",
      "21985/21985 [==============================] - 274s 12ms/sample - loss: 1.5020 - activation_loss: 0.7744 - activation_1_loss: 0.7255 - val_loss: 1.6521 - val_activation_loss: 0.8451 - val_activation_1_loss: 0.8063\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 26s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.7069353055405864\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1020 - activation_loss: 1.0562 - activation_1_loss: 1.0459\n",
      "Epoch 00001: val_loss improved from inf to 1.72719, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 294s 13ms/sample - loss: 2.1019 - activation_loss: 1.0546 - activation_1_loss: 1.0444 - val_loss: 1.7272 - val_activation_loss: 0.8559 - val_activation_1_loss: 0.8715\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6068 - activation_loss: 0.8316 - activation_1_loss: 0.7752\n",
      "Epoch 00002: val_loss improved from 1.72719 to 1.62516, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 275s 13ms/sample - loss: 1.6067 - activation_loss: 0.8304 - activation_1_loss: 0.7750 - val_loss: 1.6252 - val_activation_loss: 0.8383 - val_activation_1_loss: 0.7872\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4541 - activation_loss: 0.7569 - activation_1_loss: 0.6972\n",
      "Epoch 00003: val_loss did not improve from 1.62516\n",
      "21985/21985 [==============================] - 274s 12ms/sample - loss: 1.4543 - activation_loss: 0.7583 - activation_1_loss: 0.7009 - val_loss: 1.6554 - val_activation_loss: 0.8454 - val_activation_1_loss: 0.8101\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 26s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 3 Jaccard = 0.7016940141906132\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1209 - activation_loss: 1.0693 - activation_1_loss: 1.0516\n",
      "Epoch 00001: val_loss improved from inf to 1.67304, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 293s 13ms/sample - loss: 2.1208 - activation_loss: 1.0678 - activation_1_loss: 1.0504 - val_loss: 1.6730 - val_activation_loss: 0.8363 - val_activation_1_loss: 0.8370\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6381 - activation_loss: 0.8468 - activation_1_loss: 0.7913\n",
      "Epoch 00002: val_loss improved from 1.67304 to 1.62297, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 275s 13ms/sample - loss: 1.6380 - activation_loss: 0.8455 - activation_1_loss: 0.7902 - val_loss: 1.6230 - val_activation_loss: 0.8248 - val_activation_1_loss: 0.7982\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5013 - activation_loss: 0.7755 - activation_1_loss: 0.7257\n",
      "Epoch 00003: val_loss improved from 1.62297 to 1.61895, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 275s 13ms/sample - loss: 1.5013 - activation_loss: 0.7762 - activation_1_loss: 0.7261 - val_loss: 1.6189 - val_activation_loss: 0.8251 - val_activation_1_loss: 0.7941\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 26s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 4 Jaccard = 0.707146815806641\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1158 - activation_loss: 1.0613 - activation_1_loss: 1.0545\n",
      "Epoch 00001: val_loss improved from inf to 1.62955, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 294s 13ms/sample - loss: 2.1159 - activation_loss: 1.0649 - activation_1_loss: 1.0546 - val_loss: 1.6295 - val_activation_loss: 0.8510 - val_activation_1_loss: 0.7778\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6322 - activation_loss: 0.8394 - activation_1_loss: 0.7928\n",
      "Epoch 00002: val_loss improved from 1.62955 to 1.59264, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 276s 13ms/sample - loss: 1.6322 - activation_loss: 0.8382 - activation_1_loss: 0.7917 - val_loss: 1.5926 - val_activation_loss: 0.8296 - val_activation_1_loss: 0.7624\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4877 - activation_loss: 0.7653 - activation_1_loss: 0.7224\n",
      "Epoch 00003: val_loss did not improve from 1.59264\n",
      "21985/21985 [==============================] - 274s 12ms/sample - loss: 1.4877 - activation_loss: 0.7648 - activation_1_loss: 0.7216 - val_loss: 1.6208 - val_activation_loss: 0.8587 - val_activation_1_loss: 0.7615\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 26s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 5 Jaccard = 0.709949073649741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.7/ Kaggle submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>3cd2b0d233</td>\n",
       "      <td>wishing, i was at the michou show right now.. god. life ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>wishing, i was at the michou show right now.. god. life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>103459e206</td>\n",
       "      <td>sad about young love coming to an end  They were sooo c...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sad about young love coming to an end they were sooo cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>d78be7b22e</td>\n",
       "      <td>Same here - events + writing songs. But I just finished...</td>\n",
       "      <td>positive</td>\n",
       "      <td>excited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>1e2e4371a9</td>\n",
       "      <td>i hate stupid boys! arrgh</td>\n",
       "      <td>negative</td>\n",
       "      <td>i hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>1962095a32</td>\n",
       "      <td>rofl that`s what we like to hear</td>\n",
       "      <td>positive</td>\n",
       "      <td>like to hear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3116</th>\n",
       "      <td>dda6145689</td>\n",
       "      <td>COTTIN WITH EMILYYYYYYYY</td>\n",
       "      <td>neutral</td>\n",
       "      <td>cottin with emilyyyyyyyy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>b2dc59805a</td>\n",
       "      <td>out to play</td>\n",
       "      <td>neutral</td>\n",
       "      <td>out to play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2649</th>\n",
       "      <td>ecc56a776a</td>\n",
       "      <td>most definitely will!  also, i wanted to say 'perfect t...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>most definitely will! also, i wanted to say 'perfect ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>ee35fcd036</td>\n",
       "      <td>this evening,picking up n`pop and go for dinner at Ta Lu...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>this evening,picking up n`pop and go for dinner at ta l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3493</th>\n",
       "      <td>c9980ac0cd</td>\n",
       "      <td>airsoft is so much fun! i play with my brothers and it`...</td>\n",
       "      <td>positive</td>\n",
       "      <td>fun!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>67d5b57919</td>\n",
       "      <td>Extremely happy to see my new track 'The Awakening' feat...</td>\n",
       "      <td>positive</td>\n",
       "      <td>extremely happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>1f9019bba5</td>\n",
       "      <td>lmao you witty wacko...loves it</td>\n",
       "      <td>positive</td>\n",
       "      <td>loves it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>18bcba79be</td>\n",
       "      <td>i`ve not had a reply on my topic yet  lolz welll its not...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i`ve not had a reply on my topic yet lolz welll its not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3118</th>\n",
       "      <td>7fb2ca19e9</td>\n",
       "      <td>I have popcorn and fruitcake now...but it`s just not th...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i have popcorn and fruitcake now...but it`s just not th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>c281b14817</td>\n",
       "      <td>last class at 10:30. One final tomorrow and 2 finals on ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>last class at 10:30. one final tomorrow and 2 finals on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>fd57cd4c4e</td>\n",
       "      <td>watching the rain and reminiscing about the time when ev...</td>\n",
       "      <td>positive</td>\n",
       "      <td>love of my life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>a16f652927</td>\n",
       "      <td>OMGosh! it`s 79 degrees and I`m sweating and miserable! ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>miserable!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>e0eeef2f3f</td>\n",
       "      <td>Playing with lighters fire matches and grass</td>\n",
       "      <td>neutral</td>\n",
       "      <td>playing with lighters fire matches and grass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>a37538b291</td>\n",
       "      <td>Will have to it`s only across the water and all. Just d...</td>\n",
       "      <td>negative</td>\n",
       "      <td>damned expensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>ad0dd05c20</td>\n",
       "      <td>ali, just like you do!!!   have such a wonderful sunday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>20bf68a459</td>\n",
       "      <td>fingers crossed for you.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>fingers crossed for you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>089f2af7b9</td>\n",
       "      <td>i wish i could go to meet in the middle tomorrow.</td>\n",
       "      <td>positive</td>\n",
       "      <td>wish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>67ff57361c</td>\n",
       "      <td>Working on Canada Shoots information.  Coming your way s...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>working on canada shoots information. coming your way s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>ca10e6b390</td>\n",
       "      <td>Danny cut his beautiful curls</td>\n",
       "      <td>negative</td>\n",
       "      <td>danny cut his beautiful curls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>3d0c74e523</td>\n",
       "      <td>#asylm yay registered but lost all my friends</td>\n",
       "      <td>negative</td>\n",
       "      <td>lost all my friends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>be16255b41</td>\n",
       "      <td>Wake Up..... Please.</td>\n",
       "      <td>negative</td>\n",
       "      <td>wake up..... please.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>112da4faa0</td>\n",
       "      <td>Gonna go and see my sister at Mount Saini ....  she`s th...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>gonna go and see my sister at mount saini .... she`s th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3133</th>\n",
       "      <td>586e68c59d</td>\n",
       "      <td>Marley and me is the saddest movie ever I never cry in m...</td>\n",
       "      <td>negative</td>\n",
       "      <td>saddest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>be346f0d98</td>\n",
       "      <td>Babysitting in the sun and heat. I`m getting lots of fre...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i`m getting lots of freckles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127</th>\n",
       "      <td>8f65a41e73</td>\n",
       "      <td>well keep me posted if it comes out fully thanks</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "1419  3cd2b0d233  wishing, i was at the michou show right now.. god. life ...   \n",
       "804   103459e206   sad about young love coming to an end  They were sooo c...   \n",
       "1120  d78be7b22e   Same here - events + writing songs. But I just finished...   \n",
       "1520  1e2e4371a9                                    i hate stupid boys! arrgh   \n",
       "328   1962095a32                             rofl that`s what we like to hear   \n",
       "3116  dda6145689                                     COTTIN WITH EMILYYYYYYYY   \n",
       "773   b2dc59805a                                                  out to play   \n",
       "2649  ecc56a776a   most definitely will!  also, i wanted to say 'perfect t...   \n",
       "1810  ee35fcd036  this evening,picking up n`pop and go for dinner at Ta Lu...   \n",
       "3493  c9980ac0cd   airsoft is so much fun! i play with my brothers and it`...   \n",
       "557   67d5b57919  Extremely happy to see my new track 'The Awakening' feat...   \n",
       "1472  1f9019bba5                              lmao you witty wacko...loves it   \n",
       "3331  18bcba79be  i`ve not had a reply on my topic yet  lolz welll its not...   \n",
       "3118  7fb2ca19e9   I have popcorn and fruitcake now...but it`s just not th...   \n",
       "1393  c281b14817  last class at 10:30. One final tomorrow and 2 finals on ...   \n",
       "1421  fd57cd4c4e  watching the rain and reminiscing about the time when ev...   \n",
       "2700  a16f652927  OMGosh! it`s 79 degrees and I`m sweating and miserable! ...   \n",
       "899   e0eeef2f3f                 Playing with lighters fire matches and grass   \n",
       "2289  a37538b291   Will have to it`s only across the water and all. Just d...   \n",
       "2708  ad0dd05c20     ali, just like you do!!!   have such a wonderful sunday!   \n",
       "2378  20bf68a459                                     fingers crossed for you.   \n",
       "2890  089f2af7b9            i wish i could go to meet in the middle tomorrow.   \n",
       "1504  67ff57361c  Working on Canada Shoots information.  Coming your way s...   \n",
       "1132  ca10e6b390                                Danny cut his beautiful curls   \n",
       "1204  3d0c74e523                #asylm yay registered but lost all my friends   \n",
       "1552  be16255b41                                         Wake Up..... Please.   \n",
       "1457  112da4faa0  Gonna go and see my sister at Mount Saini ....  she`s th...   \n",
       "3133  586e68c59d  Marley and me is the saddest movie ever I never cry in m...   \n",
       "2636  be346f0d98  Babysitting in the sun and heat. I`m getting lots of fre...   \n",
       "3127  8f65a41e73             well keep me posted if it comes out fully thanks   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "1419   neutral   wishing, i was at the michou show right now.. god. life...  \n",
       "804    neutral   sad about young love coming to an end they were sooo cu...  \n",
       "1120  positive                                                      excited  \n",
       "1520  negative                                                       i hate  \n",
       "328   positive                                                 like to hear  \n",
       "3116   neutral                                     cottin with emilyyyyyyyy  \n",
       "773    neutral                                                  out to play  \n",
       "2649   neutral   most definitely will! also, i wanted to say 'perfect ti...  \n",
       "1810   neutral   this evening,picking up n`pop and go for dinner at ta l...  \n",
       "3493  positive                                                         fun!  \n",
       "557   positive                                              extremely happy  \n",
       "1472  positive                                                     loves it  \n",
       "3331   neutral   i`ve not had a reply on my topic yet lolz welll its not...  \n",
       "3118   neutral   i have popcorn and fruitcake now...but it`s just not th...  \n",
       "1393   neutral   last class at 10:30. one final tomorrow and 2 finals on...  \n",
       "1421  positive                                             love of my life.  \n",
       "2700  negative                                                   miserable!  \n",
       "899    neutral                 playing with lighters fire matches and grass  \n",
       "2289  negative                                             damned expensive  \n",
       "2708  positive                                                    wonderful  \n",
       "2378   neutral                                     fingers crossed for you.  \n",
       "2890  positive                                                         wish  \n",
       "1504   neutral   working on canada shoots information. coming your way s...  \n",
       "1132  negative                                danny cut his beautiful curls  \n",
       "1204  negative                                          lost all my friends  \n",
       "1552  negative                                         wake up..... please.  \n",
       "1457   neutral   gonna go and see my sister at mount saini .... she`s th...  \n",
       "3133  negative                                                      saddest  \n",
       "2636  negative                                 i`m getting lots of freckles  \n",
       "3127  positive                                                       thanks  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
