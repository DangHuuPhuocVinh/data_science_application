{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preface**\n",
    "**This project was inspired and referenced in [TensorFlow roBERTa - [0.705]](https://www.kaggle.com/code/cdeotte/tensorflow-roberta-0-705)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1/ About team**\n",
    "|StuID  |        Name       |     Kaggle    |       Github      |\n",
    "|-------|-------------------|---------------|-------------------|\n",
    "|1752052|Dang Huu Phuoc Vinh|[V_Notebook](https://www.kaggle.com/danghuuphuocvinh)|[V_Github](https://github.com/DangHuuPhuocVinh/data_science_application)\n",
    "|1753097|Le Nguyen Minh Tam |[T_Notebook](https://www.kaggle.com/minhtamlenguyen)|[T_Github](https://github.com/lnmtam1999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2/ About competition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1/ Name of competition**\n",
    "**[Tweet Sentiment Extraction](https://www.kaggle.com/competitions/tweet-sentiment-extraction) organized by [Kaggle](https://www.kaggle.com/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2/ Prize**\n",
    "**15000 USD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.3/ Description**\n",
    "  **E.g: \"My ridiculous dog is amazing.\" [sentiment: positive]**\n",
    "\n",
    "  **With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.**\n",
    "\n",
    "  **In this competition we've extracted support phrases from [Figure Eight's Data for Everyone platform](https://appen.com/datasets-resource-center/). The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.4/ Input and Output**\n",
    "- **Input: textID, text and sentiment**\n",
    "- **Output: selected_text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.5/ Columns**\n",
    "-  **textID - unique ID for each piece of text**\n",
    "-  **text - the text of the tweet**\n",
    "-  **sentiment - the general sentiment of the tweet**\n",
    "-  **selected_text - [train only] the text that supports the tweet's sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.6/ Meaningful**\n",
    "- **After doing this project, we can have a dataset with the phrases that were selected for using at any other NLP project**\n",
    "- **Can use for detecting some keywords that have sentiment** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.7/ Evaluation**\n",
    " ![img](https://user-images.githubusercontent.com/35680794/174698744-57b2f116-fbe4-4fb6-9216-2e83e0494dca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3/ Model roBERTa for this project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4/ Developing the project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1/ Import Libraries, Data and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:37.249683Z",
     "iopub.status.busy": "2022-07-10T19:03:37.249400Z",
     "iopub.status.idle": "2022-07-10T19:03:37.256307Z",
     "shell.execute_reply": "2022-07-10T19:03:37.255411Z",
     "shell.execute_reply.started": "2022-07-10T19:03:37.249641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1.1/ Tokenizer**\n",
    "First of all, we use tokenizer to convert the word to array for the computer can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:40.525850Z",
     "iopub.status.busy": "2022-07-10T19:03:40.525552Z",
     "iopub.status.idle": "2022-07-10T19:03:40.902687Z",
     "shell.execute_reply": "2022-07-10T19:03:40.901896Z",
     "shell.execute_reply.started": "2022-07-10T19:03:40.525819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 100\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAX_LEN** = 96 then for each training row, RoBERTa receives 96 tokens. The reason to use **lowercase** and **add_prefix_space** because when spelling with RoBERTa :\" helllo\", \"hello\", \" Hello\", and \"Hello\" use the same \" hello\" token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2/ Training data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.1/ Adjust the input**\n",
    "In this stage we ready our data for the model, all the input will be change to numerical and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:44.266109Z",
     "iopub.status.busy": "2022-07-10T19:03:44.265840Z",
     "iopub.status.idle": "2022-07-10T19:03:53.105947Z",
     "shell.execute_reply": "2022-07-10T19:03:53.105081Z",
     "shell.execute_reply.started": "2022-07-10T19:03:44.266080Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[    0   939 12905 ...     1     1     1]\n",
      " [    0    98  3036 ...     1     1     1]\n",
      " [    0   127  3504 ...     1     1     1]\n",
      " ...\n",
      " [    0  1423   857 ...     1     1     1]\n",
      " [    0    53    24 ...     1     1     1]\n",
      " [    0    70    42 ...     1     1     1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [(0, 4), (4, 9), (9, 12), (12, 18), (18, 24), (24, 27), (27, 29), (29, 33), (33, 36), (36, 37), (37, 44), (44, 45), (45, 47), (47, 49), (49, 50), (50, 53), (53, 54), (54, 57), (57, 59)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3/ Test Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same as the Training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:04:17.944124Z",
     "iopub.status.busy": "2022-07-10T19:04:17.943856Z",
     "iopub.status.idle": "2022-07-10T19:04:18.241347Z",
     "shell.execute_reply": "2022-07-10T19:04:18.240640Z",
     "shell.execute_reply.started": "2022-07-10T19:04:17.944095Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the original text is [ i][ am][ having][ a][ great][ day]</s>[ positive]</s> with tokens 0,1,2,3,4,5,6,7,8,9,10 and the selected text is \"great day\", then the training has start index = 5 and end index = 6.\n",
    "\n",
    "If our model also predicts a = 5 and b = 6 and we try to select the text from [ i][ am][ having][ a][ great][ day], the indices 5 and 6 will not return \"great day\". Instead we must use tokens[4:6] to get great day. We subtract 1 because the is now removed. And we add 1 to b because python indexing for list[3:5] does not return 5 it only returns 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4/ Build roBERTa model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4.1/ Building the bones of model**\n",
    "Built the bones of a roBERTa model, using the model has already train by the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:05:21.352602Z",
     "iopub.status.busy": "2022-07-10T19:05:21.352259Z",
     "iopub.status.idle": "2022-07-10T19:05:21.364895Z",
     "shell.execute_reply": "2022-07-10T19:05:21.364038Z",
     "shell.execute_reply.started": "2022-07-10T19:05:21.352569Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5/ Create metric**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We code this for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:06:16.388950Z",
     "iopub.status.busy": "2022-07-10T19:06:16.388632Z",
     "iopub.status.idle": "2022-07-10T19:06:16.395186Z",
     "shell.execute_reply": "2022-07-10T19:06:16.394374Z",
     "shell.execute_reply.started": "2022-07-10T19:06:16.388923Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.6/ Train roBERTa model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:07:44.465971Z",
     "iopub.status.busy": "2022-07-10T19:07:44.465660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.2274 - activation_loss: 1.1135 - activation_1_loss: 1.1139\n",
      "Epoch 00001: val_loss improved from inf to 1.68444, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 295s 13ms/sample - loss: 2.2269 - activation_loss: 1.1133 - activation_1_loss: 1.1136 - val_loss: 1.6844 - val_activation_loss: 0.8746 - val_activation_1_loss: 0.8096\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.6288 - activation_loss: 0.8414 - activation_1_loss: 0.7874\n",
      "Epoch 00002: val_loss improved from 1.68444 to 1.67145, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 279s 13ms/sample - loss: 1.6285 - activation_loss: 0.8414 - activation_1_loss: 0.7872 - val_loss: 1.6714 - val_activation_loss: 0.8710 - val_activation_1_loss: 0.8004\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.4681 - activation_loss: 0.7569 - activation_1_loss: 0.7111\n",
      "Epoch 00003: val_loss improved from 1.67145 to 1.65792, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 278s 13ms/sample - loss: 1.4677 - activation_loss: 0.7568 - activation_1_loss: 0.7108 - val_loss: 1.6579 - val_activation_loss: 0.8626 - val_activation_1_loss: 0.7950\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.6988623188090805\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1812 - activation_loss: 1.0794 - activation_1_loss: 1.1018\n",
      "Epoch 00001: val_loss improved from inf to 1.76451, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 312s 14ms/sample - loss: 2.1811 - activation_loss: 1.0778 - activation_1_loss: 1.1002 - val_loss: 1.7645 - val_activation_loss: 0.8730 - val_activation_1_loss: 0.8909\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.8720 - activation_loss: 0.9031 - activation_1_loss: 0.9689\n",
      "Epoch 00002: val_loss improved from 1.76451 to 1.71125, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 292s 13ms/sample - loss: 1.8720 - activation_loss: 0.9029 - activation_1_loss: 0.9681 - val_loss: 1.7112 - val_activation_loss: 0.8930 - val_activation_1_loss: 0.8176\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5833 - activation_loss: 0.8131 - activation_1_loss: 0.7702\n",
      "Epoch 00003: val_loss improved from 1.71125 to 1.67449, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 292s 13ms/sample - loss: 1.5833 - activation_loss: 0.8129 - activation_1_loss: 0.7698 - val_loss: 1.6745 - val_activation_loss: 0.8482 - val_activation_1_loss: 0.8257\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.6909924233516171\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2245 - activation_loss: 1.0922 - activation_1_loss: 1.1323\n",
      "Epoch 00001: val_loss improved from inf to 1.74202, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 312s 14ms/sample - loss: 2.2248 - activation_loss: 1.0975 - activation_1_loss: 1.1369 - val_loss: 1.7420 - val_activation_loss: 0.8579 - val_activation_1_loss: 0.8842\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.9631 - activation_loss: 0.9286 - activation_1_loss: 1.0345\n",
      "Epoch 00002: val_loss improved from 1.74202 to 1.67592, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 292s 13ms/sample - loss: 1.9630 - activation_loss: 0.9274 - activation_1_loss: 1.0336 - val_loss: 1.6759 - val_activation_loss: 0.8464 - val_activation_1_loss: 0.8298\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5767 - activation_loss: 0.8059 - activation_1_loss: 0.7708\n",
      "Epoch 00003: val_loss did not improve from 1.67592\n",
      "21985/21985 [==============================] - 290s 13ms/sample - loss: 1.5768 - activation_loss: 0.8056 - activation_1_loss: 0.7757 - val_loss: 1.6936 - val_activation_loss: 0.8573 - val_activation_1_loss: 0.8364\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 3 Jaccard = 0.6952809915700305\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1243 - activation_loss: 1.0676 - activation_1_loss: 1.0567\n",
      "Epoch 00001: val_loss improved from inf to 1.67097, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 311s 14ms/sample - loss: 2.1243 - activation_loss: 1.0676 - activation_1_loss: 1.0568 - val_loss: 1.6710 - val_activation_loss: 0.8576 - val_activation_1_loss: 0.8135\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6329 - activation_loss: 0.8403 - activation_1_loss: 0.7926\n",
      "Epoch 00002: val_loss improved from 1.67097 to 1.63102, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 291s 13ms/sample - loss: 1.6330 - activation_loss: 0.8446 - activation_1_loss: 0.7924 - val_loss: 1.6310 - val_activation_loss: 0.8124 - val_activation_1_loss: 0.8186\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5028 - activation_loss: 0.7773 - activation_1_loss: 0.7255\n",
      "Epoch 00003: val_loss improved from 1.63102 to 1.61738, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 291s 13ms/sample - loss: 1.5028 - activation_loss: 0.7762 - activation_1_loss: 0.7245 - val_loss: 1.6174 - val_activation_loss: 0.8208 - val_activation_1_loss: 0.7967\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 4 Jaccard = 0.6946653698758363\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2018 - activation_loss: 1.0645 - activation_1_loss: 1.1373\n",
      "Epoch 00001: val_loss improved from inf to 1.63836, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 312s 14ms/sample - loss: 2.2019 - activation_loss: 1.0689 - activation_1_loss: 1.1374 - val_loss: 1.6384 - val_activation_loss: 0.8480 - val_activation_1_loss: 0.7898\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6642 - activation_loss: 0.8526 - activation_1_loss: 0.8117\n",
      "Epoch 00002: val_loss improved from 1.63836 to 1.60778, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 292s 13ms/sample - loss: 1.6642 - activation_loss: 0.8513 - activation_1_loss: 0.8105 - val_loss: 1.6078 - val_activation_loss: 0.8387 - val_activation_1_loss: 0.7685\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5149 - activation_loss: 0.7820 - activation_1_loss: 0.7329\n",
      "Epoch 00003: val_loss improved from 1.60778 to 1.58468, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 292s 13ms/sample - loss: 1.5148 - activation_loss: 0.7808 - activation_1_loss: 0.7319 - val_loss: 1.5847 - val_activation_loss: 0.8289 - val_activation_1_loss: 0.7549\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 5 Jaccard = 0.7119270777106699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.7/ Kaggle submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3268</th>\n",
       "      <td>0a6580a2ca</td>\n",
       "      <td>and its amazing  x</td>\n",
       "      <td>positive</td>\n",
       "      <td>amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>1b6fe730fd</td>\n",
       "      <td>maybe going to see the hannah montana movie todaaaay  i`...</td>\n",
       "      <td>positive</td>\n",
       "      <td>can`t wait to see it again!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>16ee71e6df</td>\n",
       "      <td>Sadly I think I know exactly were you put it--in the ex...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sadly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>9a6736745f</td>\n",
       "      <td>yayy  you`ll can help me im doing my english homeworks ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>yayy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>da62e66fac</td>\n",
       "      <td>omfg. one of the worst days ever!</td>\n",
       "      <td>negative</td>\n",
       "      <td>omfg. one of the worst days ever!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>6c60cf0bb2</td>\n",
       "      <td>_89 I feel so sorry for her  hope she dnt cry 2nd time</td>\n",
       "      <td>negative</td>\n",
       "      <td>i feel so sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>c4393e4cba</td>\n",
       "      <td>HAPPY MOTHERS DAY!  Tell ur mom that`s she an awesome m...</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy mothers day!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>23efa13d7c</td>\n",
       "      <td>happy mommah`s day to your moms  http://plurk.com/p/stqya</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>d9e1b10a4f</td>\n",
       "      <td>wow.. tomorrow and then it`s over. i`ll never see some o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>it`s kind of sad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365</th>\n",
       "      <td>65cbb266bf</td>\n",
       "      <td>my baby shut me down</td>\n",
       "      <td>negative</td>\n",
       "      <td>my baby shut me down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>8e48929733</td>\n",
       "      <td>ipod died today</td>\n",
       "      <td>negative</td>\n",
       "      <td>died</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3181</th>\n",
       "      <td>ea032bbaaf</td>\n",
       "      <td>is thinking about a career change</td>\n",
       "      <td>neutral</td>\n",
       "      <td>is thinking about a career change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>038c049e2a</td>\n",
       "      <td>what`s wrong?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>what`s wrong?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>3dfd82c60f</td>\n",
       "      <td>I`m dreading hearing even worse news tonight</td>\n",
       "      <td>negative</td>\n",
       "      <td>i`m dreading</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2629</th>\n",
       "      <td>1eabe43846</td>\n",
       "      <td>I am in Kuala Lumpur. And I know I vanished, haha! I br...</td>\n",
       "      <td>negative</td>\n",
       "      <td>vanished,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452</th>\n",
       "      <td>9f48b1d3fa</td>\n",
       "      <td>I can`t get in, I`m in a waiting room</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i can`t get in, i`m in a waiting room</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>1115490075</td>\n",
       "      <td>_Day26 awww man, its nott</td>\n",
       "      <td>neutral</td>\n",
       "      <td>_day26 awww man, its nott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>7db53ed89f</td>\n",
       "      <td>Home from Chelsea`s  Jam sessions = &lt;3 = Chelsea + Abby ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i luh you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>4b8092674a</td>\n",
       "      <td>Ahh... I love Chinese music. Haha.  Not gonna see my luf...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>278c0dceeb</td>\n",
       "      <td>i hate it when my bff is groundedd  boooooooooo</td>\n",
       "      <td>negative</td>\n",
       "      <td>i hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2798</th>\n",
       "      <td>f68a3ece4b</td>\n",
       "      <td>_twitz lol...hiiii yourself. Maybe lay off the patron an...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>_twitz lol...hiiii yourself. maybe lay off the patron a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>a7506a67a4</td>\n",
       "      <td>_witty  yes france/belgium was good  carnt belive how mu...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>_witty yes france/belgium was good carnt belive how muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635</th>\n",
       "      <td>38b6b897f4</td>\n",
       "      <td>HMV Shinjuku</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hmv shinjuku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>8b6c53f38b</td>\n",
       "      <td>I love the guy that was standing behind me</td>\n",
       "      <td>positive</td>\n",
       "      <td>i love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>c625cee17b</td>\n",
       "      <td>you might also want to include 'never wear a moonwolf' ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>you might also want to include 'never wear a moonwolf' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2177</th>\n",
       "      <td>c3541b6a2f</td>\n",
       "      <td>that sucks</td>\n",
       "      <td>negative</td>\n",
       "      <td>that sucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3020</th>\n",
       "      <td>0197e3ebbc</td>\n",
       "      <td>i need to make more of an effort to meet some of my best...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i need to make more of an effort to meet some of my bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>37227c0dbf</td>\n",
       "      <td>Oh, **** me. I`ve just returned from the Supermarket Of ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>oh, **** me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>4d31092a19</td>\n",
       "      <td>Ive totally not got that job  i can just tell.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ive totally not got that job i can just tell.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>3285374ab0</td>\n",
       "      <td>im hangin out with my cousin holly and tlkin 2 my grandp...</td>\n",
       "      <td>negative</td>\n",
       "      <td>my phone went dead</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "3268  0a6580a2ca                                           and its amazing  x   \n",
       "3462  1b6fe730fd  maybe going to see the hannah montana movie todaaaay  i`...   \n",
       "188   16ee71e6df   Sadly I think I know exactly were you put it--in the ex...   \n",
       "529   9a6736745f   yayy  you`ll can help me im doing my english homeworks ...   \n",
       "2353  da62e66fac                            omfg. one of the worst days ever!   \n",
       "266   6c60cf0bb2       _89 I feel so sorry for her  hope she dnt cry 2nd time   \n",
       "1839  c4393e4cba   HAPPY MOTHERS DAY!  Tell ur mom that`s she an awesome m...   \n",
       "404   23efa13d7c    happy mommah`s day to your moms  http://plurk.com/p/stqya   \n",
       "635   d9e1b10a4f  wow.. tomorrow and then it`s over. i`ll never see some o...   \n",
       "3365  65cbb266bf                                         my baby shut me down   \n",
       "2328  8e48929733                                              ipod died today   \n",
       "3181  ea032bbaaf                            is thinking about a career change   \n",
       "766   038c049e2a                                                what`s wrong?   \n",
       "390   3dfd82c60f                 I`m dreading hearing even worse news tonight   \n",
       "2629  1eabe43846   I am in Kuala Lumpur. And I know I vanished, haha! I br...   \n",
       "3452  9f48b1d3fa                        I can`t get in, I`m in a waiting room   \n",
       "2519  1115490075                                    _Day26 awww man, its nott   \n",
       "1079  7db53ed89f  Home from Chelsea`s  Jam sessions = <3 = Chelsea + Abby ...   \n",
       "1234  4b8092674a  Ahh... I love Chinese music. Haha.  Not gonna see my luf...   \n",
       "1620  278c0dceeb              i hate it when my bff is groundedd  boooooooooo   \n",
       "2798  f68a3ece4b  _twitz lol...hiiii yourself. Maybe lay off the patron an...   \n",
       "586   a7506a67a4  _witty  yes france/belgium was good  carnt belive how mu...   \n",
       "2635  38b6b897f4                                                 HMV Shinjuku   \n",
       "296   8b6c53f38b                   I love the guy that was standing behind me   \n",
       "2051  c625cee17b   you might also want to include 'never wear a moonwolf' ...   \n",
       "2177  c3541b6a2f                                                   that sucks   \n",
       "3020  0197e3ebbc  i need to make more of an effort to meet some of my best...   \n",
       "149   37227c0dbf  Oh, **** me. I`ve just returned from the Supermarket Of ...   \n",
       "1274  4d31092a19               Ive totally not got that job  i can just tell.   \n",
       "1183  3285374ab0  im hangin out with my cousin holly and tlkin 2 my grandp...   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "3268  positive                                                      amazing  \n",
       "3462  positive                                 can`t wait to see it again!!  \n",
       "188   negative                                                        sadly  \n",
       "529   positive                                                         yayy  \n",
       "2353  negative                            omfg. one of the worst days ever!  \n",
       "266   negative                                              i feel so sorry  \n",
       "1839  positive                                           happy mothers day!  \n",
       "404   positive                                                        happy  \n",
       "635   negative                                            it`s kind of sad.  \n",
       "3365  negative                                         my baby shut me down  \n",
       "2328  negative                                                         died  \n",
       "3181   neutral                            is thinking about a career change  \n",
       "766    neutral                                                what`s wrong?  \n",
       "390   negative                                                 i`m dreading  \n",
       "2629  negative                                                    vanished,  \n",
       "3452   neutral                        i can`t get in, i`m in a waiting room  \n",
       "2519   neutral                                    _day26 awww man, its nott  \n",
       "1079  positive                                                   i luh you.  \n",
       "1234  positive                                                       i love  \n",
       "1620  negative                                                       i hate  \n",
       "2798   neutral   _twitz lol...hiiii yourself. maybe lay off the patron a...  \n",
       "586    neutral   _witty yes france/belgium was good carnt belive how muc...  \n",
       "2635   neutral                                                 hmv shinjuku  \n",
       "296   positive                                                       i love  \n",
       "2051   neutral   you might also want to include 'never wear a moonwolf' ...  \n",
       "2177  negative                                                   that sucks  \n",
       "3020   neutral   i need to make more of an effort to meet some of my bes...  \n",
       "149   negative                                                 oh, **** me.  \n",
       "1274   neutral                ive totally not got that job i can just tell.  \n",
       "1183  negative                                           my phone went dead  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
