{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preface**\n",
    "**This project was inspired and referenced in [TensorFlow roBERTa - [0.705]](https://www.kaggle.com/code/cdeotte/tensorflow-roberta-0-705)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1/ About team**\n",
    "|StuID  |        Name       |     Kaggle    |       Github      |\n",
    "|-------|-------------------|---------------|-------------------|\n",
    "|1752052|Dang Huu Phuoc Vinh|[V_Notebook](https://www.kaggle.com/danghuuphuocvinh)|[V_Github](https://github.com/DangHuuPhuocVinh/data_science_application)\n",
    "|1753097|Le Nguyen Minh Tam |[T_Notebook](https://www.kaggle.com/minhtamlenguyen)|[T_Github](https://github.com/lnmtam1999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2/ About competition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1/ Name of competition**\n",
    "**[Tweet Sentiment Extraction](https://www.kaggle.com/competitions/tweet-sentiment-extraction) organized by [Kaggle](https://www.kaggle.com/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2/ Prize**\n",
    "**15000 USD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.3/ Description**\n",
    "  **E.g: \"My ridiculous dog is amazing.\" [sentiment: positive]**\n",
    "\n",
    "  **With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.**\n",
    "\n",
    "  **In this competition we've extracted support phrases from [Figure Eight's Data for Everyone platform](https://appen.com/datasets-resource-center/). The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.4/ Input and Output**\n",
    "- **Input: textID, text and sentiment**\n",
    "- **Output: selected_text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.5/ Columns**\n",
    "-  **textID - unique ID for each piece of text**\n",
    "-  **text - the text of the tweet**\n",
    "-  **sentiment - the general sentiment of the tweet**\n",
    "-  **selected_text - [train only] the text that supports the tweet's sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.6/ Meaningful**\n",
    "- **After doing this project, we can have a dataset with the phrases that were selected for using at any other NLP project**\n",
    "- **Can use for detecting some keywords that have sentiment** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.7/ Evaluation**\n",
    " ![img](https://user-images.githubusercontent.com/35680794/174698744-57b2f116-fbe4-4fb6-9216-2e83e0494dca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3/ Model roBERTa for this project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4/ Developing the project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1/ Import Libraries, Data and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:37.249683Z",
     "iopub.status.busy": "2022-07-10T19:03:37.249400Z",
     "iopub.status.idle": "2022-07-10T19:03:37.256307Z",
     "shell.execute_reply": "2022-07-10T19:03:37.255411Z",
     "shell.execute_reply.started": "2022-07-10T19:03:37.249641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1.1/ Tokenizer**\n",
    "First of all, we use tokenizer to convert the word to array for the computer can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:40.525850Z",
     "iopub.status.busy": "2022-07-10T19:03:40.525552Z",
     "iopub.status.idle": "2022-07-10T19:03:40.902687Z",
     "shell.execute_reply": "2022-07-10T19:03:40.901896Z",
     "shell.execute_reply.started": "2022-07-10T19:03:40.525819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 99\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAX_LEN** = 96 then for each training row, RoBERTa receives 96 tokens. The reason to use **lowercase** and **add_prefix_space** because when spelling with RoBERTa :\" helllo\", \"hello\", \" Hello\", and \"Hello\" use the same \" hello\" token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2/ Training data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.1/ Adjust the input**\n",
    "In this stage we ready our data for the model, all the input will be change to numerical and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:03:44.266109Z",
     "iopub.status.busy": "2022-07-10T19:03:44.265840Z",
     "iopub.status.idle": "2022-07-10T19:03:53.105947Z",
     "shell.execute_reply": "2022-07-10T19:03:53.105081Z",
     "shell.execute_reply.started": "2022-07-10T19:03:44.266080Z"
    }
   },
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[    0   939 12905 ...     1     1     1]\n",
      " [    0    98  3036 ...     1     1     1]\n",
      " [    0   127  3504 ...     1     1     1]\n",
      " ...\n",
      " [    0  1423   857 ...     1     1     1]\n",
      " [    0    53    24 ...     1     1     1]\n",
      " [    0    70    42 ...     1     1     1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [(0, 4), (4, 9), (9, 12), (12, 18), (18, 24), (24, 27), (27, 29), (29, 33), (33, 36), (36, 37), (37, 44), (44, 45), (45, 47), (47, 49), (49, 50), (50, 53), (53, 54), (54, 57), (57, 59)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\",offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3/ Test Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same as the Training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:04:17.944124Z",
     "iopub.status.busy": "2022-07-10T19:04:17.943856Z",
     "iopub.status.idle": "2022-07-10T19:04:18.241347Z",
     "shell.execute_reply": "2022-07-10T19:04:18.240640Z",
     "shell.execute_reply.started": "2022-07-10T19:04:17.944095Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the original text is [ i][ am][ having][ a][ great][ day]</s>[ positive]</s> with tokens 0,1,2,3,4,5,6,7,8,9,10 and the selected text is \"great day\", then the training has start index = 5 and end index = 6.\n",
    "\n",
    "If our model also predicts a = 5 and b = 6 and we try to select the text from [ i][ am][ having][ a][ great][ day], the indices 5 and 6 will not return \"great day\". Instead we must use tokens[4:6] to get great day. We subtract 1 because the is now removed. And we add 1 to b because python indexing for list[3:5] does not return 5 it only returns 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4/ Build roBERTa model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.4.1/ Building the bones of model**\n",
    "Built the bones of a roBERTa model, using the model has already train by the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:05:21.352602Z",
     "iopub.status.busy": "2022-07-10T19:05:21.352259Z",
     "iopub.status.idle": "2022-07-10T19:05:21.364895Z",
     "shell.execute_reply": "2022-07-10T19:05:21.364038Z",
     "shell.execute_reply.started": "2022-07-10T19:05:21.352569Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5/ Create metric**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We code this for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:06:16.388950Z",
     "iopub.status.busy": "2022-07-10T19:06:16.388632Z",
     "iopub.status.idle": "2022-07-10T19:06:16.395186Z",
     "shell.execute_reply": "2022-07-10T19:06:16.394374Z",
     "shell.execute_reply.started": "2022-07-10T19:06:16.388923Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.6/ Train roBERTa model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-10T19:07:44.465971Z",
     "iopub.status.busy": "2022-07-10T19:07:44.465660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.2238 - activation_loss: 1.0810 - activation_1_loss: 1.1428\n",
      "Epoch 00001: val_loss improved from inf to 1.69580, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 292s 13ms/sample - loss: 2.2225 - activation_loss: 1.0805 - activation_1_loss: 1.1419 - val_loss: 1.6958 - val_activation_loss: 0.8802 - val_activation_1_loss: 0.8156\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.6235 - activation_loss: 0.8333 - activation_1_loss: 0.7902\n",
      "Epoch 00002: val_loss improved from 1.69580 to 1.67610, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 278s 13ms/sample - loss: 1.6230 - activation_loss: 0.8331 - activation_1_loss: 0.7899 - val_loss: 1.6761 - val_activation_loss: 0.8472 - val_activation_1_loss: 0.8288\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.4796 - activation_loss: 0.7658 - activation_1_loss: 0.7139\n",
      "Epoch 00003: val_loss did not improve from 1.67610\n",
      "21984/21984 [==============================] - 276s 13ms/sample - loss: 1.4796 - activation_loss: 0.7657 - activation_1_loss: 0.7138 - val_loss: 1.7074 - val_activation_loss: 0.8559 - val_activation_1_loss: 0.8511\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.7101609347271807\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1377 - activation_loss: 1.0497 - activation_1_loss: 1.0880\n",
      "Epoch 00001: val_loss improved from inf to 1.73666, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 309s 14ms/sample - loss: 2.1377 - activation_loss: 1.0495 - activation_1_loss: 1.0868 - val_loss: 1.7367 - val_activation_loss: 0.8882 - val_activation_1_loss: 0.8477\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6230 - activation_loss: 0.8386 - activation_1_loss: 0.7843\n",
      "Epoch 00002: val_loss improved from 1.73666 to 1.60490, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 291s 13ms/sample - loss: 1.6229 - activation_loss: 0.8376 - activation_1_loss: 0.7842 - val_loss: 1.6049 - val_activation_loss: 0.8345 - val_activation_1_loss: 0.7699\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4680 - activation_loss: 0.7616 - activation_1_loss: 0.7064\n",
      "Epoch 00003: val_loss did not improve from 1.60490\n",
      "21985/21985 [==============================] - 289s 13ms/sample - loss: 1.4680 - activation_loss: 0.7627 - activation_1_loss: 0.7062 - val_loss: 1.6481 - val_activation_loss: 0.8600 - val_activation_1_loss: 0.7875\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 15s 4ms/sample\n",
      ">>>> FOLD 2 Jaccard = 0.7196624814564871\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2632 - activation_loss: 1.0977 - activation_1_loss: 1.1655\n",
      "Epoch 00001: val_loss improved from inf to 1.72280, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 309s 14ms/sample - loss: 2.2631 - activation_loss: 1.0962 - activation_1_loss: 1.1639 - val_loss: 1.7228 - val_activation_loss: 0.8559 - val_activation_1_loss: 0.8670\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.7100 - activation_loss: 0.8615 - activation_1_loss: 0.8484\n",
      "Epoch 00002: val_loss improved from 1.72280 to 1.69031, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 291s 13ms/sample - loss: 1.7099 - activation_loss: 0.8611 - activation_1_loss: 0.8478 - val_loss: 1.6903 - val_activation_loss: 0.8644 - val_activation_1_loss: 0.8263\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5467 - activation_loss: 0.7874 - activation_1_loss: 0.7593\n",
      "Epoch 00003: val_loss improved from 1.69031 to 1.64916, saving model to v0-roberta-2.h5\n",
      "21985/21985 [==============================] - 291s 13ms/sample - loss: 1.5469 - activation_loss: 0.7882 - activation_1_loss: 0.7631 - val_loss: 1.6492 - val_activation_loss: 0.8443 - val_activation_1_loss: 0.8052\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 3 Jaccard = 0.6971014294554871\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.1691 - activation_loss: 1.0592 - activation_1_loss: 1.1099\n",
      "Epoch 00001: val_loss improved from inf to 1.69934, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 310s 14ms/sample - loss: 2.1691 - activation_loss: 1.0596 - activation_1_loss: 1.1103 - val_loss: 1.6993 - val_activation_loss: 0.8617 - val_activation_1_loss: 0.8379\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6985 - activation_loss: 0.8675 - activation_1_loss: 0.8310\n",
      "Epoch 00002: val_loss improved from 1.69934 to 1.66578, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 291s 13ms/sample - loss: 1.6985 - activation_loss: 0.8684 - activation_1_loss: 0.8300 - val_loss: 1.6658 - val_activation_loss: 0.8500 - val_activation_1_loss: 0.8158\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5527 - activation_loss: 0.7948 - activation_1_loss: 0.7579\n",
      "Epoch 00003: val_loss improved from 1.66578 to 1.63511, saving model to v0-roberta-3.h5\n",
      "21985/21985 [==============================] - 291s 13ms/sample - loss: 1.5526 - activation_loss: 0.7936 - activation_1_loss: 0.7568 - val_loss: 1.6351 - val_activation_loss: 0.8409 - val_activation_1_loss: 0.7941\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 4 Jaccard = 0.7081457587062032\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.2306 - activation_loss: 1.0813 - activation_1_loss: 1.1493\n",
      "Epoch 00001: val_loss improved from inf to 1.67230, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 310s 14ms/sample - loss: 2.2305 - activation_loss: 1.0798 - activation_1_loss: 1.1476 - val_loss: 1.6723 - val_activation_loss: 0.8676 - val_activation_1_loss: 0.8040\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6542 - activation_loss: 0.8416 - activation_1_loss: 0.8126\n",
      "Epoch 00002: val_loss did not improve from 1.67230\n",
      "21985/21985 [==============================] - 290s 13ms/sample - loss: 1.6541 - activation_loss: 0.8404 - activation_1_loss: 0.8119 - val_loss: 1.7663 - val_activation_loss: 0.8548 - val_activation_1_loss: 0.9107\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.5650 - activation_loss: 0.7957 - activation_1_loss: 0.7694\n",
      "Epoch 00003: val_loss improved from 1.67230 to 1.61465, saving model to v0-roberta-4.h5\n",
      "21985/21985 [==============================] - 291s 13ms/sample - loss: 1.5652 - activation_loss: 0.7945 - activation_1_loss: 0.7752 - val_loss: 1.6147 - val_activation_loss: 0.8466 - val_activation_1_loss: 0.7674\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      "Predicting Test...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      ">>>> FOLD 5 Jaccard = 0.7109581030180844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.7/ Kaggle submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>0a5e7d6cb1</td>\n",
       "      <td>I so know what you mean</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i so know what you mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>6a14956779</td>\n",
       "      <td>If we don`t pack, she can`t leave, right?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>if we don`t pack, she can`t leave, right?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>bc9ebe0002</td>\n",
       "      <td>I hate my life</td>\n",
       "      <td>negative</td>\n",
       "      <td>i hate my life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582</th>\n",
       "      <td>e78f6d920b</td>\n",
       "      <td>good day in the sun... little bit burnt tho... well actu...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>good day in the sun... little bit burnt tho... well act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>3ea60172a0</td>\n",
       "      <td>Paid all my bills only to go out to the mailbox to find ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>paid all my bills only to go out to the mailbox to find...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>c31d79c440</td>\n",
       "      <td>I just cried whilst watching hollyoaks  .. i need a life...</td>\n",
       "      <td>negative</td>\n",
       "      <td>cried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>5f58dc8b36</td>\n",
       "      <td>I have been playing skate for two hours. Now i need to g...</td>\n",
       "      <td>negative</td>\n",
       "      <td>but it`s too late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>e83b4e9bf0</td>\n",
       "      <td>you`re a busy Bob, I`m jealous. I`m not sure I can even...</td>\n",
       "      <td>negative</td>\n",
       "      <td>jealous.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>a14b4ff440</td>\n",
       "      <td>Happy Birthday Snickers!!!! ? I hope you have the best d...</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy birthday snickers!!!! ? i hope you have the best ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422</th>\n",
       "      <td>218412b76a</td>\n",
       "      <td>My younger sister flies in next week for a two week visi...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>my younger sister flies in next week for a two week vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>3b388efbe6</td>\n",
       "      <td>got to see 2 of my favorite guys back on long island  he...</td>\n",
       "      <td>negative</td>\n",
       "      <td>got to see 2 of my favorite guys back on long island he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>001617fca4</td>\n",
       "      <td>oh my god!!! i cried so much!!! watch this guys from BGT...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>oh my god!!! i cried so much!!! watch this guys from bgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>778c5660f8</td>\n",
       "      <td>Justin Timberlake and Andy Samberg do it again.. Mother ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>990cdeae87</td>\n",
       "      <td>I want to see David cook!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>i want to see david cook!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>87fa9ce0c5</td>\n",
       "      <td>oh no  say it aint so</td>\n",
       "      <td>negative</td>\n",
       "      <td>oh no say it aint so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>f422f53b7f</td>\n",
       "      <td>we shuld do  a dance like that its seriously the best t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>ed84aa3983</td>\n",
       "      <td>I don`t wanna work a 10-hour shift today! I`d rather whi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i don`t wanna work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>6fbd9f3ae7</td>\n",
       "      <td>LAST CALL!!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>last call!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>d613b00386</td>\n",
       "      <td>I know, right? I guess the oldest doesn`t hold rank on...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i know, right? i guess the oldest doesn`t hold rank on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>56c08b2df6</td>\n",
       "      <td>100,000 / 60 = 1667 words/day.  1667 / 10 = 167 words/h...</td>\n",
       "      <td>positive</td>\n",
       "      <td>no pressure.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>fb08563a7b</td>\n",
       "      <td>Midnight ice-cream weather! So **** bored</td>\n",
       "      <td>negative</td>\n",
       "      <td>bored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>9b1060a30e</td>\n",
       "      <td>how`s the fam? Well I hope.</td>\n",
       "      <td>positive</td>\n",
       "      <td>well i hope.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>419f3e9ef8</td>\n",
       "      <td>I know, but work is so boring. I prefer the  take on life</td>\n",
       "      <td>negative</td>\n",
       "      <td>boring.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>e9c03baff9</td>\n",
       "      <td>I`ll be sure to</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i`ll be sure to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>982b1c05d3</td>\n",
       "      <td>LOVE the album guys and can`t wait for the official rel...</td>\n",
       "      <td>positive</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>e5599343e7</td>\n",
       "      <td>knackered! been awake since 5 as couldn`t sleep! just s...</td>\n",
       "      <td>positive</td>\n",
       "      <td>awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>b6cd695d48</td>\n",
       "      <td>Working... And attempting to keep squirrels away...........</td>\n",
       "      <td>negative</td>\n",
       "      <td>i should stick to spiders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>e5a26fb323</td>\n",
       "      <td>lol! woow okay its not that big of a deal</td>\n",
       "      <td>neutral</td>\n",
       "      <td>lol! woow okay its not that big of a deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>f3912c1463</td>\n",
       "      <td>Emergency Radio for iPhone is awesome, listening to John...</td>\n",
       "      <td>positive</td>\n",
       "      <td>awesome,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>c435fb9416</td>\n",
       "      <td>I wish i had my iPod , i need some jonas . I miss their ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i wish i had my ipod , i need some jonas . i miss their...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "1357  0a5e7d6cb1                                      I so know what you mean   \n",
       "176   6a14956779                    If we don`t pack, she can`t leave, right?   \n",
       "203   bc9ebe0002                                               I hate my life   \n",
       "2582  e78f6d920b  good day in the sun... little bit burnt tho... well actu...   \n",
       "478   3ea60172a0  Paid all my bills only to go out to the mailbox to find ...   \n",
       "65    c31d79c440  I just cried whilst watching hollyoaks  .. i need a life...   \n",
       "2351  5f58dc8b36  I have been playing skate for two hours. Now i need to g...   \n",
       "456   e83b4e9bf0   you`re a busy Bob, I`m jealous. I`m not sure I can even...   \n",
       "83    a14b4ff440  Happy Birthday Snickers!!!! ? I hope you have the best d...   \n",
       "3422  218412b76a  My younger sister flies in next week for a two week visi...   \n",
       "2807  3b388efbe6  got to see 2 of my favorite guys back on long island  he...   \n",
       "128   001617fca4  oh my god!!! i cried so much!!! watch this guys from BGT...   \n",
       "2038  778c5660f8  Justin Timberlake and Andy Samberg do it again.. Mother ...   \n",
       "105   990cdeae87                                   I want to see David cook!!   \n",
       "1341  87fa9ce0c5                                        oh no  say it aint so   \n",
       "2248  f422f53b7f   we shuld do  a dance like that its seriously the best t...   \n",
       "2296  ed84aa3983  I don`t wanna work a 10-hour shift today! I`d rather whi...   \n",
       "2717  6fbd9f3ae7                                                  LAST CALL!!   \n",
       "983   d613b00386    I know, right? I guess the oldest doesn`t hold rank on...   \n",
       "3067  56c08b2df6   100,000 / 60 = 1667 words/day.  1667 / 10 = 167 words/h...   \n",
       "2226  fb08563a7b                    Midnight ice-cream weather! So **** bored   \n",
       "1409  9b1060a30e                                  how`s the fam? Well I hope.   \n",
       "615   419f3e9ef8    I know, but work is so boring. I prefer the  take on life   \n",
       "2698  e9c03baff9                                              I`ll be sure to   \n",
       "2241  982b1c05d3   LOVE the album guys and can`t wait for the official rel...   \n",
       "2019  e5599343e7   knackered! been awake since 5 as couldn`t sleep! just s...   \n",
       "2274  b6cd695d48  Working... And attempting to keep squirrels away...........   \n",
       "867   e5a26fb323                    lol! woow okay its not that big of a deal   \n",
       "1920  f3912c1463  Emergency Radio for iPhone is awesome, listening to John...   \n",
       "1071  c435fb9416  I wish i had my iPod , i need some jonas . I miss their ...   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "1357   neutral                                      i so know what you mean  \n",
       "176    neutral                    if we don`t pack, she can`t leave, right?  \n",
       "203   negative                                               i hate my life  \n",
       "2582   neutral   good day in the sun... little bit burnt tho... well act...  \n",
       "478    neutral   paid all my bills only to go out to the mailbox to find...  \n",
       "65    negative                                                        cried  \n",
       "2351  negative                                            but it`s too late  \n",
       "456   negative                                                     jealous.  \n",
       "83    positive   happy birthday snickers!!!! ? i hope you have the best ...  \n",
       "3422   neutral   my younger sister flies in next week for a two week vis...  \n",
       "2807  negative   got to see 2 of my favorite guys back on long island he...  \n",
       "128    neutral     oh my god!!! i cried so much!!! watch this guys from bgt  \n",
       "2038  positive                                                        great  \n",
       "105   positive                                   i want to see david cook!!  \n",
       "1341  negative                                         oh no say it aint so  \n",
       "2248  positive                                                         best  \n",
       "2296  negative                                           i don`t wanna work  \n",
       "2717   neutral                                                  last call!!  \n",
       "983    neutral   i know, right? i guess the oldest doesn`t hold rank on ...  \n",
       "3067  positive                                                 no pressure.  \n",
       "2226  negative                                                        bored  \n",
       "1409  positive                                                 well i hope.  \n",
       "615   negative                                                      boring.  \n",
       "2698   neutral                                              i`ll be sure to  \n",
       "2241  positive                                                         love  \n",
       "2019  positive                                                      awesome  \n",
       "2274  negative                                    i should stick to spiders  \n",
       "867    neutral                    lol! woow okay its not that big of a deal  \n",
       "1920  positive                                                     awesome,  \n",
       "1071   neutral   i wish i had my ipod , i need some jonas . i miss their...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
