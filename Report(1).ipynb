{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Report.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xlqAD9pUtgVK",
        "5t6bKNrmtgVM",
        "JSDRDMtotgVQ",
        "PIyH0kRQtgVV"
      ]
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Preface**\n",
        "**This project was inspired and referenced in [TensorFlow roBERTa - [0.705]](https://www.kaggle.com/code/cdeotte/tensorflow-roberta-0-705)**"
      ],
      "metadata": {
        "id": "B0uoFvNNtgUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feeling about this project**"
      ],
      "metadata": {
        "id": "c2voR7LLtgU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Vinh**\n",
        "**Before getting this course, i just a newbie in data science. Everything with me are new and i think i should do more for getting the information through the project.\n",
        "It's so lucky for me to have a dedicated teacher and a good teammate. First of all, doing this NLP project is so hard for me. Some concepts are so hard to learn but Tam helped me and showed me a lot. Day by day, with the instructions and comments of teacher and other people. I can do it better and better. Now, i know the structure of the data science project and some steps to do it, from know the questions to data mining, visualize the data, build model to training the model.\n",
        "Thank for helping me in this project.**\n"
      ],
      "metadata": {
        "id": "GrfxX2aLtgU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tam**\n",
        "**From all the lessons i have passed from the beginning of this course, the achivements i've earned are a lot, from building tools and mindset for data analytic, using kaggle for both leaning and building a complete project profile,etc... And from teacher and classmates opinions, guides and mindset, i learned to improve my personal skills in representations, working for our team projet. Gradually, our team have done better and better in both project and personal skills. I'm sincerly thank you for all instructions and ideas from teacher and classmates.**"
      ],
      "metadata": {
        "id": "N1H7o6JqtgU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1/ About team**\n",
        "|StuID  |        Name       |     Kaggle    |       Github      |\n",
        "|-------|-------------------|---------------|-------------------|\n",
        "|1752052|Dang Huu Phuoc Vinh|[V_Notebook](https://www.kaggle.com/danghuuphuocvinh)|[V_Github](https://github.com/DangHuuPhuocVinh/data_science_application)\n",
        "|1753097|Le Nguyen Minh Tam |[T_Notebook](https://www.kaggle.com/minhtamlenguyen)|[T_Github](https://github.com/lnmtam1999)"
      ],
      "metadata": {
        "id": "09WKhlCetgU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1/ Link Github of team**\n",
        "\n",
        "[Github](https://github.com/DangHuuPhuocVinh/data_science_application)\n"
      ],
      "metadata": {
        "id": "u-2KnJLNtgU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2/ Schedule for future plans:**\n",
        "\n",
        "*   Improve the data and the model for better leanning rate.\n",
        "*   Using model for live streamming with raw data from twitter and can classified hashtag(#) for classes."
      ],
      "metadata": {
        "id": "dkd6hyWetgU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2/ About competition**"
      ],
      "metadata": {
        "id": "Yry8QJWxtgU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1/ Name of competition**\n",
        "**[Tweet Sentiment Extraction](https://www.kaggle.com/competitions/tweet-sentiment-extraction) organized by [Kaggle](https://www.kaggle.com/)**"
      ],
      "metadata": {
        "id": "zNRuI3zytgU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2/ Prize**\n",
        "**15000 USD**"
      ],
      "metadata": {
        "id": "rDDhviXRtgVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3/ Description**\n",
        "  **E.g: \"My ridiculous dog is amazing.\" [sentiment: positive]**\n",
        "\n",
        "  **With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.**\n",
        "\n",
        "  **In this competition we've extracted support phrases from [Figure Eight's Data for Everyone platform](https://appen.com/datasets-resource-center/). The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.**"
      ],
      "metadata": {
        "id": "m7KYqlEAtgVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.4/ Input and Output**\n",
        "- **Input: textID, text and sentiment**\n",
        "- **Output: selected_text**"
      ],
      "metadata": {
        "id": "ZKyNQvnptgVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.5/ Columns**\n",
        "-  **textID - unique ID for each piece of text**\n",
        "-  **text - the text of the tweet**\n",
        "-  **sentiment - the general sentiment of the tweet**\n",
        "-  **selected_text - [train only] the text that supports the tweet's sentiment**"
      ],
      "metadata": {
        "id": "jqVOJRawtgVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.6/ Meaningful**\n",
        "- **After doing this project, we can have a dataset with the phrases that were selected for using at any other NLP project**\n",
        "- **Can use for detecting some keywords that have sentiment** "
      ],
      "metadata": {
        "id": "4PTGaHbltgVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.7/ Evaluation**\n",
        " ![img](https://user-images.githubusercontent.com/35680794/174698744-57b2f116-fbe4-4fb6-9216-2e83e0494dca.png)"
      ],
      "metadata": {
        "id": "J6p8dbpFtgVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3/ Model roBERTa for this project**"
      ],
      "metadata": {
        "id": "2FeNN5ZKtgVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A bit about CNN model using after processing data and analyze, the model is about how the machine can regconize Bag-Of-Words and transfers them into many little neurons to understand what is the meaning of text or the sentiments for each charaters, sentences represents for.**"
      ],
      "metadata": {
        "id": "cH2xtnpJtgVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://images.viblo.asia/e8bfa0ba-dacd-4e6e-af7e-9c13370c2e3f.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "9kOIxs0FtgVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instead of the input as points in Computer Vision, the input of natural language analysis is each line of a matrix corresponds to a code, which is most often a word, but it can also be a character. The row itself is a vector representation for a word. Usually these word vectors are presented at a low level such as word2vec or Glove, but it can also be a vector with jobs that will be evaluated as belonging to only one set of vocabulary. After process the input points, the filters will slide through the patchwork of the input image, but in NLP we usually use filters that slide over all the rows of the matrix (words). So our filter will be - width equal to the width of the matrix, the length can vary but normally we will slide through 2-5 words**"
      ],
      "metadata": {
        "id": "xiNeYzewtgVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is the summary for the application of CNN in NLP and for data analytics**"
      ],
      "metadata": {
        "id": "vvpPwh5KtgVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://images.viblo.asia/bdb4660c-33bd-4b90-b36f-477593416302.png)"
      ],
      "metadata": {
        "id": "Fg5Q0CvvtgVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://images.viblo.asia/e89ac9ee-7ac9-4e8c-af6f-abe433f268ca.png)"
      ],
      "metadata": {
        "id": "VB9UPT6utgVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The pooling operation is used to combine the vectors resulting from different convolution windows into a single -dimensional vector. This is done again by taking the max or the average value observed in resulting vector from the convolutions. This vector will capture the most relevant features of the sentence/document.** (Ref: https://www.davidsbatista.net/blog/2018/03/31/SentenceClassificationConvNets/)"
      ],
      "metadata": {
        "id": "IlMCwYkrtgVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://editor.analyticsvidhya.com/uploads/69307Screenshot%20(175).png)"
      ],
      "metadata": {
        "id": "i5OBRLiWtgVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is the final architecture for this CNN model, that is used for transfer and understanding of machine in NLP.**"
      ],
      "metadata": {
        "id": "Nwtl2M6rtgVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://editor.analyticsvidhya.com/uploads/23540Screenshot%20(177).png)"
      ],
      "metadata": {
        "id": "kJJnmV8ytgVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4/ Developing the project**"
      ],
      "metadata": {
        "id": "8Y9s-fcctgVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.1/ Import Libraries, Data and Tokenizer**"
      ],
      "metadata": {
        "id": "5UJHwhb0tgVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.status.busy": "2022-08-05T19:15:46.795355Z",
          "iopub.execute_input": "2022-08-05T19:15:46.795692Z",
          "iopub.status.idle": "2022-08-05T19:15:47.281735Z",
          "shell.execute_reply.started": "2022-08-05T19:15:46.795651Z",
          "shell.execute_reply": "2022-08-05T19:15:47.281029Z"
        },
        "trusted": true,
        "id": "ms6vxHr8tgVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1.1/ Tokenizer**\n",
        "First of all, we use tokenizer to convert the word to array for the computer can understand."
      ],
      "metadata": {
        "id": "xlqAD9pUtgVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 100\n",
        "PATH = '../input/tf-roberta/'\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file=PATH+'vocab-roberta-base.json', \n",
        "    merges_file=PATH+'merges-roberta-base.txt', \n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
        "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
        "train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T19:13:26.561316Z",
          "iopub.execute_input": "2022-08-05T19:13:26.561599Z",
          "iopub.status.idle": "2022-08-05T19:13:26.837716Z",
          "shell.execute_reply.started": "2022-08-05T19:13:26.561570Z",
          "shell.execute_reply": "2022-08-05T19:13:26.836926Z"
        },
        "trusted": true,
        "id": "iHJsrHu9tgVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAX_LEN** = 100 then for each training row, RoBERTa receives 96 tokens. The reason to use **lowercase** and **add_prefix_space** because when spelling with RoBERTa :\" helllo\", \"hello\", \" Hello\", and \"Hello\" use the same \" hello\" token."
      ],
      "metadata": {
        "id": "ufxq3l46tgVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.2/ Training data**\n"
      ],
      "metadata": {
        "id": "q1uTlZ52tgVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![roberta.jpg](attachment:55941e09-2337-4285-94e9-6d3d116a8f18.jpg)"
      ],
      "metadata": {
        "id": "cqFzf2hwtgVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.2.1/ Adjust the input**\n",
        "In this stage we ready our data for the model, all the input will be change to numerical and training."
      ],
      "metadata": {
        "id": "5t6bKNrmtgVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ct = train.shape[0]\n",
        "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in range(train.shape[0]):\n",
        "    \n",
        "    # FIND OVERLAP\n",
        "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
        "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
        "    idx = text1.find(text2)\n",
        "    chars = np.zeros((len(text1)))\n",
        "    chars[idx:idx+len(text2)]=1\n",
        "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
        "    enc = tokenizer.encode(text1) \n",
        "        \n",
        "    # ID_OFFSETS\n",
        "    offsets = []; idx=0\n",
        "    for t in enc.ids:\n",
        "        w = tokenizer.decode([t])\n",
        "        offsets.append((idx,idx+len(w)))\n",
        "        idx += len(w)\n",
        "    \n",
        "    # START END TOKENS\n",
        "    toks = []\n",
        "    for i,(a,b) in enumerate(offsets):\n",
        "        sm = np.sum(chars[a:b])\n",
        "        if sm>0: toks.append(i) \n",
        "        \n",
        "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
        "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "    attention_mask[k,:len(enc.ids)+5] = 1\n",
        "    if len(toks)>0:\n",
        "        start_tokens[k,toks[0]+1] = 1\n",
        "        end_tokens[k,toks[-1]+1] = 1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T19:13:32.188594Z",
          "iopub.execute_input": "2022-08-05T19:13:32.188875Z",
          "iopub.status.idle": "2022-08-05T19:13:39.852789Z",
          "shell.execute_reply.started": "2022-08-05T19:13:32.188843Z",
          "shell.execute_reply": "2022-08-05T19:13:39.852082Z"
        },
        "trusted": true,
        "id": "5AsJBkt4tgVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Output for data after mmasking and input layers:**"
      ],
      "metadata": {
        "id": "rXcq6zEktgVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\",input_ids)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T19:13:43.332791Z",
          "iopub.execute_input": "2022-08-05T19:13:43.333113Z",
          "iopub.status.idle": "2022-08-05T19:13:43.341846Z",
          "shell.execute_reply.started": "2022-08-05T19:13:43.333079Z",
          "shell.execute_reply": "2022-08-05T19:13:43.341005Z"
        },
        "trusted": true,
        "id": "t88xKQsJtgVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\",attention_mask)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T19:13:46.354202Z",
          "iopub.execute_input": "2022-08-05T19:13:46.354510Z",
          "iopub.status.idle": "2022-08-05T19:13:46.361620Z",
          "shell.execute_reply.started": "2022-08-05T19:13:46.354477Z",
          "shell.execute_reply": "2022-08-05T19:13:46.360724Z"
        },
        "trusted": true,
        "id": "TxEdB5tktgVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\",offsets)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T19:13:48.863748Z",
          "iopub.execute_input": "2022-08-05T19:13:48.864055Z",
          "iopub.status.idle": "2022-08-05T19:13:48.868814Z",
          "shell.execute_reply.started": "2022-08-05T19:13:48.864023Z",
          "shell.execute_reply": "2022-08-05T19:13:48.867794Z"
        },
        "trusted": true,
        "id": "LWL2Vy26tgVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.2.2/Visualizing the sentiment**"
      ],
      "metadata": {
        "id": "JSDRDMtotgVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will see which words have the negative sentiment"
      ],
      "metadata": {
        "id": "qW_l_z--tgVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Negative = train[train['sentiment'] == ('negative')]\n",
        "wordCloud = WordCloud(background_color=\"white\", width=1600, height=800).generate(' '.join(Negative.text))\n",
        "plt.figure(figsize=(20,10), facecolor='k')\n",
        "plt.imshow(wordCloud)\n",
        "print(len(Negative))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T19:15:51.969566Z",
          "iopub.execute_input": "2022-08-05T19:15:51.969846Z",
          "iopub.status.idle": "2022-08-05T19:15:55.122363Z",
          "shell.execute_reply.started": "2022-08-05T19:15:51.969817Z",
          "shell.execute_reply": "2022-08-05T19:15:55.120856Z"
        },
        "trusted": true,
        "id": "w0NPMUImtgVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will see which words have the positive sentiment"
      ],
      "metadata": {
        "id": "6aN6kEX5tgVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Positive = train[train['sentiment'] == ('positive')]\n",
        "wordCloud = WordCloud(background_color=\"white\", width=1600, height=800).generate(' '.join(Positive.text))\n",
        "plt.figure(figsize=(20,10), facecolor='k')\n",
        "plt.imshow(wordCloud)\n",
        "print(len(Negative))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T19:16:01.287637Z",
          "iopub.execute_input": "2022-08-05T19:16:01.287954Z",
          "iopub.status.idle": "2022-08-05T19:16:04.182297Z",
          "shell.execute_reply.started": "2022-08-05T19:16:01.287899Z",
          "shell.execute_reply": "2022-08-05T19:16:04.180889Z"
        },
        "trusted": true,
        "id": "E8eChyMytgVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will see which words have the neutral sentiment"
      ],
      "metadata": {
        "id": "O8cMvC0VtgVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Neutral = train[train['sentiment'] == ('neutral')]\n",
        "wordCloud = WordCloud(background_color=\"white\", width=1600, height=800).generate(' '.join(Neutral.text))\n",
        "plt.figure(figsize=(20,10), facecolor='k')\n",
        "plt.imshow(wordCloud)\n",
        "print(len(Negative))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T19:16:07.751247Z",
          "iopub.execute_input": "2022-08-05T19:16:07.751543Z",
          "iopub.status.idle": "2022-08-05T19:16:10.849483Z",
          "shell.execute_reply.started": "2022-08-05T19:16:07.751512Z",
          "shell.execute_reply": "2022-08-05T19:16:10.845176Z"
        },
        "trusted": true,
        "id": "E-QarlI7tgVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.3/ Test Data**"
      ],
      "metadata": {
        "id": "Jr6PfupttgVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do the same as the Training stage."
      ],
      "metadata": {
        "id": "XiSkKpVQtgVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
        "\n",
        "ct = test.shape[0]\n",
        "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in range(test.shape[0]):\n",
        "        \n",
        "    # INPUT_IDS\n",
        "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
        "    enc = tokenizer.encode(text1)                \n",
        "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
        "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "    attention_mask_t[k,:len(enc.ids)+5] = 1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T19:19:39.037599Z",
          "iopub.execute_input": "2022-08-05T19:19:39.037959Z",
          "iopub.status.idle": "2022-08-05T19:19:39.305925Z",
          "shell.execute_reply.started": "2022-08-05T19:19:39.037891Z",
          "shell.execute_reply": "2022-08-05T19:19:39.305220Z"
        },
        "trusted": true,
        "id": "x7K02_YGtgVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "if the original text is [ i][ am][ having][ a][ great][ day]</s>[ positive]</s> with tokens 0,1,2,3,4,5,6,7,8,9,10 and the selected text is \"great day\", then the training has start index = 5 and end index = 6.\n",
        "\n",
        "If our model also predicts a = 5 and b = 6 and we try to select the text from [ i][ am][ having][ a][ great][ day], the indices 5 and 6 will not return \"great day\". Instead we must use tokens[4:6] to get great day. We subtract 1 because the is now removed. And we add 1 to b because python indexing for list[3:5] does not return 5 it only returns 3, 4"
      ],
      "metadata": {
        "id": "qhgXoUlKtgVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.3.1/Visualizing the sentiment**"
      ],
      "metadata": {
        "id": "PIyH0kRQtgVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will see which words have the negative sentiment"
      ],
      "metadata": {
        "id": "0JxzdY8etgVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Negative = test[test['sentiment'] == ('negative')]\n",
        "wordCloud = WordCloud(background_color=\"white\", width=1600, height=800).generate(' '.join(Negative.text))\n",
        "plt.figure(figsize=(20,10), facecolor='k')\n",
        "plt.imshow(wordCloud)\n",
        "print(len(Negative))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T19:19:43.189154Z",
          "iopub.execute_input": "2022-08-05T19:19:43.189432Z",
          "iopub.status.idle": "2022-08-05T19:19:45.790020Z",
          "shell.execute_reply.started": "2022-08-05T19:19:43.189402Z",
          "shell.execute_reply": "2022-08-05T19:19:45.789262Z"
        },
        "trusted": true,
        "id": "9TjtsVfstgVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will see which words have the positive sentiment"
      ],
      "metadata": {
        "id": "AbrvbXottgVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Positive = test[test['sentiment'] == ('positive')]\n",
        "wordCloud = WordCloud(background_color=\"white\", width=1600, height=800).generate(' '.join(Positive.text))\n",
        "plt.figure(figsize=(20,10), facecolor='k')\n",
        "plt.imshow(wordCloud)\n",
        "print(len(Negative))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T19:20:00.727424Z",
          "iopub.execute_input": "2022-08-05T19:20:00.727706Z",
          "iopub.status.idle": "2022-08-05T19:20:03.330478Z",
          "shell.execute_reply.started": "2022-08-05T19:20:00.727677Z",
          "shell.execute_reply": "2022-08-05T19:20:03.329830Z"
        },
        "trusted": true,
        "id": "KVRmyQowtgVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will see which words have the neutral sentiment"
      ],
      "metadata": {
        "id": "h_alixmatgVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Neutral = test[test['sentiment'] == ('neutral')]\n",
        "wordCloud = WordCloud(background_color=\"white\", width=1600, height=800).generate(' '.join(Neutral.text))\n",
        "plt.figure(figsize=(20,10), facecolor='k')\n",
        "plt.imshow(wordCloud)\n",
        "print(len(Negative))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-05T19:20:07.083299Z",
          "iopub.execute_input": "2022-08-05T19:20:07.083620Z",
          "iopub.status.idle": "2022-08-05T19:20:10.101277Z",
          "shell.execute_reply.started": "2022-08-05T19:20:07.083586Z",
          "shell.execute_reply": "2022-08-05T19:20:10.100398Z"
        },
        "trusted": true,
        "id": "M6IAx8a-tgVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.4/ Build roBERTa model**\n"
      ],
      "metadata": {
        "id": "Nf5c03Z0tgVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![roberta 2.jpg](attachment:e7ba18f0-b2ab-49a7-a45a-8debbe06611c.jpg)"
      ],
      "metadata": {
        "id": "67wFGRJltgVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.4.1/ Building the bones of model**\n",
        "Built the bones of a roBERTa model, using the model has already train by the author."
      ],
      "metadata": {
        "id": "LWtBw1RctgVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "\n",
        "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
        "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
        "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
        "    \n",
        "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-10T19:05:21.352259Z",
          "iopub.execute_input": "2022-07-10T19:05:21.352602Z",
          "iopub.status.idle": "2022-07-10T19:05:21.364895Z",
          "shell.execute_reply.started": "2022-07-10T19:05:21.352569Z",
          "shell.execute_reply": "2022-07-10T19:05:21.364038Z"
        },
        "trusted": true,
        "id": "ET6-F6qCtgVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.5/ Create metric**\n"
      ],
      "metadata": {
        "id": "-UEu85litgVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We code this for the evaluation"
      ],
      "metadata": {
        "id": "FxTt-90ctgVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    if (len(a)==0) & (len(b)==0): return 0.5\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-10T19:06:16.388632Z",
          "iopub.execute_input": "2022-07-10T19:06:16.388950Z",
          "iopub.status.idle": "2022-07-10T19:06:16.395186Z",
          "shell.execute_reply.started": "2022-07-10T19:06:16.388923Z",
          "shell.execute_reply": "2022-07-10T19:06:16.394374Z"
        },
        "trusted": true,
        "id": "UkbutV4ftgVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.6/ Train roBERTa model**"
      ],
      "metadata": {
        "id": "JugpnLH9tgVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
        "\n",
        "    print('#'*25)\n",
        "    print('### FOLD %i'%(fold+1))\n",
        "    print('#'*25)\n",
        "    \n",
        "    K.clear_session()\n",
        "    model = build_model()\n",
        "        \n",
        "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
        "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
        "        \n",
        "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
        "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
        "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
        "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
        "    \n",
        "    print('Loading model...')\n",
        "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
        "    \n",
        "    print('Predicting OOF...')\n",
        "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
        "    \n",
        "    print('Predicting Test...')\n",
        "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
        "    preds_start += preds[0]/skf.n_splits\n",
        "    preds_end += preds[1]/skf.n_splits\n",
        "    \n",
        "    # DISPLAY FOLD JACCARD\n",
        "    all = []\n",
        "    for k in idxV:\n",
        "        a = np.argmax(oof_start[k,])\n",
        "        b = np.argmax(oof_end[k,])\n",
        "        if a>b: \n",
        "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
        "        else:\n",
        "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
        "            enc = tokenizer.encode(text1)\n",
        "            st = tokenizer.decode(enc.ids[a-1:b])\n",
        "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
        "    jac.append(np.mean(all))\n",
        "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
        "    print()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-10T19:07:44.465660Z",
          "iopub.execute_input": "2022-07-10T19:07:44.465971Z"
        },
        "trusted": true,
        "id": "Ur5qdgrdtgVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.7/ Kaggle submission**"
      ],
      "metadata": {
        "id": "RlVKZ4REtgVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all = []\n",
        "for k in range(input_ids_t.shape[0]):\n",
        "    a = np.argmax(preds_start[k,])\n",
        "    b = np.argmax(preds_end[k,])\n",
        "    if a>b: \n",
        "        st = test.loc[k,'text']\n",
        "    else:\n",
        "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
        "        enc = tokenizer.encode(text1)\n",
        "        st = tokenizer.decode(enc.ids[a-1:b])\n",
        "    all.append(st)"
      ],
      "metadata": {
        "id": "4RCUB4oxtgVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['selected_text'] = all\n",
        "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
        "pd.set_option('max_colwidth', 60)\n",
        "test.sample(30)"
      ],
      "metadata": {
        "id": "OjcSSdZ1tgVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.7.1/ Results**"
      ],
      "metadata": {
        "id": "oZ49mmW5tgVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://github.com/DangHuuPhuocVinh/data_science_application/blob/main/submit.PNG?raw=true)"
      ],
      "metadata": {
        "id": "UW9XYrxct2Pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://github.com/DangHuuPhuocVinh/data_science_application/blob/main/submit%202.PNG?raw=true)"
      ],
      "metadata": {
        "id": "KxVP24rstgVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5/ References:**"
      ],
      "metadata": {
        "id": "eQnFYGgytgVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code: \n",
        "* https://www.kaggle.com/code/cdeotte/tensorflow-roberta-0-705\n",
        "* https://www.kaggle.com/code/abhishek/bert-inference-5-folds\n",
        "*https://www.kaggle.com/code/cdeotte/tensorflow-transformer-0-790\n",
        "\n",
        "Images : \n",
        "\n",
        "*   https://viblo.asia/p/understanding-convolutional-neural-networks-for-natural-language-processing-bJzKmW0Bl9N\n",
        "*   https://www.analyticsvidhya.com/blog/2021/09/natural-language-processing-using-cnns-for-sentence-classification/\n"
      ],
      "metadata": {
        "id": "dwUU1A2BtgVd"
      }
    }
  ]
}